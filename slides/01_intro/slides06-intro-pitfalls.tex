%TODO: Chapter needs to be improved a lot
\documentclass[11pt,compress,t,notes=noshow, aspectratio=169, xcolor=table]{beamer}

\usepackage{../../style/lmu-lecture}
% Defines macros and environments
\input{../../style/common.tex}

\usepackage[export]{adjustbox}
\usepackage[most]{tcolorbox}

\newtcolorbox{BlueBox}[2][]{%
   enhanced,
   colback   = blue!5!white,
   colframe  = blue!65!black,
   arc       = 1mm,
   outer arc = 1mm,
   fonttitle = \Large\slshape\textbf,
   center title,
   title     = #2,
   #1}

\tikzset{main node/.style={rectangle,draw,minimum size=1.2cm, minimum width=3cm,inner sep=4pt},}

\title{Interpretable Machine Learning}
% \author{LMU}
%\institute{\href{https://compstat-lmu.github.io/lecture_iml/}{compstat-lmu.github.io/lecture\_iml}}
\date{}

\begin{document}

\newcommand{\titlefigure}{figure/open_blackbox}
\newcommand{\learninggoals}{
\item General pitfalls of interpretation methods
\item Practices to avoid pitfalls}

\lecturechapter{Pitfalls and Best Practices}
\lecture{Interpretable Machine Learning}
%
%

% \begin{frame}[t]{Pitfalls and Best Practices~\citebutton{Molnar et. al (2021)}{https://arxiv.org/abs/2007.04131v2}}

% \begin{itemize}[<+->]
%     \item \textbf{Proper training and evaluation}:
%     To gain insights into data generating process, deployed model should at least generalize well to unseen data (garbage in, garbage out)
%     %Model interpretation is only as good as the underlying model (garbage in, garbage out).
%     \item \textbf{Avoid unnecessary complexity}: Prefer simple interpretable models and use them as baseline
%     \item \textbf{Quantify uncertainty}: Interpretation methods are often (statistical) estimators \\
%     $\leadsto$ Beware of uncertainty, we may need confidence intervals
%     %To avoid interpretation of noise, include uncertainty estimates for the interpretations, e.g.,  confidence intervals for feature importance.
%     \item \textbf{Careful with causality}:
%      Do you want to understand the model or the nature of DGP?\\
%      $\leadsto$ Your goal should guide the choice of interpretation method
%      %Causal interpretation requires assumptions about relationships in the data and a corresponding model considerations (e.g., including confounders into the model).
%     \item \textbf{Consider dependencies}: Some interpretation methods suffer when features are dependent\\
%     $\leadsto$ Check presence of dependencies and use suitable methods
%     \item \textbf{Beware of simplifications}:
%     % Interpretation methods map complex models to low-dim. explanations
%     % Interpretation methods
%     Mapping of complex models to low-dim. explanations\\
%     $\leadsto$ Information loss, e.g., some interpretation methods hide interactions
% \end{itemize}

% \end{frame}

\begin{frame}{Sources of pitfalls~\citebutton{Molnar et. al (2021)}{https://arxiv.org/abs/2007.04131v2}}

\begin{center}
    %\textbf{Data analysis process}\\
    \quad \\ \vspace{0.5cm}
    \begin{tikzpicture}[every path/.style={->,line width=0.35mm,thick},
                            every label/.append style={align=left, font=\footnotesize, text width=4cm},
                            scale=0.7, transform shape]
            \node[main node] (1) { Data };
            \node[main node] (2) [right of=1, node distance=4.5cm] { ML Model };
            \node[main node, right of=2, node distance=4.5cm] (3) { IML Method };
            \node[main node, right of=3, node distance=4.5cm] (4) { Interpretation };
            \node[main node, below of=3, node distance=3cm, fill = lightgray] (5) { Sources of Pitfalls };
            \node[main node, below of=2, node distance=6cm] (6) { Issues of ML model };
            \node[main node, below of=3, node distance=6cm] (7) { Issues of IML method };
            \node[main node, below of=4, node distance=6cm, align = center] (8) { Issues due to wrong \\ use of IML method };
            
            \draw (1) -- (2);
            \draw (2) -- (3);
            \draw (3) -- (4);
            \draw (2) -- (5);
            \draw (3) -- (5);
            \draw (4) -- (5);
            \draw (5) -- (6);
            \draw (5) -- (7);
            \draw (5) -- (8);
        \end{tikzpicture} 
\end{center}
\end{frame}


\begin{frame}{Issues of ML model~\citebutton{Molnar et. al (2021)}{https://arxiv.org/abs/2007.04131v2}}
    \begin{itemize}
        \item \textbf{Proper training and evaluation}: To gain insights into DGP, deployed model should generalize well to unseen data (garbage in, garbage out)\\
        \only<2->{\textit{Example}: $X_1, X_2, X_3 \sim Unif(-3,3)$ with $Y = X_1^2 +X_2 - 5X_1X_2 + \epsilon$, $\epsilon \sim \mathcal{N}(0, 5)$\\
        Figure: PDP of DGP (true effect), linear regression model (underfitted), random forest (overfitted), and SVM with radial basis kernel (good fit).
        \begin{center}
            \includegraphics[width=0.8\textwidth]{figure/pitfall1.png}
        \end{center}}
        \item<3> \textbf{Avoid unnecessary complexity}: Prefer simple interpretable models and use them as baseline, move to more complex models if performance not sufficient
    \end{itemize}
\end{frame}


\begin{frame}{Issues of IML method~\citebutton{Molnar et. al (2021)}{https://arxiv.org/abs/2007.04131v2}}
    \begin{itemize}
        \item \textbf{Consider dependencies}: Some interpretation methods have issues in case of dependent features \\
        $\leadsto$ Check presence of dependencies and use suitable interpretation methods\\
        \only<2->{\textit{Example:} Explanations may rely on unreliable pred. where model extrapolated}
    \end{itemize}
    \only<2->{\centerline{\includegraphics[width=0.9\textwidth]{figure/extrapolation}}}
    \only<3->{\begin{columns}[T, totalwidth=\textwidth]
    \begin{column}{0.6\linewidth}
	    \begin{itemize}
        \item \textbf{Beware of simplifications}:
        Mapping of complex models to low-dim. explanations\\
        $\leadsto$ Information loss, e.g., some interpretation methods hide interactions or heterogeneous effects (Figure: PDP and ICE Curves)
    \end{itemize}
	\end{column}
	\begin{column}{0.4\linewidth}
    \centerline{\includegraphics[width=0.8\textwidth]{../03_feature-effects/figure/pdp_xor.pdf}}
    \end{column}
	\end{columns}}
    
\end{frame}


\begin{frame}{Issue: Wrong use of IML method~\citebutton{Molnar et. al (2021)}{https://arxiv.org/abs/2007.04131v2}}
    \begin{itemize}
        \item \textbf{Quantify uncertainty}: Interpretation methods are often (statistical) estimators \\
        $\leadsto$ Beware of uncertainty, we may need confidence intervals\\
        \only<2->{\textit{Example:} Left plot (IML method output) misleading compared to fitted models in right plot \\
        \centerline{\includegraphics[width=0.9\textwidth]{figure/pitfall5}}}
        \item<3> \textbf{Careful with causality}:
        Want to understand the model or the nature of DGP?\\
        $\leadsto$ Goal should guide the choice of interpretation method
    \end{itemize}
\end{frame}


\endlecture
\end{document}
