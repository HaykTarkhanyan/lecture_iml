\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\usepackage[]{graphicx}\usepackage[]{color}
% maxwidth is the original width if it is less than linewidth
% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
\ifdim\Gin@nat@width>\linewidth
\linewidth
\else
\Gin@nat@width
\fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%
\let\hlipl\hlkwb

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
\def\at@end@of@kframe{}%
\ifinner\ifhmode%
\def\at@end@of@kframe{\end{minipage}}%
\begin{minipage}{\columnwidth}%
\fi\fi%
\def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
\colorbox{shadecolor}{##1}\hskip-\fboxsep
% There is no \\@totalrightmargin, so:
\hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
\MakeFramed {\advance\hsize-\width
\@totalleftmargin\z@ \linewidth\hsize
\@setminipage}}%
{\par\unskip\endMakeFramed%
\at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}
\newcommand{\SweaveOpts}[1]{}  % do not interfere with LaTeX
\newcommand{\SweaveInput}[1]{} % because they are not real TeX commands
\newcommand{\Sexpr}[1]{}       % will only be parsed by R

\usepackage[english]{babel}
\usepackage[utf8]{inputenc}

\usepackage{dsfont}
\usepackage{verbatim}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{bm}
\usepackage{csquotes}
\usepackage{multirow}
\usepackage{longtable}
\usepackage{booktabs}
\usepackage{enumerate}
\usepackage[absolute,overlay]{textpos}
\usepackage{psfrag}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{eqnarray}
\usepackage{arydshln}
\usepackage{tabularx}
\usepackage{placeins}
\usepackage{tikz}
\usepackage{setspace}
\usepackage{colortbl}
\usepackage{mathtools}
\usepackage{wrapfig}
\usepackage{bm}

\usetikzlibrary{shapes,arrows,automata,positioning,calc,chains,trees, shadows}
\tikzset{
%Define standard arrow tip
>=stealth',
%Define style for boxes
punkt/.style={
rectangle,
rounded corners,
draw=black, very thick,
text width=6.5em,
minimum height=2em,
text centered},
% Define arrow style
pil/.style={
->,
thick,
shorten <=2pt,
shorten >=2pt,}
}

\usepackage{subfig}

% Defines macros and environments
\input{../../style/common.tex}

%\usetheme{lmu-lecture}
\usepackage{../../style/lmu-lecture}

\let\code=\texttt
\let\proglang=\textsf

\setkeys{Gin}{width=0.9\textwidth}

\title{Interpretable Machine Learning}
% \author{Bernd Bischl, Christoph Molnar, Daniel Schalk, Fabian Scheipl}
\institute{\href{https://compstat-lmu.github.io/lecture_iml/}{compstat-lmu.github.io/lecture\_iml}}
\date{}

\setbeamertemplate{frametitle}{\expandafter\uppercase\expandafter\insertframetitle}

\begin{document}

\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}

\lecturechapter{Introduction and Background}
\lecture{Interpretable Machine Learning}



\begin{vbframe}{Why Interpretability?}

\begin{itemize}
  \item  Machine learning (ML) has a huge potential to aid the decision-making process in various scientific and business applications due to its predictive power.
  \item ML models usually are intransparent black boxes (or rather greyish boxes), 
  e.g., bagged or boosted trees, RBF SVM, deep neural networks.
  \item The lack of explanation hurts trust and creates a barrier for the adoption of ML, especially in critical areas where decisions can affect human life (e.g., medicine).
  \item As a result, a lot of disciplines where trust in the model is critical still rely on traditional statistical models, e.g., generalized linear models (GLMs), with less predictive performance.
\end{itemize}

\end{vbframe}



\begin{vbframe}{Brief History of Interpretability}
\begin{itemize}
\item Linear regression models date back as far as Gauss (1777 - 1855), Legendre (1752 - 1833), and Quetelet (1796 - 1874).
\item Rule-based ML, which covers decision rules and decision trees, has been an active research area since the middle of the 20th century.
\item Active research in the interpretation of modern ML models began in the 2000s, which reused a lot of concepts from sensitivity analysis (SA). Publications in SA date back as far as the 1940s.
\item The built-in feature importance measure of random forests was one of the first important milestones.
\item The deep learning hype in the 2010s resulted in a lot of publications related to explainable AI (XAI).
\item IML as an independent field of research took off around 2015 with novel techniques such as LIME. 
\end{itemize}
\end{vbframe}


\begin{vbframe}{When do we need interpretability?}
\begin{columns}
\begin{column}{0.6\textwidth}
%  \begin{itemize}
%   \item Debugging machine learning models
%   \item Increasing trust in models
%   \item Analyzing newly developed systems with unknown consequences
%   \item Decisions about humans
%   \item Models using proxies instead of causal inputs, e.g. predicting flu outbreaks from google searches.
%   \item When loss function does not cover constraints like fairness (e.g. credit score) or need for insights (e.g. science).
% \end{itemize}
\begin{itemize}
  \item To \textbf{Justify} (and increase trust in models): investigate if and why biased, unexpected or discriminatory predictions were made.
  \item To \textbf{Control}: debug models, identify and correct vulnerabilities and flaws.
  \item To \textbf{Improve}: understanding why a prediction was made makes it easier to improve the model.
  \item To \textbf{Discover}: learn new facts, gather information and gain insights.
\end{itemize}
\end{column}
\begin{column}{0.6\textwidth}  %%<--- here
 \vspace{0.5cm}
 \begin{center}
 \begin{figure}
  \includegraphics[width=0.7\textwidth]{figure_man/explain-to}
  \caption{Reasons for IML.}
 \end{figure}
 \end{center}
\end{column}
\end{columns}
 \lz
\tiny{Doshi-Velez, F., and Kim, B. (2017)}
\tiny{Adadi, Amina, and Mohammed Berrada (2018)}
\end{vbframe}


\begin{vbframe}{What tools do we have?}
 \begin{center}
  \includegraphics[width=0.9\textwidth]{figure_man/overview}
 \end{center}
 $\Rightarrow$ We will focus on model-agnostic interpretability!
\end{vbframe}

\begin{vbframe}{Intrinsic and Model-Agnostic Interpretation}
\begin{itemize}
  \item Intrinsically interpretable models:
  \begin{itemize}
  \item Examples are linear models and decision trees.
  \item They are interpretable because of their simple structures, e.g. 
  weighted combination of feature values or tree structure. 
  \item They are difficult to interpret with many features or complex interaction terms.
  \end{itemize}
  \lz
  \item Model-agnostic interpretation methods:
  \begin{itemize}
  \item They are applied after training (post-hoc).
  \item They also work for more complex black box models.
  \item They can also be applied to intrinsically interpretable models, e.g.
    feature importance for decision trees. 
  \end{itemize}
\end{itemize}
\end{vbframe}

\begin{vbframe}{Model-Agnostic Interpretability}
 \begin{itemize}
  \itemsep2em
  \item Model-agnostic interpretability methods should work for \textbf{any} kind of machine learning model.
  \item Orthogonal combination: explanation type is not tied to the underlying model type.
  \item Often, only access to data and fitted predictor is required. No further knowledge about the model itself is necessary.
  \item We usually distinguish between \textbf{feature effect} and \textbf{feature importance} methods.
 \end{itemize}
\end{vbframe}


\begin{vbframe}{Feature Effects vs. Feature Importance}
\textbf{Feature Effects} visualize or quantify the (average) relationship or contribution of a feature to the model prediction.
\begin{center}
\includegraphics[page=1, width=\textwidth]{figure_man/feature-effects}
\end{center}
  \begin{itemize}
    \item Methods: Partial Dependence Plots, Individual Conditional Expectation, Accumulated Local Effects (ALE)
    \item Pendant in linear models: Regression coefficient $\hat{\theta}_j$
  \end{itemize}
\framebreak

\textbf{Feature importance} methods rank features by how much they contribute to the predictive performance or prediction variance of the model.
\begin{columns}
\begin{column}{0.6\textwidth}
\begin{itemize}
    \itemsep1em
    \item Methods: Permutation Feature Importance, Functional Anova
    \item Analog in linear models: Absolute t-statistic $\left|\frac{\hat{\theta}_j}{SE(\hat{\theta}_j)}\right|$
\end{itemize}
\end{column}
\begin{column}{0.4\textwidth}
\begin{center}
\includegraphics[page=1, width=\textwidth]{figure_man/feature-importance}
\end{center}
\end{column}
\end{columns}
\end{vbframe}


\begin{vbframe}{Global and Local Interpretability}
Global interpretability methods explain the expected model behavior for the entire input space by considering all available observations (or representative subsets). For example:
  \begin{itemize}
    \item Permutation Feature Importance
    \item Partial Dependence Plot
    \item Functional Anova
    \item ...
  \end{itemize}
\lz
Local interpretability methods explain single predictions or a group of similar observations. For example:
 \begin{itemize}
  \item Individual Conditional Expectation (ICE) Plots
  \item Local Interpretable Model-Agnostic Explanations (LIME)
  \item Shapley Values
  \item ...
 \end{itemize}
\end{vbframe}


\begin{vbframe}{Fixed model vs. refits}
  \begin{itemize}
     \itemsep2em
     \item Most methods presented in this lecture analyze a fixed, trained model 
     (e.g. permutation feature importance).
     \item Some methods require refitting the model (e.g. PIMP).
     \item Trained model $\Rightarrow$ Model is the object of analysis.
     \item Refitting $\Rightarrow$ Learning process is the object of analysis.
     \item The advantage of refitting is that it includes information about the variability in the learning process.
  \end{itemize}
\end{vbframe}



\begin{vbframe}{Recap: Interpretable Models}

\begin{itemize}
\item Linear models (LM) and generalized linear models (GLM):
\item Generalized additive models (GAM):
\item Model-based boosting (MBoost):
\item Ruled based ML:
\end{itemize}
\end{vbframe}

\begin{vbframe}{Marginal Effects}
The default way to interpret LMs and GLMs with non-linear feature effects is the marginal effect.
\begin{itemize}
\item A
\end{itemize}

\end{vbframe}


\begin{vbframe}{Additive Decomposition of a Prediction Function}


\end{vbframe}


\endlecture
\end{document}
