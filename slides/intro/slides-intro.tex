\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\usepackage[]{graphicx}\usepackage[]{color}
% maxwidth is the original width if it is less than linewidth
% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
\ifdim\Gin@nat@width>\linewidth
\linewidth
\else
\Gin@nat@width
\fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%
\newcommand{\predvar}{Var\left[\hat{f}(x)\right]}
\let\hlipl\hlkwb

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
\def\at@end@of@kframe{}%
\ifinner\ifhmode%
\def\at@end@of@kframe{\end{minipage}}%
\begin{minipage}{\columnwidth}%
\fi\fi%
\def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
\colorbox{shadecolor}{##1}\hskip-\fboxsep
% There is no \\@totalrightmargin, so:
\hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
\MakeFramed {\advance\hsize-\width
\@totalleftmargin\z@ \linewidth\hsize
\@setminipage}}%
{\par\unskip\endMakeFramed%
\at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}
\newcommand{\SweaveOpts}[1]{}  % do not interfere with LaTeX
\newcommand{\SweaveInput}[1]{} % because they are not real TeX commands
\newcommand{\Sexpr}[1]{}       % will only be parsed by R

\usepackage[english]{babel}
\usepackage[utf8]{inputenc}

\usepackage{dsfont}
\usepackage{verbatim}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{bm}
\usepackage{csquotes}
\usepackage{multirow}
\usepackage{longtable}
\usepackage{booktabs}
\usepackage{enumerate}
\usepackage[absolute,overlay]{textpos}
\usepackage{psfrag}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{eqnarray}
\usepackage{arydshln}
\usepackage{tabularx}
\usepackage{placeins}
\usepackage{tikz}
\usepackage{setspace}
\usepackage{colortbl}
\usepackage{mathtools}
\usepackage{wrapfig}
\usepackage{bm}

\usetikzlibrary{shapes,arrows,automata,positioning,calc,chains,trees, shadows}
\tikzset{
%Define standard arrow tip
>=stealth',
%Define style for boxes
punkt/.style={
rectangle,
rounded corners,
draw=black, very thick,
text width=6.5em,
minimum height=2em,
text centered},
% Define arrow style
pil/.style={
->,
thick,
shorten <=2pt,
shorten >=2pt,}
}

\usepackage{subfig}

% Defines macros and environments
\input{../../style/common.tex}

%\usetheme{lmu-lecture}
\usepackage{../../style/lmu-lecture}

\let\code=\texttt
\let\proglang=\textsf

\setkeys{Gin}{width=0.9\textwidth}

\title{Interpretable Machine Learning}
% \author{Bernd Bischl, Christoph Molnar, Daniel Schalk, Fabian Scheipl}
\institute{\href{https://compstat-lmu.github.io/lecture_iml/}{compstat-lmu.github.io/lecture\_iml}}
\date{}

\setbeamertemplate{frametitle}{\expandafter\uppercase\expandafter\insertframetitle}

\begin{document}

\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}

\lecturechapter{Introduction and Background}
\lecture{Interpretable Machine Learning}



\begin{vbframe}{Why Interpretability?}

\begin{itemize}
  \item  Machine learning (ML) has a huge potential to aid the decision-making process in various scientific and business applications due to its predictive power.
  \item ML models usually are intransparent black boxes (or rather greyish boxes), 
  e.g., bagged or boosted trees, RBF SVM, deep neural networks.
  \item The lack of explanation hurts trust and creates a barrier for the adoption of ML, especially in critical areas where decisions can affect human life (e.g., medicine).
  \item As a result, a lot of disciplines where trust in the model is critical still rely on traditional statistical models, e.g., generalized linear models (GLMs), with less predictive performance.
\end{itemize}

\end{vbframe}



\begin{vbframe}{Brief History of Interpretability}
\begin{itemize}
\item Linear regression models date back as far as Gauss (1777 - 1855), Legendre (1752 - 1833), and Quetelet (1796 - 1874).
\item Rule-based ML, which covers decision rules and decision trees, has been an active research area since the middle of the 20th century.
\item Active research in the interpretation of modern ML models began in the 2000s, which reused a lot of concepts from sensitivity analysis (SA). Publications in SA date back as far as the 1940s.
\item The built-in feature importance measure of random forests was one of the first important milestones.
\item The deep learning hype in the 2010s resulted in a lot of publications related to explainable AI (XAI).
\item IML as an independent field of research took off around 2015 with novel techniques such as LIME. 
\end{itemize}
\end{vbframe}


\begin{vbframe}{When do we need interpretability?}
\begin{columns}
\begin{column}{0.6\textwidth}
%  \begin{itemize}
%   \item Debugging machine learning models
%   \item Increasing trust in models
%   \item Analyzing newly developed systems with unknown consequences
%   \item Decisions about humans
%   \item Models using proxies instead of causal inputs, e.g. predicting flu outbreaks from google searches.
%   \item When loss function does not cover constraints like fairness (e.g. credit score) or need for insights (e.g. science).
% \end{itemize}
\begin{itemize}
  \item To \textbf{Justify} (and increase trust in models): investigate if and why biased, unexpected or discriminatory predictions were made.
  \item To \textbf{Control}: debug models, identify and correct vulnerabilities and flaws.
  \item To \textbf{Improve}: understanding why a prediction was made makes it easier to improve the model.
  \item To \textbf{Discover}: learn new facts, gather information and gain insights.
\end{itemize}
\end{column}
\begin{column}{0.6\textwidth}  %%<--- here
 \vspace{0.5cm}
 \begin{center}
 \begin{figure}
  \includegraphics[width=0.7\textwidth]{figure_man/explain-to}
  \caption{Reasons for IML.}
 \end{figure}
 \end{center}
\end{column}
\end{columns}
 \lz
\tiny{Doshi-Velez, F., and Kim, B. (2017)}
\tiny{Adadi, Amina, and Mohammed Berrada (2018)}
\end{vbframe}


\begin{vbframe}{What tools do we have?}
 \begin{center}
  \includegraphics[width=0.9\textwidth]{figure_man/overview}
 \end{center}
 $\Rightarrow$ We will focus on model-agnostic interpretability!
\end{vbframe}

\begin{vbframe}{Intrinsic and Model-Agnostic Interpretation}
\begin{itemize}
  \item Intrinsically interpretable models:
  \begin{itemize}
  \item Examples are linear models and decision trees.
  \item They are interpretable because of their simple structures, e.g. 
  weighted combination of feature values or tree structure. 
  \item They are difficult to interpret with many features or complex interaction terms.
  \end{itemize}
  \lz
  \item Model-agnostic interpretation methods:
  \begin{itemize}
  \item They are applied after training (post-hoc).
  \item They also work for more complex black box models.
  \item They can also be applied to intrinsically interpretable models, e.g.
    feature importance for decision trees. 
  \end{itemize}
\end{itemize}
\end{vbframe}

\begin{vbframe}{Model-Agnostic Interpretability}
 \begin{itemize}
  \itemsep2em
  \item Model-agnostic interpretability methods should work for \textbf{any} kind of machine learning model.
  \item Orthogonal combination: explanation type is not tied to the underlying model type.
  \item Often, only access to data and fitted predictor is required. No further knowledge about the model itself is necessary.
  \item We usually distinguish between \textbf{feature effect} and \textbf{feature importance} methods.
 \end{itemize}
\end{vbframe}


\begin{vbframe}{Feature Effects vs. Feature Importance}
\textbf{Feature Effects} visualize or quantify the (average) relationship or contribution of a feature to the model prediction.
\begin{center}
\includegraphics[page=1, width=\textwidth]{figure_man/feature-effects}
\end{center}
  \begin{itemize}
    \item Methods: Partial Dependence Plots, Individual Conditional Expectation, Accumulated Local Effects (ALE)
    \item Pendant in linear models: Regression coefficient $\hat{\theta}_j$
  \end{itemize}
\framebreak

\textbf{Feature importance} methods rank features by how much they contribute to the predictive performance or prediction variance of the model.
\begin{columns}
\begin{column}{0.6\textwidth}
\begin{itemize}
    \itemsep1em
    \item Methods: Permutation Feature Importance, Functional Anova
    \item Analog in linear models: Absolute t-statistic $\left|\frac{\hat{\theta}_j}{SE(\hat{\theta}_j)}\right|$
\end{itemize}
\end{column}
\begin{column}{0.4\textwidth}
\begin{center}
\includegraphics[page=1, width=\textwidth]{figure_man/feature-importance}
\end{center}
\end{column}
\end{columns}
\end{vbframe}


\begin{vbframe}{Global and Local Interpretability}
Global interpretability methods explain the expected model behavior for the entire input space by considering all available observations (or representative subsets). For example:
  \begin{itemize}
    \item Permutation Feature Importance
    \item Partial Dependence Plot
    \item Functional Anova
    \item ...
  \end{itemize}
\lz
Local interpretability methods explain single predictions or a group of similar observations. For example:
 \begin{itemize}
  \item Individual Conditional Expectation (ICE) Plots
  \item Local Interpretable Model-Agnostic Explanations (LIME)
  \item Shapley Values
  \item ...
 \end{itemize}
\end{vbframe}


\begin{vbframe}{Fixed model vs. refits}
  \begin{itemize}
     \itemsep2em
     \item Most methods presented in this lecture analyze a fixed, trained model 
     (e.g. permutation feature importance).
     \item Some methods require refitting the model (e.g. PIMP).
     \item Trained model $\Rightarrow$ Model is the object of analysis.
     \item Refitting $\Rightarrow$ Learning process is the object of analysis.
     \item The advantage of refitting is that it includes information about the variability in the learning process.
  \end{itemize}
\end{vbframe}



\begin{vbframe}{Recap: Interpretable Models}


\textbf{Linear models (LM) and generalized linear models (GLM):}

The specification of model parameters makes LMs and GLMs intrinsically interpretable.

\begin{itemize}

\item \textbf{LM:}
$$
\mathbb{E}_Y(y \vert x) = \beta_0 + \beta_1 x_1 + \dots + \beta_p x_p + \epsilon
$$

\item \textbf{GLM:}
$$
g\left(\mathbb{E}_Y(y \vert x)\right) = \beta_0 + \beta_1 x_1 + \dots + \beta_p x_p
$$
\end{itemize}


\textbf{Generalized additive models (GAM):} Add flexibility by replacing linear term by more general functional form, but retain interpretability by keeping the pre-specified additive predictor.

$$
g\left(\mathbb{E}_Y(y \vert x)\right) = \beta_0 + \beta_1 h(x_1) + \dots + \beta_p h(x_p)
$$

\end{vbframe}

\begin{vbframe}{Recap: Interpretable Models}


\textbf{Model-based Boosting:}



\begin{itemize}
\item Model-based boosting is intrinsically interpretable.

\item 
Idea: Combine boosting with interpretable base learners (e.g., an LM). As a result, the prediction is a weighted sum of interpretable terms.

\item
Consider two linear base learners $b_j(x, \Theta)$ and $b_j(x, \Theta^{\star})$ with the same type, but distinct parameter vectors $\Theta$ and $\Theta^{\star}$. It is possible to combine them in a base learner of the same type:

$$
b_j(x, \Theta) + b_j(x, \Theta^{\star}) = b_j(x, \Theta + \Theta^{\star})
$$

\item In each iteration, all base learners are fitted to the so-called pseudo residuals in the current iteration that act as a kind of error of the actual model. A new base learner is then selected by choosing the base learner with the smallest empirical risk.
\end{itemize}



\end{vbframe}

\begin{vbframe}{Recap: Interpretable Models}


\textbf{Rule-based ML:}


Decision rules follow a general structure: IF the conditions are met THEN make a certain prediction. A single decision rule or a combination of several rules can be used to make predictions.

\vspace{0.5cm}
There are many ways to learn rules from data, e.g.,:
\begin{itemize}
\item OneR learns rules from a single feature. OneR is characterized by its simplicity, interpretability and its use as a benchmark.
\item Sequential covering is a general procedure that iteratively learns rules and removes the data points that are covered by the new rule. This procedure is used by many rule learning algorithms.
\item Bayesian Rule Lists combine pre-mined frequent patterns into a decision list using Bayesian statistics. Using pre-mined patterns is a common approach used by many rule learning algorithms.
\end{itemize}

\end{vbframe}

\begin{vbframe}{Marginal Effects}

\begin{itemize}
\item In LMs with interactions, higher order terms, or GLMs, the feature term cannot be interpreted as a direct effect on the predicted outcome.
\item The default way to interpret LMs and GLMs with non-linear feature effects is the marginal effect.
\item Either, we take the derivative of the prediction function w.r.t. a feature, or we evaluate changes in prediction due to an intervention in the data, e.g., by increasing a feature value by one unit:
\begin{align*}
ME_j(x) &= \frac{\partial f(x)}{\partial x_j} \\
ME_j(x, h_j) &= f(x_1, \dots, x_j + h_j, \dots, x_p) - f(x)
\end{align*}
\item Marginal effects can also be used as a model-agnostic technique to interpret any ML model.
\end{itemize}

\end{vbframe}


\begin{vbframe}{Additive Decomposition of a Prediction Function}

\begin{itemize}
\item
Every function can be decomposed into a sum of components of increasing order:
\begin{align*}
\hat{f}(x) &= g_{\{0\}} + g_{\{1\}}(x_1) + g_{\{2\}}(x_2) + \;\dots\; + g_{\{1, 2\}}(x_1, x_2) \\
&\phantom{{}={}} + \;\dots\; + g_{\{1,\ldots,p\}}(x_1, \ldots,x_p)
\end{align*}
\item The features need to be independent to make the decomposition unique.
\item Different techniques to estimate an additive decomposition exist, e.g., its recursive estimation via the partial dependence (PD) or accumulated local effects (ALE).
\item The PD on a subset of features $X_S$ is the minimum L2 loss approximation of the prediction function given $X_S$, denoted by $\mathbb{E}_{X_{-S}}\left[\widehat{f}(x) \; \vert \; X_S \right]$, where $X_{-S}$ is the subset of remaining features.

\end{itemize}
\end{vbframe}

\begin{vbframe}{Additive Decomposition of a Prediction Function}

Consider a recursive estimation via the PD:
\begin{align*}
 g_{\{0\}} &= \mathbb{E}_X\left[\widehat{f}(x)\right] \\
 g_{\{1\}}(x_1) &= \mathbb{E}_{X_{-1}}\left[\widehat{f}(x) \; \vert  \; X_1 \right] - g_{\{0\}} \\
 g_{\{2\}}(x_2) &= \mathbb{E}_{X_{-2}}\left[\widehat{f}(x) \; \vert  \; X_2 \right] - g_{\{0\}} \\
 g_{\{1, 2\}}(x_1, x_2) &= \mathbb{E}_{X_{-\{1,2\}}}\left[\widehat{f}(x) \; \vert \; X_1, X_2 \right] - g_{\{2\}}(x_2) - g_{\{1\}}(x_1) - g_{\{0\}}\\
 &\vdots \\
 g_{\{1, \dots, p\}}(x) &= \widehat{f}(x) - \dots - g_{\{1, 2\}}(x_1, x_2) \\
 &\phantom{{}={}} - g_{\{2\}}(x_2) - g_{\{1\}}(x_1) - g_{\{0\}}\\
\end{align*}

\end{vbframe}

\begin{vbframe}{Functional ANOVA}
\begin{itemize}
\item
After $\hat{f}$ has been decomposed, we may conduct a functional analysis of variance (functional ANOVA / FANOVA)
\begin{align*}
Var\left[\hat{f}(x)\right] &= Var\left[g_{\{0\}} + g_{\{1\}}(x_1) + g_{\{2\}}(x_2) + \;\dots\; + g_{\{1, 2\}}(x_1, x_2) \right. \\
&\phantom{{}={}} \left. + \;\dots\; + g_{\{1,\ldots,p\}}(x) \right]
\end{align*}
\item If the features are independent, the variance can be additively decomposed:
\begin{align*}
Var\left[\hat{f}(x)\right] &= Var\left[g_{\{0\}}\right] + Var\left[g_{\{1\}}(x_1)\right] + Var\left[g_{\{2\}}(x_2)\right] \\
&\phantom{{}={}} + Var\left[g_{\{1, 2\}}(x_1, x_2)\right] + \;\dots\; + Var\left[g_{\{1,\ldots,p\}}(x)\right] 
\end{align*}
\item Dividing by the prediction variance results in the fraction of variance explained by each term:
\begin{align*}
1 &= \frac{Var\left[g_{\{0\}}\right]}{\predvar} + \frac{Var\left[g_{\{1\}}(x_1)\right]}{\predvar} + \frac{Var\left[g_{\{2\}}(x_2)\right]}{\predvar} \\
&\phantom{{}={}} + \frac{Var\left[g_{\{1, 2\}}(x_1, x_2)\right]}{\predvar} + \;\dots\; + \frac{Var\left[g_{\{1,\ldots,p\}}(x)\right]}{\predvar} 
\end{align*}
\item The fraction of variance explained by a set of terms $S = \{1, \dots, s\}$ is referred to as the Sobol index of $S$.
\item The FANOVA decomposition stems from SA and dates back as far as the 1940s. It has been an active area of research ever since, specifically the generalization to dependent features, where neither the decomposition is unique, nor the Sobol indices sum up to 1.
\end{itemize}

\end{vbframe}


\endlecture
\end{document}
