\documentclass[11pt,compress,t,notes=noshow, aspectratio=169, xcolor=table]{beamer}

\usepackage{../../style/lmu-lecture}
% Defines macros and environments
\input{../../style/common.tex}
\newcommand{\open}{}
\newcommand{\close}{}

\title{Interpretable Machine Learning}
% \author{LMU}
%\institute{\href{https://compstat-lmu.github.io/lecture_iml/}{compstat-lmu.github.io/lecture\_iml}}
\date{}

\begin{document}

\newcommand{\titlefigure}{figure/open_blackbox}
\newcommand{\learninggoals}{
\item What are additive decomposition of prediction functions?
\item Why are they useful?
\item How do we obtain them?}

\lecturechapter{Additive Decomposition}
\lecture{Interpretable Machine Learning}
 
\begin{frame}{High-Dimensional Model Representation \\ \citebutton{Sobol (1993)}{https://doi.org/10.1016/S0951-8320(02)00229-6}
\citebutton{Li and Rabitz (2011)}{https://doi.org/10.1007/s10910-011-9898-0}
%\citebutton{Sobol (2003)}{http://www.andreasaltelli.eu/file/repository/sobol1993.pdf}
}
%\citebutton{Li et al. (2001)}{https://doi.org/10.1021/jp010450t}

%\begin{itemize}
%\itemsep1em
%\item Prediction function $\fh: \mathbb{R}^p \mapsto \mathbb{R}$ produces output from $p$-dimensional input space
%decomposed into a hierarchical 
%\item 
\textbf{High-Dimensional Model Representation (HDMR):} Expansion of a square-integrable function $\fh: \mathbb{R}^p \mapsto \mathbb{R}$ into sum of components of different dimensions w.r.t. input features:
%\item \textbf{High-Dimensional Model Representation (HDMR)} of prediction function $\fh: \mathbb{R}^p \mapsto \mathbb{R}$ as function expansion of low dimensional independent terms of increasing order:
%\vspace{5pt}
% \centerline{$\fh(\xv) = g_{\open \emptyset \close} + g_{\open 1 \close}(x_1) + g_{\open 2 \close}(x_2) + \dots + g_{\open 1, 2 \close}(x_1, x_2) + \dots + g_{\open 1,\ldots,p \close}(x_1, \ldots, x_p)$}
%\vspace{5pt}
\begin{equation*}
\begin{split}
\fh(\xv) =  %g_{\open \emptyset \close} +
\textstyle\sum_{S \subseteq \{1,\ldots,p\}} g_{S}(\xv_S) = \; & g_{\open \emptyset \close} + g_{\open 1 \close}(x_1) + g_{\open 2 \close}(x_2) + \dots + g_{\open p \close}(x_p) + \\
& g_{\open 1, 2 \close}(x_1, x_2) + \dots + g_{\open p-1, p \close}(x_{p-1}, x_p) + \dots + \\
& g_{\open 1,\ldots,p \close}(x_1, \ldots, x_p)
\end{split}
\end{equation*}
\vspace{-5pt}%\pause
\begin{itemize}
\item $g_{\open \emptyset \close} \hat = $ Constant mean (intercept) %$\mathbb{E}_X (\fh(\xv)) $
\item $g_{\open j \close} \hat = $ first-order or main effect of $j$-th feature alone on $\fh(\xv)$
%\item Bivariate terms $g_{\open j, k \close} \hat = $ second-order effect of features $j$ and $k$ w.r.t. $\fh(\xv)$%, etc.
\item $g_{S}(\xv_S) \hat = $ $|S|$-order effect, depends \textbf{only} on features in $S$ %$x_j$ for all $j \in S$
\end{itemize}

\textbf{N.B.:} Further constraints on components required to obtain unique and optimal solution
%Without further constraints on components, there is no unique solution.
%\end{itemize}
\end{frame}

\begin{frame}{High-Dimensional Model Representation}

Desirable properties of additive decomposition:

\begin{itemize}
\item Zero-means property (all components are centered): %\\
$\mathbb{E}_{X_S} (g_{S}(\xv_S)) = 0, \forall S \neq \emptyset$
\item Orthogonality (components are independent): %\\
$\mathbb{E}_{X} (g_{V}(\xv_V) g_{S}(\xv_S)) = 0, \forall V \neq S$
\item Variance decomposition:
$ Var[\fh(\xv)] =  \textstyle\sum_{S \subseteq \{1,\ldots,p\}}  Var\left[ g_{S}(\xv_S)\right]$ (under orthogonality)
%effect terms of increasing order 
%HDMR decomposes model into sum of effect terms of increasing order:
% \begin{align*}
% \fh(\xv) &= g_{\open \emptyset \close} + g_{\open 1 \close}(x_1) + g_{\open 2 \close}(x_2) + \;\dots\; + g_{\open 1, 2 \close}(x_1, x_2) \\
% &\phantom{{}={}} + \;\dots\; + g_{\open 1,\ldots,p \close}(x_1, \ldots,x_p)
% \end{align*}
%\item Features need to be independent to make HDMR unique
\end{itemize}

\pause

Different ways to compute components exist, e.g.: % (with slightly different constraints / properties)
		\begin{itemize}
			\item %Recursive expectations (via partial dependence functions) \\
			Functional ANOVA decomposition for \textbf{independent features} \citebutton{Hooker (2004)}{http://portal.acm.org/citation.cfm?doid=1014052.1014122} \\
			$\leadsto$ By recursive expectations of partial dependence functions\\
			$\leadsto$ Satisfies all desirable properties
			\item Generalized functional ANOVA decomposition for \textbf{dependent features} \citebutton{Hooker (2007)}{http://www.tandfonline.com/doi/abs/10.1198/106186007X237892} \\
			$\leadsto$ By minimizing squared error and relaxing the orthogonality property
			\item Accumulated local effect (ALE) decomposition (via finite differences) \citebutton{Zhu and Apley (2020)}{https://doi.org/10.1111/rssb.12377}
		\end{itemize}
%\item N.B.: Without further constraints, components are not unique

\end{frame}


\begin{frame}{Functional ANOVA}

Computation of components via recursive expectations (where $-S = \{1, \ldots, p \} \setminus S$):

$$g_{S}(\xv_S) = \mathbb{E}_{X_{-S}}\left[\fh(\xv) \; \vert  \; x_S \right] - \sum_{V \subset S} g_V(x_V)$$

\begin{itemize}
    \item Expectation integrates $\fh(\xv)$ over all input features except $\xv_S$
    \item Subtract all components $g_V$ with $V \subset S$ to remove all lower-order effects and center the effect
%\pause
\item Recursive computation:
\begin{align*}
 g_{\open \emptyset \close} &= \mathbb{E}_X\left[\fh(\xv)\right] \\
 g_{\open j \close}(x_j) &= \mathbb{E}_{X_{-j}}\left[\fh(\xv) \; \vert  \; x_j \right] - g_{\open \emptyset \close}, \; \forall j \in \{1, \ldots, p\} \\% \text{ and } g_{\open 2 \close}(x_2) = \mathbb{E}_{X_{-2}}\left[\fh(\xv) \; \vert  \; x_2 \right] - g_{\open \emptyset \close}  \\
 %g_{\open 2 \close}(x_2) &= \mathbb{E}_{X_{-2}}\left[\fh(\xv) \; \vert  \; x_2 \right] - g_{\open \emptyset \close} \\
 g_{\open j, k \close}(x_j, x_k) &= \mathbb{E}_{X_{-\{ j,k \}}}\left[\fh(\xv) \; \vert \; x_j, x_k \right] - g_{\open k \close}(x_k) - g_{\open j \close}(x_j) - g_{\open \emptyset \close}, \; \forall j < k\\%,  \text{ etc.}\\
 &\vdots \\
 g_{\open 1, \dots, p \close}(\xv) &= \fh(\xv) - \textstyle\sum_{S \subseteq \{1,\ldots,p-1\}} g_{S}(\xv_S) 
 %g_{\open 1, \dots, p-1 \close}(x_{1}, \dots x_{p-1}) - \dots - g_{\open 1, 2 \close}(x_1, x_2)
 %&\phantom{{}={}} 
 %- g_{\open 2 \close}(x_2) - g_{\open 1 \close}(x_1) - g_{\open \emptyset \close}\\
\end{align*}

\end{itemize}

\end{frame}

\begin{frame}{Functional ANOVA - Example}
Assume $\fh(\xv) = 2 + x_1^2 - x_2^2 + x_1 \cdot x_2$ (e.g., if $x_1 = 5$ and $x_2 = 10$ $\Rightarrow$ $\fh(\xv) = -23$)

\begin{itemize}
    \item Computation of components using feature values $x_1 = x_2 = (-10, -9, \ldots, 10)^\top$ gives
    \begin{figure}
    \includegraphics[width = 0.7 \textwidth]{figure/interaction2}
    \end{figure}
    \item Checking values of components for $x_1 = 5$ and $x_2 = 10$:\\
    $g_{\open \emptyset \close} = 2$, $g_{\open 1 \close} = -9.67$, $g_{\open 2 \close} = -65.33$, $g_{\open 1,2 \close} = 50$ $\Rightarrow$ $\fh(\xv) = -23$
\end{itemize} 


\end{frame}

% \begin{frame}{High-Dimensional Model Representation}

% \begin{itemize}
% \itemsep1em
%     \item Even if your goal is not to decompose $\hat{f}(\xv)$, it is useful to keep in mind that a decomposition exists.
%     \item When interpreting a model, one is often interested in feature effects of various orders, e.g., first-order and second-order effects.
%     \item One may assume that the relevance of effects decreases with increasing order, e.g., that the model can be approximated via first-order and second-order effects.
%     \item Do not confuse the outputs of interpretation methods such as the ICE or PD with the effect terms of the additive decomposition!
%     \\$\rightarrow$ A second-order effect is the sole interaction effect on the target after a constant effect or main effects have been removed.
% \end{itemize}
% \end{frame}


\begin{frame}{Variance decomposition}

\begin{itemize}
\item Decomposition of $\hat{f}(\xv)$ allows to conduct functional analysis of variance (fANOVA):
$$ Var\left[\hat{f}(\xv)\right] = Var\left[g_{\open \emptyset \close} + g_{\open 1 \close}(x_1) + \;\dots\; + g_{\open 1, 2 \close}(x_1, x_2) + \;\dots\; + g_{\open 1,\ldots,p \close}(\xv) \right]$$
% \begin{align*}
% Var\left[\hat{f}(\xv)\right] &= Var\left[g_{\open \emptyset \close} + g_{\open 1 \close}(x_1) + g_{\open 2 \close}(x_2) + \;\dots\; + g_{\open 1, 2 \close}(x_1, x_2) \right. \\
% &\phantom{{}={}} \left. + \;\dots\; + g_{\open 1,\ldots,p \close}(\xv) \right]
% \end{align*}
\item If features are independent $\Rightarrow$ Variance can be additively decomposed without covariances:
$$Var\left[\hat{f}(\xv)\right] = Var\left[g_{\open \emptyset \close}\right] + Var\left[g_{\open 1 \close}(x_1)\right] + \;\dots\; + Var\left[g_{\open 1, 2 \close}(x_1, x_2)\right] + \;\dots\; + Var\left[g_{\open 1,\ldots,p \close}(\xv)\right]$$
% \begin{align*}
% Var\left[\hat{f}(\xv)\right] &= Var\left[g_{\open \emptyset \close}\right] + Var\left[g_{\open 1 \close}(x_1)\right] + Var\left[g_{\open 2 \close}(x_2)\right] \\
% &\phantom{{}={}} + Var\left[g_{\open 1, 2 \close}(x_1, x_2)\right] + \;\dots\; + Var\left[g_{\open 1,\ldots,p \close}(\xv)\right]
% \end{align*}
% \end{itemize}
% \end{frame}

% \begin{frame}{Variance decomposition}

% \begin{itemize}
\item Dividing by the prediction variance, yields fraction of variance explained by each term:
\begin{align*}
1 &= \frac{Var\left[g_{\open \emptyset \close}\right]}{\predvar} + \frac{Var\left[g_{\open 1 \close}(x_1)\right]}{\predvar} + %\frac{Var\left[g_{\open 2 \close}(x_2)\right]}{\predvar} \\
%&\phantom{{}={}} 
\;\dots\;
+ \frac{Var\left[g_{\open 1, 2 \close}(x_1, x_2)\right]}{\predvar} + \;\dots\; + \frac{Var\left[g_{\open 1,\ldots,p \close}(\xv)\right]}{\predvar}
\end{align*}

\item Fraction of variance explained by a component $g_{\open S \close}(\xv_S)$ is the Sobol index:
$$
S_S = \frac{Var\left[g_{\open S \close}(\xv_S)\right]}{Var\left[\hat{f}(\xv)\right]}
$$
\end{itemize}

\end{frame}


\endlecture
\end{document}
