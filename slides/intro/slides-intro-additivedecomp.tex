\documentclass[11pt,compress,t,notes=noshow, aspectratio=169, xcolor=table]{beamer}

\usepackage{../../style/lmu-lecture}
% Defines macros and environments
\input{../../style/common.tex}

\title{Interpretable Machine Learning}
% \author{LMU}
%\institute{\href{https://compstat-lmu.github.io/lecture_iml/}{compstat-lmu.github.io/lecture\_iml}}
\date{}

\begin{document}

\newcommand{\titlefigure}{figure/open_blackbox}
\newcommand{\learninggoals}{
\item What are additive decomposition of prediction functions?
\item Why are they useful?
\item How do we obtain them?}

\lecturechapter{Additive Decomposition}
\lecture{Interpretable Machine Learning}
 
\begin{frame}{High-Dimensional Model Representation (HDMR)}

\begin{itemize}
\itemsep1em
\item
HDMR decomposes model into sum of effect terms of increasing order:
\begin{align*}
\hat{f}(x) &= g_{\{0\}} + g_{\{1\}}(x_1) + g_{\{2\}}(x_2) + \;\dots\; + g_{\{1, 2\}}(x_1, x_2) \\
&\phantom{{}={}} + \;\dots\; + g_{\{1,\ldots,p\}}(x_1, \ldots,x_p)
\end{align*}
\item Univariate terms $g_{\{j\}} \hat = $ first-order or main effects
\item Bivariate terms $g_{\{j, k\}} \hat = $ second-order effects, etc.
\item Features need to be independent to make HDMR unique
\item Different techniques to estimate additive decomposition exist, e.g.:
		\begin{itemize}
			\item recursive expectations (via partial dependence) 
			\item accumulated local effects (ALE)
		\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{High-Dimensional Model Representation}

Estimation of effect terms via recursive expectations (based on partial dependence, see later):
\begin{align*}
 g_{\{0\}} &= \mathbb{E}_X\left[\widehat{f}(x)\right] \\
 g_{\{1\}}(x_1) &= \mathbb{E}_{X_{-1}}\left[\widehat{f}(x) \; \vert  \; X_1 \right] - g_{\{0\}} \\
 g_{\{2\}}(x_2) &= \mathbb{E}_{X_{-2}}\left[\widehat{f}(x) \; \vert  \; X_2 \right] - g_{\{0\}} \\
 g_{\{1, 2\}}(x_1, x_2) &= \mathbb{E}_{X_{-\{1,2\}}}\left[\widehat{f}(x) \; \vert \; X_1, X_2 \right] - g_{\{2\}}(x_2) - g_{\{1\}}(x_1) - g_{\{0\}}\\
 &\vdots \\
 g_{\{1, \dots, p\}}(x) &= \widehat{f}(x) - \dots - g_{\{1, 2\}}(x_1, x_2) \\
 &\phantom{{}={}} - g_{\{2\}}(x_2) - g_{\{1\}}(x_1) - g_{\{0\}}\\
\end{align*}

\end{frame}

% \begin{frame}{High-Dimensional Model Representation}

% \begin{itemize}
% \itemsep1em
%     \item Even if your goal is not to decompose $\hat{f}(x)$, it is useful to keep in mind that a decomposition exists.
%     \item When interpreting a model, one is often interested in feature effects of various orders, e.g., first-order and second-order effects.
%     \item One may assume that the relevance of effects decreases with increasing order, e.g., that the model can be approximated via first-order and second-order effects.
%     \item Do not confuse the outputs of interpretation methods such as the ICE or PD with the effect terms of the additive decomposition!
%     \\$\rightarrow$ A second-order effect is the sole interaction effect on the target after a constant effect or main effects have been removed.
% \end{itemize}
% \end{frame}


\begin{frame}{Functional ANOVA}

\begin{itemize}
\item After $\hat{f}(x)$ has been decomposed, we can conduct a functional analysis of variance (fANOVA):
\begin{align*}
Var\left[\hat{f}(x)\right] &= Var\left[g_{\{0\}} + g_{\{1\}}(x_1) + g_{\{2\}}(x_2) + \;\dots\; + g_{\{1, 2\}}(x_1, x_2) \right. \\
&\phantom{{}={}} \left. + \;\dots\; + g_{\{1,\ldots,p\}}(x) \right]
\end{align*}
\item If features are independent $\Rightarrow$ Variance can be additively decomposed without covariances:
\begin{align*}
Var\left[\hat{f}(x)\right] &= Var\left[g_{\{0\}}\right] + Var\left[g_{\{1\}}(x_1)\right] + Var\left[g_{\{2\}}(x_2)\right] \\
&\phantom{{}={}} + Var\left[g_{\{1, 2\}}(x_1, x_2)\right] + \;\dots\; + Var\left[g_{\{1,\ldots,p\}}(x)\right]
\end{align*}
\end{itemize}
\end{frame}

\begin{frame}{Functional ANOVA}

\begin{itemize}
\item Dividing by the prediction variance, yields fraction of variance explained by each term:
\begin{align*}
1 &= \frac{Var\left[g_{\{0\}}\right]}{\predvar} + \frac{Var\left[g_{\{1\}}(x_1)\right]}{\predvar} + \frac{Var\left[g_{\{2\}}(x_2)\right]}{\predvar} \\
&\phantom{{}={}} + \frac{Var\left[g_{\{1, 2\}}(x_1, x_2)\right]}{\predvar} + \;\dots\; + \frac{Var\left[g_{\{1,\ldots,p\}}(x)\right]}{\predvar}
\end{align*}

\item Fraction of variance explained by a term is the Sobol index:
$$
S_j = \frac{Var\left[g_{\{j\}}(x_j)\right]}{Var\left[\hat{f}(x)\right]}
$$
\end{itemize}

\end{frame}


\endlecture
\end{document}
