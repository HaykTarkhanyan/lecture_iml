\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\usepackage[]{graphicx}\usepackage[]{color}
% maxwidth is the original width if it is less than linewidth
% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
\ifdim\Gin@nat@width>\linewidth
\linewidth
\else
\Gin@nat@width
\fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%
\newcommand{\predvar}{Var\left[\hat{f}(x)\right]}
\let\hlipl\hlkwb

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
\def\at@end@of@kframe{}%
\ifinner\ifhmode%
\def\at@end@of@kframe{\end{minipage}}%
\begin{minipage}{\columnwidth}%
\fi\fi%
\def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
\colorbox{shadecolor}{##1}\hskip-\fboxsep
% There is no \\@totalrightmargin, so:
\hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
\MakeFramed {\advance\hsize-\width
\@totalleftmargin\z@ \linewidth\hsize
\@setminipage}}%
{\par\unskip\endMakeFramed%
\at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}
\newcommand{\SweaveOpts}[1]{}  % do not interfere with LaTeX
\newcommand{\SweaveInput}[1]{} % because they are not real TeX commands
\newcommand{\Sexpr}[1]{}       % will only be parsed by R

\usepackage[english]{babel}
\usepackage[utf8]{inputenc}

\usepackage{dsfont}
\usepackage{verbatim}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{bm}
\usepackage{csquotes}
\usepackage{multirow}
\usepackage{longtable}
\usepackage{booktabs}
\usepackage{enumerate}
\usepackage[absolute,overlay]{textpos}
\usepackage{psfrag}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{eqnarray}
\usepackage{arydshln}
\usepackage{tabularx}
\usepackage{placeins}
\usepackage{tikz}
\usepackage{setspace}
\usepackage{colortbl}
\usepackage{mathtools}
\usepackage{wrapfig}
\usepackage{bm}

\usetikzlibrary{shapes,arrows,automata,positioning,calc,chains,trees, shadows}
\tikzset{
%Define standard arrow tip
>=stealth',
%Define style for boxes
punkt/.style={
rectangle,
rounded corners,
draw=black, very thick,
text width=6.5em,
minimum height=2em,
text centered},
% Define arrow style
pil/.style={
->,
thick,
shorten <=2pt,
shorten >=2pt,}
}

\usepackage{subfig}

% Defines macros and environments
\input{../../style/common.tex}

%\usetheme{lmu-lecture}
\usepackage{../../style/lmu-lecture}

\let\code=\texttt
\let\proglang=\textsf

\setkeys{Gin}{width=0.9\textwidth}

\title{Interpretable Machine Learning}
% \author{Bernd Bischl, Christoph Molnar, Daniel Schalk, Fabian Scheipl}
\institute{\href{https://compstat-lmu.github.io/lecture_iml/}{compstat-lmu.github.io/lecture\_iml}}
\date{}

\setbeamertemplate{frametitle}{\expandafter\uppercase\expandafter\insertframetitle}

\begin{document}

\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}
\newcommand{\titlefigure}{figure/me_movement}
\newcommand{\learninggoals}{
\item Why parameter-based interpretations are not always possible for parametric models
\item How marginal effects can be used in such cases
\item Drawbacks of marginal effects
\item Model-agnostic applicability}


\lecturechapter{Marginal Effects}
\lecture{Interpretable Machine Learning}

\begin{vbframe}{Interpretations of Linear Models}

\begin{itemize}
\setlength\itemsep{2em}
\item The LM can be directly interpreted by evaluating the model coefficients:
\begin{equation*}
y = \beta_0 + \beta_1 x_1 + \dots + \beta_p x_p + \dots + \epsilon
\end{equation*}
\item A change in $x_1$ by $\Delta x_1$ results in a change in $y$ by $\Delta y = \Delta x_1 \cdot \beta_1$.
\item Default interpretations correspond to $\Delta x_1 = 1$, i.e., $\Delta y = \beta$. 
\item All interpretations are done ceteris paribus, i.e., all remaining features are kept constant.
\end{itemize}
\end{vbframe}

\begin{vbframe}{Interpretations of Polynomial Models}


\begin{itemize}
\setlength\itemsep{2em}
\item If higher-order terms or interactions are present, parameter-based interpretations are not possible anymore:
\begin{equation}
y = \beta_0 + \beta_{1} x_1^2 + \beta_{2} x_2^2 + \beta_{1, 2} x_1, x_2 + \epsilon
\label{eq:poly_model}
\end{equation}
\item The isolated main effects of both features vary across different values
\item The interaction depends on values of the remaining feature
\item The marginal effect (ME) allows us to determine a feature effect nonetheless. 
\end{itemize}

\end{vbframe}

\begin{vbframe}{Marginal Effects}

\begin{itemize}
\setlength\itemsep{2em}
\item
The most common definition of the marginal effect (ME) corresponds to the derivative of the prediction function w.r.t. a feature. We refer to this variant as the derivative ME (dME):
\begin{equation*}
dME_j(x) = \frac{\partial f(x)}{\partial x_j}
\end{equation*}
\item 
A less commonly known definition corresponds to the change in predicted outcome due to an intervention in the data, e.g., by increasing a feature value by one unit. As this variant corresponds to a forward difference, we refer to it as a forward ME (fME):
\begin{equation*}
fME_j(x, h_j) = f(x_1, \dots, x_j + h_j, \dots, x_p) - f(x)
\end{equation*}
%\item For Eq. (\ref{eq:poly_model}), the dME and fME of $x_1$ with step size 1 correspond to:
%\begin{align*}
%dME_j(x) &= 2\beta_1 x_1 +  \beta_{1, 2} x_2 \\
%fME_j(x, h_j) &= 2\beta_1 h_1^2 +  \beta_{1, 2} x_2 h_1
%\end{align*}
\end{itemize}
\end{vbframe}

\begin{vbframe}{Derivative versus Forward Difference}

\begin{itemize}
\setlength\itemsep{2em}
\item The dME is not suited to interpret non-linear prediction functions, as the derivative of the prediction function at one point may be substantially different at another.
\item The fME is better suited for non-linear prediction functions. It essentially corresponds to a movement on the prediction function, indicating changes in predicted outcome regardless of the function's shape.
\item However, with both variants, we lose information about the prediction function along the finite difference.
\end{itemize}
\end{vbframe}

\begin{vbframe}{Derivative versus Forward Difference}
\begin{figure}
  \includegraphics[width = 0.65\textwidth]{figure/derivative_me_error.png}
\end{figure}
\end{vbframe}

\begin{vbframe}{Additive Recovery}

\begin{itemize}
\setlength\itemsep{2em}
\item Due being based on a finite difference, both variants only recover terms within the prediction function that depend on the feature(s) of interest.
\item Consider a prediction function $\widehat{f}(x) = ax_1 + bx_2$. It follows that:
\begin{align*}
dME_1(x) &= a \\
fME_1(x, h_1) &= ah_1
\end{align*}
\item The ME removes effects of other features that are linked additively, regardless of their number and effect structure.
\end{itemize}

\end{vbframe}


\begin{vbframe}{Model-Agnostic Applicability}

\end{vbframe}








\endlecture
\end{document}
