\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\usepackage[]{graphicx}\usepackage[]{color}
% maxwidth is the original width if it is less than linewidth
% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
\ifdim\Gin@nat@width>\linewidth
\linewidth
\else
\Gin@nat@width
\fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%
\newcommand{\predvar}{Var\left[\hat{f}(x)\right]}
\let\hlipl\hlkwb

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
\def\at@end@of@kframe{}%
\ifinner\ifhmode%
\def\at@end@of@kframe{\end{minipage}}%
\begin{minipage}{\columnwidth}%
\fi\fi%
\def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
\colorbox{shadecolor}{##1}\hskip-\fboxsep
% There is no \\@totalrightmargin, so:
\hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
\MakeFramed {\advance\hsize-\width
\@totalleftmargin\z@ \linewidth\hsize
\@setminipage}}%
{\par\unskip\endMakeFramed%
\at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}
\newcommand{\SweaveOpts}[1]{}  % do not interfere with LaTeX
\newcommand{\SweaveInput}[1]{} % because they are not real TeX commands
\newcommand{\Sexpr}[1]{}       % will only be parsed by R

\usepackage[english]{babel}
\usepackage[utf8]{inputenc}

\usepackage{dsfont}
\usepackage{verbatim}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{bm}
\usepackage{csquotes}
\usepackage{multirow}
\usepackage{longtable}
\usepackage{booktabs}
\usepackage{enumerate}
\usepackage[absolute,overlay]{textpos}
\usepackage{psfrag}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{eqnarray}
\usepackage{arydshln}
\usepackage{tabularx}
\usepackage{placeins}
\usepackage{tikz}
\usepackage{setspace}
\usepackage{colortbl}
\usepackage{mathtools}
\usepackage{wrapfig}
\usepackage{bm}

\usetikzlibrary{shapes,arrows,automata,positioning,calc,chains,trees, shadows}
\tikzset{
%Define standard arrow tip
>=stealth',
%Define style for boxes
punkt/.style={
rectangle,
rounded corners,
draw=black, very thick,
text width=6.5em,
minimum height=2em,
text centered},
% Define arrow style
pil/.style={
->,
thick,
shorten <=2pt,
shorten >=2pt,}
}

\usepackage{subfig}
\usepackage{../../style/lmu-lecture}
% Defines macros and environments
\input{../../style/common.tex}

%\usetheme{lmu-lecture}
\usepackage{../../style/lmu-lecture}

\let\code=\texttt
\let\proglang=\textsf

\setkeys{Gin}{width=0.9\textwidth}

\title{Interpretable Machine Learning}
% \author{Bernd Bischl, Christoph Molnar, Daniel Schalk, Fabian Scheipl}
\institute{\href{https://compstat-lmu.github.io/lecture_iml/}{compstat-lmu.github.io/lecture\_iml}}
\date{}

\setbeamertemplate{frametitle}{\expandafter\uppercase\expandafter\insertframetitle}

\begin{document}

\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}
\newcommand{\titlefigure}{figure/open_blackbox}
\newcommand{\learninggoals}{
\item Why do we need interpretability?
\item What have been the developments until now?}


\lecturechapter{Introduction, Motivation and History}
\lecture{Interpretable Machine Learning}



	\begin{vbframe}{Why Interpretability?}
		
		\begin{itemize}
			\item Due to its predictive power, machine learning (ML) has a huge potential to aid the decision-making process in many applications.
			\smallskip
			\item ML models often are intransparent black boxes, e.g., XGBoost, RBF SVM, or NNs.
			\begin{itemize}
				\item[$\rightarrow$] too complex to be understood by humans.
			\end{itemize}
			\smallskip
			\item A lack in explanations diminishes trust in the model and creates barriers for adoption, especially in areas with critical decision-making consequences, e.g., medicine.
			\smallskip
			\item Such disciplines often rely on traditional models,\\ e.g., linear models, with less predictive performance.
			\smallskip
			\item Interpretable machine learning (IML) aims to bridge the gap between interpretability and predictive performance.
		\end{itemize}
		
	\end{vbframe}
	
	%-----------------------------------------------------------------------------------------------------------------------------	
	
	\begin{vbframe}{Brief History of Interpretability}
		\begin{itemize}
			\item 18th and 19th century: linear regression models (Gauss, Legendre, Quetelet)
			\medskip
			\item 1940s: emergence of sensitivity analysis (SA).
			\medskip
			\item Middle of 20th century: Rule-based ML, incl. decision rules and decision trees.
			\medskip
			\item 2001: built-in feature importance measure of random forests.
			\medskip
			\item >2010: Explainable AI (XAI) for deep learning.
			\medskip
			\item >2015: IML as an independent field of research .
		\end{itemize}
	\end{vbframe}
	
	%-----------------------------------------------------------------------------------------------------------------------------
\begin{vbframe}{When do we need interpretability?}
\begin{columns}
\begin{column}{0.6\textwidth}
%  \begin{itemize}
%   \item Debugging machine learning models
%   \item Increasing trust in models
%   \item Analyzing newly developed systems with unknown consequences
%   \item Decisions about humans
%   \item Models using proxies instead of causal inputs, e.g. predicting flu outbreaks from google searches.
%   \item When loss function does not cover constraints like fairness (e.g. credit score) or need for insights (e.g. science).
% \end{itemize}
\begin{itemize}
  \item To \textbf{Justify} (and increase trust in models): investigate if and why biased, unexpected or discriminatory predictions were made.
  \item To \textbf{Control}: debug models, identify and correct vulnerabilities and flaws.
  \item To \textbf{Improve}: understanding why a prediction was made, makes it easier to improve the model.
  \item To \textbf{Discover}: learn new facts, gather information and gain insights.
\end{itemize}
\end{column}
\begin{column}{0.4\textwidth}  %%<--- here
 \vspace{0.5cm}
 \begin{center}
 
 \begin{figure}
  \includegraphics[width=\textwidth]{figure/explain-to}
 \end{figure}
 \end{center}
\end{column}
\end{columns}
     \lz
    \footnote[frame]{Doshi-Velez, F., \& Kim, B. (2017). Towards A Rigorous Science of Interpretable Machine Learning. arXiv: 1702.08608.}
    \footnote[frame]{A. Adadi and M. Berrada, "Peeking Inside the Black-Box: A Survey on Explainable Artificial Intelligence (XAI)," in IEEE Access, vol. 6, pp. 52138-52160, 2018.}
\end{vbframe}



\begin{vbframe}{Why is Interpretability Important?}
	
	\begin{itemize}
	    \item Machine learning is (mostly) about discovering patterns in data.
	    \medskip
	    \item Unfortunately, it is not guaranteed that ML will identify the correct patterns.
	    
	    \medskip
	    \item We humans might not be able to discover patterns ML models discovered.
	    \begin{itemize}
	        \item Good for science or to get new insights.
	        \item Bad in applications where unexpected behavior is not desired.
	    \end{itemize}
	    \medskip
	    
	    \item \alert{How can you check whether the model is correct in its inference?}
	\end{itemize}
	
\end{vbframe}

% \begin{vbframe}{Clever Hans \footnote{Lapuschkin et al. 2019}{https://www.nature.com/articles/s41467-019-08987-4}}
	
% 	\centering
% 	\includegraphics[width=0.6\textwidth]{figure/horse_without_label.PNG}
	
% \end{vbframe}

% \begin{vbframe}{Clever Hans \footnote{Lapuschkin et al. 2019}{https://www.nature.com/articles/s41467-019-08987-4}}
	
% 	\centering
% 	\includegraphics[width=0.6\textwidth]{figure/horse_with_label.PNG}
	
% \end{vbframe}

% \begin{vbframe}{Clever Hans \footnote{Lapuschkin et al. 2019}{https://www.nature.com/articles/s41467-019-08987-4}}
	
% 	\centering
% 	\includegraphics[width=0.6\textwidth]{figure/horse_map_with_label.PNG}
	
% \end{vbframe}

% \begin{vbframe}{Clever Hans \footnote{Lapuschkin et al. 2019}{https://www.nature.com/articles/s41467-019-08987-4}}
	
% 	\centering
% 	\includegraphics[width=0.6\textwidth]{figure/horse_map_without_label.PNG}
	
% \end{vbframe}

% \begin{vbframe}{Clever Hans \footnote{Lapuschkin et al. 2019}{https://www.nature.com/articles/s41467-019-08987-4}}
	
% 	\centering
% 	\includegraphics[width=0.6\textwidth]{figure/boats_maps.PNG}
	
% \end{vbframe}

% \begin{vbframe}{COMPASS}

%     \begin{itemize}
%         \item Correctional Offender Management Profiling for Alternative Sanctions 
%         \item predict recidivism risk
%         \begin{itemize}
%             \item i.e., criminal re-offense after previous crime, resulting in jail booking
%             \item different risk levels: high risk, medium risk or low risk
%         \end{itemize}
%         \item evaluation based on a questionnaire the defendant has to answer
%     \end{itemize}	
	
% \end{vbframe}

% \begin{vbframe}{COMPAS Model Analysis~\footnote{Larson et al. 2016}{https://www.propublica.org/article/how-we-analyzed-the-compas-recidivism-algorithm}}
    
%     \centering
%     \includegraphics[width=0.7\textwidth]{figure/compass_black_white.PNG}
	
% 	$\leadsto$ Strong indication that the model is discriminating black defendants
	
% \end{vbframe}

% \begin{vbframe}{Examples ML failed in}

%     \begin{itemize}
%         \item Credit and insurance scoring
        
%         \item Medical applications
%         \begin{itemize}
%             \item Diagnosis
%             \item Chance of recovery
%             \item Treatment recommendations
%         \end{itemize}
        
%         \item Crime predictions
        
%         \item Rating job applications
        
%         \item \ldots
%     \end{itemize}

    
%     \bigskip

%     $\leadsto$ GDPR (aka DSGVO) requires that for some applications predictive models have to be explainable        
	
% \end{vbframe}



\begin{vbframe}{Motivation - Adversarial Examples}
    
    \begin{center}
    \includegraphics[width=0.9\textwidth]{figure/panda-airplane.pdf}
    \end{center}
	\bigskip
	
	$\rightarrow$ ML Models might not capture human-like understanding.        
    \footnote[frame]{Goodfellow, Ian \& Shlens, Jonathon \& Szegedy, Christian. (2014). Explaining and Harnessing Adversarial Examples. arXiv 1412.6572.}
\end{vbframe}

	
\begin{vbframe}{Adversarial Noise}
    \begin{center}
    \includegraphics[width=0.8\textwidth]{figure/adv-noise.pdf}    \footnote[frame]{Goodfellow, Ian \& Shlens, Jonathon \& Szegedy, Christian. (2014). Explaining and Harnessing Adversarial Examples. arXiv 1412.6572.}
    % https://arxiv.org/pdf/1412.6572.pdf
	\end{center}
	\normalsize
	\bigskip
	$\rightarrow$\textbf{Adversarial Noise:} Noise that is imperceptible to \textbf{humans}\\ but results in incorrect classification results.
\end{vbframe}

\endlecture
\end{document}
