\documentclass[11pt,compress,t,notes=noshow, aspectratio=169, xcolor=table]{beamer}

\usepackage{../../style/lmu-lecture}
% Defines macros and environments
\input{../../style/common.tex}

\title{Interpretable Machine Learning}
% \author{LMU}
%\institute{\href{https://compstat-lmu.github.io/lecture_iml/}{compstat-lmu.github.io/lecture\_iml}}
\date{}

\begin{document}

\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}
\newcommand{\titlefigure}{figure/open_blackbox}
\newcommand{\learninggoals}{
\item Why do we need interpretability?
\item What have been the developments until now?}


\lecturechapter{Introduction, Motivation and History}
\lecture{Interpretable Machine Learning}



\begin{frame}{Why Interpretability?}
		
% 		\begin{itemize}
% 			\item Machine learning (ML) has a huge potential to aid the decision-making process in various  applications.
% 			\pause
% 			\smallskip
% 			\item ML models often are intransparent black boxes, e.g., XGBoost, RBF SVM, or NNs.
% 			\begin{itemize}
% 				\item[$\rightarrow$] too complex to be understood by humans.
% 			\end{itemize}
% 			\smallskip
% 			\item A lack in explanations diminishes trust in the model and creates barriers for adoption, especially in areas with critical decision-making consequences, e.g., medicine.
% 			\smallskip
% 			\item Such disciplines often rely on traditional models,\\ e.g., linear models, with less predictive performance.
% 			\smallskip
% 			\item Interpretable machine learning (IML) aims to bridge the gap between interpretability and predictive performance.
% 		\end{itemize}
		
		\begin{itemize}
			\item Machine Learning (ML) has a huge potential to aid the decision-making process in various scientific and business applications due to its predictive power.
			\pause
			\smallskip
			\item ML models usually are intransparent black boxes, e.g., XGBoost, RBF SVM or DNNs.
			\begin{itemize}
				\item[$\leadsto$] too complex to be understood by humans
			\end{itemize}
			\pause
			\smallskip
			\item The lack of explanation
			\begin{enumerate}
				\item hurts trust
				\item creates barriers
			\end{enumerate}  
			\pause
			\smallskip
		    \item[$\leadsto$] Harder to adapt for critical areas with decisions affecting human life (e.g., medicine or credits).
			\pause
			%\smallskip
			\item[$\leadsto$] Many disciplines with required trust rely on traditional models,\\ e.g., linear models, with less predictive performance.
		\end{itemize}
	\end{frame}
	
	%-----------------------------------------------------------------------------------------------------------------------------	
	
	\begin{frame}{Brief History of Interpretability}
		\begin{itemize}
			\item 18th and 19th century: linear regression models (Gauss, Legendre, Quetelet)
			\medskip
			\item 1940s: emergence of sensitivity analysis (SA).
			\medskip
			\item Middle of 20th century: Rule-based ML, incl. decision rules and decision trees.
			\medskip
			\item 2001: built-in feature importance measure of random forests.
			\medskip
			\item >2010: Explainable AI (XAI) for deep learning.
			\medskip
			\item >2015: IML as an independent field of research .
		\end{itemize}
	\end{frame}
	
	%-----------------------------------------------------------------------------------------------------------------------------
\begin{frame}{When do we need interpretability?}
\begin{columns}[T]
\begin{column}{0.55\textwidth}
%  \begin{itemize}
%   \item Debugging machine learning models
%   \item Increasing trust in models
%   \item Analyzing newly developed systems with unknown consequences
%   \item Decisions about humans
%   \item Models using proxies instead of causal inputs, e.g. predicting flu outbreaks from google searches.
%   \item When loss function does not cover constraints like fairness (e.g. credit score) or need for insights (e.g. science).
% \end{itemize}
\begin{itemize}
  \item To \textbf{Justify} (and increase trust in models): investigate if and why biased, unexpected or discriminatory predictions were made. 	\pause
  \item To \textbf{Control}: debug models, identify and correct vulnerabilities and flaws. 	\pause
  \item To \textbf{Improve}: understanding why a prediction was made, makes it easier to improve the model. 	\pause
  \item To \textbf{Discover}: learn new facts, gather information and gain insights.
\end{itemize}
\end{column}
\begin{column}{0.4\textwidth}  %%<--- here
 %\vspace{0.5cm}
%  \begin{center}
%  \begin{figure}
  \includegraphics[width=0.9\textwidth]{figure/explain-to}
%  \end{figure}
%  \end{center}
\end{column}
\end{columns}
     \lz
    \footnote[frame]{Doshi-Velez, F., \& Kim, B. (2017). Towards A Rigorous Science of Interpretable Machine Learning. arXiv: 1702.08608.}
    \footnote[frame]{A. Adadi and M. Berrada, "Peeking Inside the Black-Box: A Survey on Explainable Artificial Intelligence (XAI)," in IEEE Access, vol. 6, pp. 52138-52160, 2018.}
\end{frame}



\begin{frame}{Why is Interpretability Important?}
	
	\begin{itemize}
	    \item Machine learning is (mostly) about discovering patterns in data.
	    \medskip
	    \item Unfortunately, it is not guaranteed that ML will identify the correct patterns.
	    
	    \medskip
	    \item We humans might not be able to discover patterns ML models discovered.
	    \begin{itemize}
	        \item Good for science or to get new insights.
	        \item Bad in applications where unexpected behavior is not desired.
	    \end{itemize}
	    \medskip
	    
	    \item \alert{How can you check whether the model is correct in its inference?}
	\end{itemize}
	
\end{frame}

\begin{frame}{Clever Hans \citebutton{Lapuschkin et al. 2019}{https://www.nature.com/articles/s41467-019-08987-4} }
	
	\centering
	\begin{columns}[T]
	\begin{column}{0.6\textwidth}
	\includegraphics<1>[width=\textwidth]{figure/horse_without_label.PNG}
	\includegraphics<2>[width=\textwidth]{figure/horse_with_label.PNG}
	\includegraphics<3>[width=\textwidth]{figure/horse_map_with_label.PNG}
	\includegraphics<4>[width=\textwidth]{figure/horse_map_without_label.PNG}
	\end{column}
	\begin{column}{0.4\textwidth}
	
	\begin{itemize}
	    \item Horse claimed to do basic math
	    \item Answered questions by hoof tapping or head shaking
	    \item Hans was not able to math and just reacted to involuntary cues in the body language of the human %asking person
	    %(e.g., tense attitude)
	    \item ...
	\end{itemize}
	
	\end{column}
	\end{columns}
\end{frame}


% \begin{frame}{Clever Hans \citebutton{Lapuschkin et al. 2019}{https://www.nature.com/articles/s41467-019-08987-4}}
	
% 	\centering
% 	\includegraphics[width=0.6\textwidth]{figure/boats_maps.PNG}
	
% \end{frame}

\begin{frame}{COMPASS}

    \begin{itemize}
        \item Correctional Offender Management Profiling for Alternative Sanctions 
        \item predict recidivism risk
        \begin{itemize}
            \item i.e., criminal re-offense after previous crime, resulting in jail booking
            \item different risk levels: high risk, medium risk or low risk
        \end{itemize}
        \item evaluation based on a questionnaire the defendant has to answer
    \end{itemize}	
	
\end{frame}

\begin{frame}{COMPAS Model Analysis~\citebutton{Larson et al. 2016}{https://www.propublica.org/article/how-we-analyzed-the-compas-recidivism-algorithm}}
    
    \centering
    \includegraphics[width=0.7\textwidth]{figure/compass_black_white.PNG}
	
	$\leadsto$ Strong indication that the model is discriminating black defendants
	
\end{frame}

\begin{frame}{Examples ML failed in}

    \begin{itemize}
        \item Credit and insurance scoring
        
        \item Medical applications
        \begin{itemize}
            \item Diagnosis
            \item Chance of recovery
            \item Treatment recommendations
        \end{itemize}
        
        \item Crime predictions
        
        \item Rating job applications
        
        \item \ldots
    \end{itemize}

    
    \bigskip

    $\leadsto$ GDPR (aka DSGVO) requires that for some applications predictive models have to be explainable        
	
\end{frame}



\begin{frame}{Motivation - Adversarial Examples}
    
    \begin{center}
    \includegraphics[width=0.7\textwidth]{figure/panda-airplane.pdf}
    \end{center}
	\bigskip
	
	$\rightarrow$ ML Models might not capture human-like understanding.        
    \footnote[frame]{Goodfellow, Ian \& Shlens, Jonathon \& Szegedy, Christian. (2014). Explaining and Harnessing Adversarial Examples. arXiv 1412.6572.}
\end{frame}

	
\begin{frame}{Adversarial Noise}
    \begin{center}
    \includegraphics[width=0.65\textwidth]{figure/adv-noise.pdf}    \footnote[frame]{Goodfellow, Ian \& Shlens, Jonathon \& Szegedy, Christian. (2014). Explaining and Harnessing Adversarial Examples. arXiv 1412.6572.}
    % https://arxiv.org/pdf/1412.6572.pdf
	\end{center}
	\normalsize
	%\bigskip
	$\rightarrow$\textbf{Adversarial Noise:} Noise not visible to \textbf{humans} but results in incorrect classification results.
\end{frame}

\endlecture
\end{document}
