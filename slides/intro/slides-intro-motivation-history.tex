\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\usepackage[]{graphicx}\usepackage[]{color}
% maxwidth is the original width if it is less than linewidth
% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
\ifdim\Gin@nat@width>\linewidth
\linewidth
\else
\Gin@nat@width
\fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%
\newcommand{\predvar}{Var\left[\hat{f}(x)\right]}
\let\hlipl\hlkwb

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
\def\at@end@of@kframe{}%
\ifinner\ifhmode%
\def\at@end@of@kframe{\end{minipage}}%
\begin{minipage}{\columnwidth}%
\fi\fi%
\def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
\colorbox{shadecolor}{##1}\hskip-\fboxsep
% There is no \\@totalrightmargin, so:
\hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
\MakeFramed {\advance\hsize-\width
\@totalleftmargin\z@ \linewidth\hsize
\@setminipage}}%
{\par\unskip\endMakeFramed%
\at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}
\newcommand{\SweaveOpts}[1]{}  % do not interfere with LaTeX
\newcommand{\SweaveInput}[1]{} % because they are not real TeX commands
\newcommand{\Sexpr}[1]{}       % will only be parsed by R

\usepackage[english]{babel}
\usepackage[utf8]{inputenc}

\usepackage{dsfont}
\usepackage{verbatim}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{bm}
\usepackage{csquotes}
\usepackage{multirow}
\usepackage{longtable}
\usepackage{booktabs}
\usepackage{enumerate}
\usepackage[absolute,overlay]{textpos}
\usepackage{psfrag}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{eqnarray}
\usepackage{arydshln}
\usepackage{tabularx}
\usepackage{placeins}
\usepackage{tikz}
\usepackage{setspace}
\usepackage{colortbl}
\usepackage{mathtools}
\usepackage{wrapfig}
\usepackage{bm}

\usetikzlibrary{shapes,arrows,automata,positioning,calc,chains,trees, shadows}
\tikzset{
%Define standard arrow tip
>=stealth',
%Define style for boxes
punkt/.style={
rectangle,
rounded corners,
draw=black, very thick,
text width=6.5em,
minimum height=2em,
text centered},
% Define arrow style
pil/.style={
->,
thick,
shorten <=2pt,
shorten >=2pt,}
}

\usepackage{subfig}
\usepackage{../../style/lmu-lecture}
% Defines macros and environments
\input{../../style/common.tex}

%\usetheme{lmu-lecture}
\usepackage{../../style/lmu-lecture}

\let\code=\texttt
\let\proglang=\textsf

\setkeys{Gin}{width=0.9\textwidth}

\title{Interpretable Machine Learning}
% \author{Bernd Bischl, Christoph Molnar, Daniel Schalk, Fabian Scheipl}
\institute{\href{https://compstat-lmu.github.io/lecture_iml/}{compstat-lmu.github.io/lecture\_iml}}
\date{}

\setbeamertemplate{frametitle}{\expandafter\uppercase\expandafter\insertframetitle}

\begin{document}

\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}
\newcommand{\titlefigure}{figure/open_blackbox}
\newcommand{\learninggoals}{
\item Why do we need interpretability?
\item What have been the developments until now?}


\lecturechapter{Introduction, Motivation and History}
\lecture{Interpretable Machine Learning}



\begin{vbframe}{Why Interpretability?}

\begin{itemize}
  \item  Machine learning (ML) has a huge potential to aid the decision-making process in various scientific and business applications due to its predictive power.
  \item ML models usually are intransparent black boxes (or rather greyish boxes), 
  e.g., bagged or boosted trees, RBF SVM, deep neural networks.
  \item The lack of explanation hurts trust and creates a barrier for the adoption of ML, especially in critical areas where decisions can affect human life (e.g., medicine).
  \item As a result, a lot of disciplines where trust in the model is critical still rely on traditional statistical models, e.g., GLMs, with less predictive performance.
\end{itemize}

\end{vbframe}



\begin{vbframe}{Brief History of Interpretability}
\begin{itemize}
\item Linear regression models date back as far as Gauss (1777 - 1855), Legendre (1752 - 1833), and Quetelet (1796 - 1874).
\item Rule-based ML, which covers decision rules and decision trees, has been an active research area since the middle of the 20th century.
\item Active research in the interpretation of modern ML models began in the 2000s, which reused a lot of concepts from sensitivity analysis (SA). Publications in SA date back as far as the 1940s.
\item The built-in feature importance measure of random forests was one of the first important milestones.
\item The deep learning hype in the 2010s resulted in a lot of publications related to explainable AI (XAI).
\item IML as an independent field of research took off around 2015 with novel techniques such as LIME. 
\end{itemize}
\end{vbframe}


\begin{vbframe}{When do we need interpretability?}
\begin{columns}
\begin{column}{0.6\textwidth}
%  \begin{itemize}
%   \item Debugging machine learning models
%   \item Increasing trust in models
%   \item Analyzing newly developed systems with unknown consequences
%   \item Decisions about humans
%   \item Models using proxies instead of causal inputs, e.g. predicting flu outbreaks from google searches.
%   \item When loss function does not cover constraints like fairness (e.g. credit score) or need for insights (e.g. science).
% \end{itemize}
\begin{itemize}
  \item To \textbf{Justify} (and increase trust in models): investigate if and why biased, unexpected or discriminatory predictions were made.
  \item To \textbf{Control}: debug models, identify and correct vulnerabilities and flaws.
  \item To \textbf{Improve}: understanding why a prediction was made makes it easier to improve the model.
  \item To \textbf{Discover}: learn new facts, gather information and gain insights.
\end{itemize}
\end{column}
\begin{column}{0.6\textwidth}  %%<--- here
 \vspace{0.5cm}
 \begin{center}
 \begin{figure}
  \includegraphics[width=0.7\textwidth]{figure/explain-to}
  \caption{Reasons for IML.}
 \end{figure}
 \end{center}
\end{column}
\end{columns}
 \lz
\tiny{Doshi-Velez, F., and Kim, B. (2017)}
\tiny{Adadi, Amina, and Mohammed Berrada (2018)}
\end{vbframe}


\endlecture
\end{document}
