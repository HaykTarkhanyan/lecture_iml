\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\usepackage[]{graphicx}\usepackage[]{color}
% maxwidth is the original width if it is less than linewidth
% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
\ifdim\Gin@nat@width>\linewidth
\linewidth
\else
\Gin@nat@width
\fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%
\newcommand{\predvar}{Var\left[\hat{f}(x)\right]}
\let\hlipl\hlkwb

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
\def\at@end@of@kframe{}%
\ifinner\ifhmode%
\def\at@end@of@kframe{\end{minipage}}%
\begin{minipage}{\columnwidth}%
\fi\fi%
\def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
\colorbox{shadecolor}{##1}\hskip-\fboxsep
% There is no \\@totalrightmargin, so:
\hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
\MakeFramed {\advance\hsize-\width
\@totalleftmargin\z@ \linewidth\hsize
\@setminipage}}%
{\par\unskip\endMakeFramed%
\at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}
\newcommand{\SweaveOpts}[1]{}  % do not interfere with LaTeX
\newcommand{\SweaveInput}[1]{} % because they are not real TeX commands
\newcommand{\Sexpr}[1]{}       % will only be parsed by R

\usepackage[english]{babel}
\usepackage[utf8]{inputenc}

\usepackage{dsfont}
\usepackage{verbatim}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{bm}
\usepackage{csquotes}
\usepackage{multirow}
\usepackage{longtable}
\usepackage{booktabs}
\usepackage{enumerate}
\usepackage[absolute,overlay]{textpos}
\usepackage{psfrag}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{eqnarray}
\usepackage{arydshln}
\usepackage{tabularx}
\usepackage{placeins}
\usepackage{tikz}
\usepackage{setspace}
\usepackage{colortbl}
\usepackage{mathtools}
\usepackage{wrapfig}
\usepackage{bm}

\usetikzlibrary{shapes,arrows,automata,positioning,calc,chains,trees, shadows}
\tikzset{
%Define standard arrow tip
>=stealth',
%Define style for boxes
punkt/.style={
rectangle,
rounded corners,
draw=black, very thick,
text width=6.5em,
minimum height=2em,
text centered},
% Define arrow style
pil/.style={
->,
thick,
shorten <=2pt,
shorten >=2pt,}
}

\usepackage{subfig}
\usepackage{../../style/lmu-lecture}
% Defines macros and environments
\input{../../style/common.tex}

%\usetheme{lmu-lecture}
\usepackage{../../style/lmu-lecture}

\let\code=\texttt
\let\proglang=\textsf

\setkeys{Gin}{width=0.9\textwidth}

\title{Interpretable Machine Learning}
% \author{Bernd Bischl, Christoph Molnar, Daniel Schalk, Fabian Scheipl}
\institute{\href{https://compstat-lmu.github.io/lecture_iml/}{compstat-lmu.github.io/lecture\_iml}}
\date{}

\setbeamertemplate{frametitle}{\expandafter\uppercase\expandafter\insertframetitle}

\begin{document}

\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}
\newcommand{\titlefigure}{figure/open_blackbox}
\newcommand{\learninggoals}{
\item Why do we need interpretability?
\item What have been the developments until now?}


\lecturechapter{Introduction, Motivation and History}
\lecture{Interpretable Machine Learning}



	\begin{vbframe}{Why Interpretability?}
		
		\begin{itemize}
			\item Machine Learning (ML) has a huge potential to aid the decision-making process in various scientific and business applications due to its predictive power.
			\smallskip
			\item ML models usually are intransparent black boxes, e.g., XGBoost, RBF SVM or DNNs.
			\begin{itemize}
				\item[$\leadsto$] too complex to be understood by humans
			\end{itemize}
			\smallskip
			\item The lack of explanation
			\begin{enumerate}
				\item hurts trust
				\item creates barriers
			\end{enumerate}  
			
			\smallskip
		    \item[$\leadsto$] Harder to adapt for critical areas with decisions affecting human life (e.g., medicine or credits).
			
			%\smallskip
			\item[$\leadsto$] Many disciplines with required trust rely on traditional models,\\ e.g., linear models, with less predictive performance.
		\end{itemize}
		
	\end{vbframe}
	
	%-----------------------------------------------------------------------------------------------------------------------------	
	
	\begin{vbframe}{Brief History of Interpretability}
		\begin{itemize}
			\item 18th and 19th century: linear regression models (Gauss, Legendre, Quetelet)
			\item 1940s: sensitivity analysis (SA), still used today
			\item Middle of 20th century: Rule-based ML, incl. decision rules and decision trees
			\item 2001: built-in feature importance measure of random forests
			\item >2010: explainable AI (XAI) for deep learning
			\item >2015: IML as an independent field of research 
			\item 2018: GDPR requires explainability for some applications 
		\end{itemize}
	\end{vbframe}
	
	%-----------------------------------------------------------------------------------------------------------------------------
	
	\begin{vbframe}{When do We Need Interpretability?}
		\begin{columns}
			\begin{column}{0.6\textwidth}
				\begin{itemize}
					\item To \textbf{Justify} (and increase trust in models): investigate if and why biased, unexpected or discriminatory predictions were made.
					\item To \textbf{Control}: debug models, identify and correct vulnerabilities and flaws.
					\item To \textbf{Improve}: understanding why a prediction was made makes it easier to improve the model.
					\item To \textbf{Discover}: learn new facts, gather information and gain insights.
				\end{itemize}
			\end{column}
			\begin{column}{0.6\textwidth}  
				\begin{center}
					\begin{figure}
						\includegraphics[width=0.58\textwidth]{figure/explain-to}
					\end{figure}
				\end{center}
			\end{column}
		\end{columns}
	\end{vbframe}


\begin{vbframe}{Why is Interpretability Important?}
	
	\begin{itemize}
	    \item Machine learning is (mostly) about discovering patterns in data
	    \item Unfortunately, it is not guaranteed that ML will identify the correct patterns
	    
	    \medskip
	    \item We humans might not be able to discover patterns ML models discovered
	    \begin{itemize}
	        \item That's good for science or to get new insights
	        \item That's bad in many practical application where unexpected behavior is not wanted
	    \end{itemize}
	    \medskip
	    
	    \item \alert{How can you check whether the model is correct in its inference?}
	\end{itemize}
	
\end{vbframe}

\begin{vbframe}{Clever Hans \footnote{Lapuschkin et al. 2019}{https://www.nature.com/articles/s41467-019-08987-4}}
	
	\centering
	\includegraphics[width=0.6\textwidth]{figure/horse_without_label.PNG}
	
\end{vbframe}

\begin{vbframe}{Clever Hans \footnote{Lapuschkin et al. 2019}{https://www.nature.com/articles/s41467-019-08987-4}}
	
	\centering
	\includegraphics[width=0.6\textwidth]{figure/horse_with_label.PNG}
	
\end{vbframe}

\begin{vbframe}{Clever Hans \footnote{Lapuschkin et al. 2019}{https://www.nature.com/articles/s41467-019-08987-4}}
	
	\centering
	\includegraphics[width=0.6\textwidth]{figure/horse_map_with_label.PNG}
	
\end{vbframe}

\begin{vbframe}{Clever Hans \footnote{Lapuschkin et al. 2019}{https://www.nature.com/articles/s41467-019-08987-4}}
	
	\centering
	\includegraphics[width=0.6\textwidth]{figure/horse_map_without_label.PNG}
	
\end{vbframe}

\begin{vbframe}{Clever Hans \footnote{Lapuschkin et al. 2019}{https://www.nature.com/articles/s41467-019-08987-4}}
	
	\centering
	\includegraphics[width=0.6\textwidth]{figure/boats_maps.PNG}
	
\end{vbframe}

\begin{vbframe}{COMPASS}

    \begin{itemize}
        \item Correctional Offender Management Profiling for Alternative Sanctions 
        \item predict recidivism risk
        \begin{itemize}
            \item i.e., criminal re-offense after previous crime, resulting in jail booking
            \item different risk levels: high risk, medium risk or low risk
        \end{itemize}
        \item evaluation based on a questionnaire the defendant has to answer
    \end{itemize}	
	
\end{vbframe}

\begin{vbframe}{COMPAS Model Analysis~\footnote{Larson et al. 2016}{https://www.propublica.org/article/how-we-analyzed-the-compas-recidivism-algorithm}}
    
    \centering
    \includegraphics[width=0.7\textwidth]{figure/compass_black_white.PNG}
	
	$\leadsto$ Strong indication that the model is discriminating black defendants
	
\end{vbframe}

\begin{vbframe}{Other Examples ML failed in}

    \begin{itemize}
        \item Credit and insurance scoring
        
        \item Medical applications
        \begin{itemize}
            \item Identification of diseases
            \item Chance of recovering
            \item Recommendations of treatments
        \end{itemize}
        
        \item Crime predictions
        
        \item Rating job applications
        
        \item \ldots
    \end{itemize}

    
    \bigskip

    $\leadsto$ GDPR (aka DSGVO) requires that for some applications predictive models have to be explainable        
	
\end{vbframe}



\begin{vbframe}{Adversarial Examples~\footnote{Goodfellow et al. 2016}{https://arxiv.org/pdf/1412.6572.pdf}}
    
    \centering
    \includegraphics[width=0.7\textwidth]{figure/panda-airplane.pdf}
	
	$\leadsto$ ML Models might not capture human-like understanding.        
	
\end{vbframe}

	
\begin{vbframe}{Adversarial Noise~\footnote{Goodfellow et al. 2016}{https://arxiv.org/pdf/1412.6572.pdf}}
    
    \centering
    \includegraphics[width=0.6\textwidth]{figure/adv-noise.pdf}
	
	$\leadsto$ \textbf{Adversarial Noise:} Noise that is imperceptible to \textbf{humans}\\ but results in incorrect classification results
	
\end{vbframe}

<<<<<<< HEAD
\begin{vbframe}{When do we need interpretability?}
\begin{columns}
\begin{column}{0.6\textwidth}
%  \begin{itemize}
%   \item Debugging machine learning models
%   \item Increasing trust in models
%   \item Analyzing newly developed systems with unknown consequences
%   \item Decisions about humans
%   \item Models using proxies instead of causal inputs, e.g. predicting flu outbreaks from google searches.
%   \item When loss function does not cover constraints like fairness (e.g. credit score) or need for insights (e.g. science).
% \end{itemize}
\begin{itemize}
  \item To \textbf{Justify} (and increase trust in models): investigate if and why biased, unexpected or discriminatory predictions were made.
  \item To \textbf{Control}: debug models, identify and correct vulnerabilities and flaws.
  \item To \textbf{Improve}: understanding why a prediction was made makes it easier to improve the model.
  \item To \textbf{Discover}: learn new facts, gather information and gain insights.
\end{itemize}
\end{column}
\begin{column}{0.6\textwidth}  %%<--- here
 \vspace{0.5cm}
 \begin{center}
 \begin{figure}
  \includegraphics[width=0.7\textwidth]{figure/explain-to}
  \caption{Reasons for IML.}
 \end{figure}
 \end{center}
\end{column}
\end{columns}
 \lz
\tiny{Doshi-Velez, F., and Kim, B. (2017)}
\tiny{Adadi, Amina, and Mohammed Berrada (2018)}
=======
\begin{vbframe}{Adversarial Examples~\footnote{Goodfellow et al. 2016}{https://arxiv.org/pdf/1412.6572.pdf}}
    
    \centering
    \includegraphics[width=0.7\textwidth]{figure/adv-noise-2.pdf}
	
>>>>>>> intro
\end{vbframe}


\endlecture
\end{document}
