\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\usepackage[]{graphicx}\usepackage[]{color}
% maxwidth is the original width if it is less than linewidth
% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
\ifdim\Gin@nat@width>\linewidth
\linewidth
\else
\Gin@nat@width
\fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%
\newcommand{\predvar}{Var\left[\hat{f}(x)\right]}
\let\hlipl\hlkwb

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
\def\at@end@of@kframe{}%
\ifinner\ifhmode%
\def\at@end@of@kframe{\end{minipage}}%
\begin{minipage}{\columnwidth}%
\fi\fi%
\def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
\colorbox{shadecolor}{##1}\hskip-\fboxsep
% There is no \\@totalrightmargin, so:
\hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
\MakeFramed {\advance\hsize-\width
\@totalleftmargin\z@ \linewidth\hsize
\@setminipage}}%
{\par\unskip\endMakeFramed%
\at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}
\newcommand{\SweaveOpts}[1]{}  % do not interfere with LaTeX
\newcommand{\SweaveInput}[1]{} % because they are not real TeX commands
\newcommand{\Sexpr}[1]{}       % will only be parsed by R

\usepackage[english]{babel}
\usepackage[utf8]{inputenc}

\usepackage{dsfont}
\usepackage{verbatim}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{bm}
\usepackage{csquotes}
\usepackage{multirow}
\usepackage{longtable}
\usepackage{booktabs}
\usepackage{enumerate}
\usepackage[absolute,overlay]{textpos}
\usepackage{psfrag}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{eqnarray}
\usepackage{arydshln}
\usepackage{tabularx}
\usepackage{placeins}
\usepackage{tikz}
\usepackage{setspace}
\usepackage{colortbl}
\usepackage{mathtools}
\usepackage{wrapfig}
\usepackage{bm}
\usepackage{verbatim}

\usetikzlibrary{shapes,arrows,automata,positioning,calc,chains,trees, shadows}
\tikzset{
%Define standard arrow tip
>=stealth',
%Define style for boxes
punkt/.style={
rectangle,
rounded corners,
draw=black, very thick,
text width=6.5em,
minimum height=2em,
text centered},
% Define arrow style
pil/.style={
->,
thick,
shorten <=2pt,
shorten >=2pt,}
}

\usepackage{subfig}

% Defines macros and environments
\input{../../style/common.tex}

%\usetheme{lmu-lecture}
\usepackage{../../style/lmu-lecture}

\let\code=\texttt
\let\proglang=\textsf

\setkeys{Gin}{width=0.9\textwidth}

\title{Interpretable Machine Learning}
% \author{Bernd Bischl, Christoph Molnar, Daniel Schalk, Fabian Scheipl}
\institute{\href{https://compstat-lmu.github.io/lecture_iml/}{compstat-lmu.github.io/lecture\_iml}}
\date{}

\setbeamertemplate{frametitle}{\expandafter\uppercase\expandafter\insertframetitle}

\begin{document}

\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}
\newcommand{\titlefigure}{figure/whitebox}
\newcommand{\learninggoals}{
\item What characteristics does an interpretable model have?
\item Why should we use interpretable models at all?
\item Examples for interpretable models.}

\lecturechapter{Interpretable Models}
\lecture{Interpretable Machine Learning}




\begin{vbframe}{Motivation}

    \begin{itemize}
        \item Obtaining interpretations by using interpretable models is the easiest and least error-prone approach.
        \bigskip
        \item Classes of models deemed interpretable:
        \begin{itemize}
            \item Linear regression models 
            \item Generalized linear models
            \item Generalized additive models
            \item Rule-based learning
            \item Model-based boosting
            \item ...
        \end{itemize}
    \end{itemize}	
	
\end{vbframe}
	
\begin{vbframe}{Advantages}

    \begin{itemize}
    \itemsep2em
        \item No additional model-agnostic techniques required, which eliminates a source of failure.
        \item Since the models are often rather simple, training time is also fairly small.
        \item Many people are familiar with traditional models. This increases trust in the model, and facilitates communication about its results.
        \item It is easier to deploy simple models in practice. Implementations are available in many programming languages, and they are often also easy to implement from scratch.
    \end{itemize}	
	
\end{vbframe}

\begin{vbframe}{Disadvantages}

    \begin{itemize}
    \itemsep2em
        \item One often needs to make certain assumptions about the data and / or model structure, which significantly influences predictive performance.
        \item If the wrong assumptions are made, interpretable models may have a bad predictive performance.
        \item The model may not even be capable of modeling highly complex data structures.
        \item A complex structure of an interpretable model may also be difficult to interpret, e.g., a linear model with hundreds of features and interactions.
    \end{itemize}	
	
\end{vbframe}

\begin{vbframe}{Further Comments}

    \begin{itemize}
    \itemsep1em
        \item Some argue that one should always use interpretable models in the first place (Rudin, 2019).
        \footnote[frame]{Rudin, C. Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead. Nat Mach Intell 1, 206â€“215 (2019).}
        % {https://www.nature.com/articles/s42256-019-0048-x}
        \item Interpretable models also have the potential for a high predictive performance, but require more knowledge and time spent on data preprocessing.
        \item However, one can assume a trade-off between interpretability and model performance which generally (but not always) holds.
        \item \textbf{Recommendation:} Always start with the most simple model that makes sense for your particular application. Gradually increase complexity, which will gradually lower interpretability and require additional interpretation methods. Choose the most simple, sufficient model (Occam's razor).
    \end{itemize}	
	
\end{vbframe}



\begin{vbframe}{Linear and Polynomial Regression}

\begin{align*}
\mathbb{E}_Y(Y \vert X) &= \beta_0 + \beta_1 x_1 + \dots + \beta_p x_p + \dots + \epsilon \\
 &= X^T\beta + \mathcal{E}
\end{align*}

\begin{itemize}
\itemsep2em
\item The model equation is identical across the entire feature space.
\item The predictive power of LMs is determined by specifying the correct model structure.
\item
A polynomial regression model is an extension of the LM that includes higher order terms or interactions. This enables us to model non-linear data while making use of the entire arsenal of LM functionality.
\item
By knowing the model equation, we can exactly determine feature effects (e.g., beta coefficients, effect plots) and importance scores (e.g., p-values, t-statistics).
\item For higher order effects or interactions, beta coefficients cannot be interpreted in isolation.
\item Note that for inference-based metrics (p-values, t-statistics, confidence intervals) to be valid, the error term needs to be normally distributed with zero mean, i.e., $\epsilon \sim N(0, \sigma^2)$. It follows that $(y \vert x) \sim N(x^T \beta, \sigma^2)$.
\item This restricts the usage of LMs in practice, as the distribution of the error term is a prior assumption about the data.
\end{itemize}
\end{vbframe}

\begin{vbframe}{Linear and Polynomial Models}
\tiny
\verbatiminput{figure/lm_output.txt}
\end{vbframe}

\begin{vbframe}{Linear and Polynomial Models}
\begin{figure}
  \includegraphics[width = 0.6\textwidth]{figure/lm_effect_plot.png}
\end{figure}
\end{vbframe}


\begin{vbframe}{Generalized Linear Models}

\begin{align*}
g\left(\mathbb{E}_Y(Y \vert X)\right) &= X^T\beta \\
\mathbb{E}_Y \left(Y \vert X\right) &= g^{-1}(X^T\beta) 
\end{align*}
\begin{itemize}
\setlength\itemsep{2em}
\item Generalized linear models (GLMs) keep the linear predictor $X^T\beta$ which now explains the transformed, expected conditional target through a more flexible link function $g$.
\item GLMs are a framework for target distributions of the exponential family, e.g., Gaussian, Binomial, Poisson, Exponential, Gamma. A Gaussian target distribution with identity link corresponds to a linear / polynomial regression model.
\item The link function describes how the linear predictor $X^T\beta$ relates to the expected, conditional target $\mathbb{E}_Y(Y \vert X)$.
\item If the target is distributed binomially, a natural link function is the logit link $log\left(\frac{\mathbb{E}_Y(Y \vert X)}{1 - \mathbb{E}_Y(Y \vert X)}\right)$ (logistic regression).
\item We need to specify the correct model equation, target distribution, and link function in order to receive a good model fit.
\item As the model equation is still known, interpretations are possible the same way as for polynomial regression models. However, even linear terms become non-linear through the link function (if the identity link is not used)!
\end{itemize}
\end{vbframe}


\begin{vbframe}{Generalized Linear Models}
\tiny
\verbatiminput{figure/glm_output.txt}
\end{vbframe}

\begin{vbframe}{Generalized Linear Models}
\begin{figure}
  \includegraphics[width = 0.6\textwidth]{figure/glm_effect_plot.png}
\end{figure}
\end{vbframe}

\begin{vbframe}{Generalized Additive Models}

$$
g\left(\mathbb{E}_Y(y \vert x)\right) = \beta_0 + \beta_1 h_1(x_1) + \dots + \beta_p h_p(x_p) + \dots
$$
\begin{itemize}
\setlength\itemsep{2em}
\item Replaces pre-specified terms with smoothing functions.
\item For the component functions, we may either specify a parametric form (e.g., a regression splines), or a non-parametric one, e.g., locally estimated scatterplot smoothing (LOESS).
\item Makes GAMs much more adaptive, as model is more determined by the structure of the data instead of premade assumptions as in LMs and GLMs.
\item We still need to specify the order and type of the component functions to be estimated, e.g., which interactions to include.
\item The smoothing degree of each component function can be tuned to avoid overfitting. Regularization increases interpretability, as the interpretations drawn from the model can be transferred more easily to new data.
\item Retains interpretability by keeping the additive model equation (as long as the component functions are interpretable). 
\item
As the model equation is known, we can use similar interpretation methods as for LMs and GLMs, e.g., evaluating the estimated components that depend on the features of interest.
\end{itemize}
\end{vbframe}


\begin{vbframe}{Generalized Additive Models}
\tiny
\verbatiminput{figure/gam_output.txt}
\end{vbframe}


\begin{vbframe}{Generalized Additive Models}
\begin{figure}
  \includegraphics[width = 0.6\textwidth]{figure/gam_effect_plot.png}
\end{figure}
\end{vbframe}

\begin{vbframe}{Model-Based Boosting}

\begin{itemize}
\setlength\itemsep{2em}
\item 
Boosting iteratively joins weak base learners to create a powerful ensemble model.
\item 
Idea: Combine boosting with interpretable base learners (e.g., single feature LM). The resulting ensemble is also interpretable.
\item
Consider two linear base learners $b_j(x, \Theta)$ and $b_j(x, \Theta^{\star})$ with the same type, but distinct parameter vectors $\Theta$ and $\Theta^{\star}$. They can be combined in a base learner of the same type:
$$
b_j(x, \Theta) + b_j(x, \Theta^{\star}) = b_j(x, \Theta + \Theta^{\star})
$$
\item In each iteration, a selection of base learners is trained on the so-called pseudo residuals. The one with the best fit is added to the previously computed model:
\medskip
\begin{align*}
\widehat{f}^{[1]}(x) &= f_0 + \beta b_3(x_3, \theta^{[1]}) \\
\widehat{f}^{[2]}(x) &= f_0 + \beta b_3(x_3, \theta^{[1]}) + \beta b_3(x_3, \theta^{[2]})\\
\widehat{f}^{[3]}(x) &= f_0 + \beta b_3(x_3, \theta^{[1]}) + \beta b_3(x_3, \theta^{[2]}) + \beta b_1(x_1, \theta^{[3]}) \\
&= f_0 + \beta \left(b_3(x_3, \theta^{[1]} + \theta^{[2]}) + b_1(x_1, \theta^{[3]})\right)
\end{align*}

\item The final model has an additive structure (equivalent to a GAM), where each component function is interpretable itself.

\end{itemize}
\end{vbframe}



% \begin{vbframe}{Linear Regression}

%     $$y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \ldots + \beta_p x_p + \epsilon $$

%     \begin{itemize}
%         \item $y$ target
%         \item $\beta_i$ weight of input feature $x_j$
%         \item $\epsilon$ remaining error
%         \item[$\rightarrow$] model consists of $p+1$ weights $\beta_i$
%         \item Properties and assumptions:
%         \begin{itemize}
%         \item The relationship between the predictor and the mean of $y$ is linear.
%         \item iid observations
%         \item Homoscedasticity (i.e., constant error variance)
%         \end{itemize} 
%     \end{itemize}

% \end{vbframe}


%     Linearity: The relationship between X and the mean of Y is linear.
%     Homoscedasticity: The variance of residual is the same for any value of X.
%     Independence: Observations are independent of each other.
%     Normality: For any fixed value of X, Y is normally distributed.



%------------------------------------------------------------------
%------------------------------------------------------------------

% \begin{vbframe}{Interpretation of Linear Regression}

%     $$y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \ldots + \beta_p x_p + \epsilon $$

%     Let's consider different feature types:
%     \begin{itemize}
%         \item Numerical features: Increase of numerical value will lead to $\beta_i$ times increased output
%         \item Binary feature: Either weight $\beta_i$ is active (1) or not (0).
%         \item Categorical feature: One-hot-encoding of $L-1$ new features for $L$ categories
%         \item Intercept $\beta_0$: reflects expected features values if features were standardised (0-mean, 1-stdev)
%     \end{itemize}	
    
%     Feature importance:
%     \begin{itemize}
%         \item t-statistic by the estimated weight scaled with its standard error\\ (i.e., less certain about the correct value)
%     \end{itemize}
%     $$t_{\beta_i} = \frac{\beta_i}{SE(\beta_i)} $$
% \end{vbframe}

% %------------------------------------------------------------------
% %------------------------------------------------------------------
	
% \begin{vbframe}{Logistic Regression}

%     $$P(y = 1) =\frac{1}{1 + \exp(-( \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \ldots + \beta_p x_p ))} $$

%     \begin{itemize}
%         \item Probabilistic \alert{classification} model 
%         \item Typically, we set the threshold to $0.5$ to predict 
%         \begin{itemize}
%             \item Class 1 if $P(y=1) > 0.5$
%             \item Class 0 if $P(y=1) \leq 0.5$
%         \end{itemize}
%     \end{itemize}	

% \end{vbframe}

% %------------------------------------------------------------------
% %------------------------------------------------------------------

% \begin{vbframe}{Interpretation of Logistic Regression}

%     $$\log \left(\frac{P(y = 1)}{P(y=0)}\right) = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \ldots + \beta_p x_p  $$

%     \begin{itemize}
%         \item weights relate to log odds-ratio
%         \item Again linear in log odds-ratio
%         \item[$\leadsto$] change by one unit changes the odds ratio by a \alert{factor} of $\exp(\beta_i)$.
%         \medskip
        
%         \item Interpretation for different feature types is the same as for linear regression
%     \end{itemize}	

% \end{vbframe}

% %------------------------------------------------------------------
% %------------------------------------------------------------------

% \begin{vbframe}{GLM and Interactions}

%     \begin{itemize}
%         \item Linear models are often too restrictive for many applications
%     \end{itemize}
    
%     \medskip
%     Non-Gaussian outputs via Generalized Linear Models (GLMs):
%     $$g(\mathrm{E}_Y (y\mid x)) = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \ldots + \beta_p x_p$$
%     \begin{itemize}
%         \item link function $g$ -- can be freely chosen
%         \item exponential family defining $\mathrm{E}_Y$ -- can be freely chosen
%         \item weighted sum $X^\top W$
%     \end{itemize}
    
%     \medskip 
    
%     Interaction effects via feature engineering:
%     \begin{itemize}
%         \item E.g., feature expansion: $\beta_{x_i,x_j} x_i \cdot x_j$
%     \end{itemize}
    
% \end{vbframe}

% %------------------------------------------------------------------
% %------------------------------------------------------------------

% \begin{vbframe}{Generalized Additive Models (GAMs)}

% Non-Linear relations can be addressed by:
%     \begin{itemize}
%         \item feature transformations (e.g., exp or log)
%         \item Categorization of features (i.e., intervals / buckets of feature values)
%         \item GAMs:
%     \end{itemize}
    
%     $$g(\mathrm{E}_Y (y\mid x)) = \beta_0 + f_1(x_1) + f_2(x_2) + \ldots + f_p(x_p)$$
    
%     \begin{itemize}
%         \item instead of $\beta_i x_i$ use flexible functions $f_i(x_i)$ $\leadsto$ splines
%     \end{itemize}

% \end{vbframe}

% %------------------------------------------------------------------
%------------------------------------------------------------------

% \begin{vbframe}{Decision Trees}

% \begin{columns}

% \begin{column}{0.3\textwidth}

%   \begin{tikzpicture}
%   \usetikzlibrary{arrows}
%     \usetikzlibrary{shapes}
%      \tikzset{treenode/.style={draw, circle, font=\small}}
%      \tikzset{line/.style={draw, thick}}
%      \node [treenode, draw=red] (a0) {$a_0$};
%      \node [treenode, below=0.75cm of a0, xshift=-1cm]  (a1) {$a_1$};
%      \node [treenode, draw=red, below=0.75cm of a0, xshift=1cm]  (a2) {$a_2$};
     
%      \node [treenode, draw=red, below=0.75cm of a2, xshift=-1cm] (a3) {$a_3$};
%      \node [treenode, below=0.75cm of a2, xshift=1cm]  (a4) {$a_4$};
     
%      \node [treenode, below=0.75cm of a3, xshift=-1cm] (a5) {$a_5$};
%      \node [treenode, below=0.75cm of a3, xshift=1cm]  (a6) {$a_6$};
     
%      \path [line] (a0.south) -- + (0,-0.4cm) -| (a1.north) node [midway, above] {$x_1<0.3$};
%      \path [line] (a0.south) -- +(0,-0.4cm) -|  (a2.north) node [midway, above] {$x_1\geq0.3$};
     
%      \path [line] (a2.south) -- + (0,-0.4cm) -| (a3.north) node [midway, above] {$x_1<0.6$};;
%      \path [line] (a2.south) -- +(0,-0.4cm) -|  (a4.north) node [midway, above] {$x_1\geq0.6$};
     
          
%      \path [line] (a3.south) -- + (0,-0.4cm) -| (a5.north) node [midway, above] {$x_2<0.2$};;
%      \path [line] (a3.south) -- +(0,-0.4cm) -|  (a6.north) node [midway, above] {$x_2\geq0.2$};
     
%   \end{tikzpicture}

% \end{column}

% \begin{column}{0.7\textwidth}

% Properties:
% \begin{itemize}
%     \item able to model non-linear effects
%     \item terminal nodes (aka leaf nodes) can have several observations and predicts the mean outcome over these
%     \item Applicable to regression and classification
% \end{itemize}

% Interpretation:
% \begin{itemize}
%     \item directly by following the tree (i.e., sequence of rules)
%     \item Feature importance by (scaled) score of much the splitting criterion was reduced compared to the parent
% \end{itemize}

% \end{column}

% \end{columns}

% \end{vbframe}

% %------------------------------------------------------------------
% %------------------------------------------------------------------

% \begin{vbframe}{Decision Rules}

% \texttt{IF COND$_1$ AND COND$_2$ AND ... THEN value}

% \begin{itemize}
%     \item \texttt{COND$_i$} can be of the form \texttt{feature <op> value} where \texttt{<op>} can be for example $\{=, <, > \}$
% \end{itemize}


% \medskip

% Properties:
% \begin{description}
%     \item{Support} Fraction of observations to support appliance of rule
%     \item{Accuracy} for predicting the correct class under the condition(s)
% \end{description}

% $\leadsto$ often trade-off between these two


% \medskip

% $\leadsto$ many different ways to learn a set of rules (incl. a default rule if none of the rules are met)

% \end{vbframe}

%------------------------------------------------------------------
%------------------------------------------------------------------



\begin{vbframe}{Model-Based Boosting}
\tiny
\verbatiminput{figure/mboost_output.txt}
\end{vbframe}


\begin{vbframe}{Model-Based Boosting}
\begin{figure}
  \includegraphics[width = 0.6\textwidth]{figure/mboost_effect_plot.png}
\end{figure}
\end{vbframe}


\begin{vbframe}{Rule-Based Learning}

\begin{itemize}
\setlength\itemsep{1em}
\item 
Decision rules follow a general structure: IF the conditions are met, THEN make a certain prediction.
\item Although a single decision rule is very intelligibile, a large collection of decision rules might not be. An interpretable rule-based model should therefore be based on as few rules as possible.
\item Descripte Rule Discovery (DRD) is based on mining databases for frequent item sets, and converting these into a set of decision rules.
\item For instance, a rule \{mobile phone\} $\Rightarrow$ \{mobile phone case\} indicates that mobile phones and phone cases are bought together frequently.
\item
DRD is not suited to create a model that makes predictions on unseen data.
\item Predictive Rule Learning (PRL) aims to discover a set of rules that cover the entire feature space\footnote[frame]{FÃ¼rnkranz J., Kliegr T. (2015) A Brief Overview of Rule Learning. In: Bassiliades N., Gottlob G., Sadri F., Paschke A., Roman D. (eds) Rule Technologies: Foundations, Tools, and Applications. RuleML 2015. Lecture Notes in Computer Science, vol 9202. Springer, Cham.}. There are innumerable rule-based learning algorithms, e.g., decision tree learning, OneR, Sequential Covering, or Bayesian Rule Lists.
\item Decision tree learning, e.g, classification and regression trees (CART), is the most popular group of rule-based models, as every tree can be represented by a collection of decision rules.
\item A decision tree recursively partitions the data by optimizing an objective function, e.g., minimizing the data impurity.
\item A succession of tree splits can be converted to a decision rule by forming a conjunction of successive split criteria.
\end{itemize}
\end{vbframe}


%\item The RuleFit algorithm combines polynomial regression with rule ensembles for automatic interaction detection. In the first stage, a tree ensemble is trained to produce decision rules (where the predictions are discarded and only the splits are utilized), e.g., $x_1 < 0 \land x_2 > 1$. In the second stage, a linear model with main effects is augmented with each decision rule being represented by a single dummy feature.
%\item OneR learns rules from a single feature. OneR is characterized by its simplicity, interpretability and its use as a benchmark.
%\item Sequential covering is a general procedure that iteratively learns rules and removes the data points that are covered by the new rule. This procedure is used by many rule learning algorithms.
%\item Bayesian Rule Lists combine pre-mined frequent patterns into a decision list using Bayesian statistics. Using pre-mined patterns is a common approach used by many rule learning algorithms.
%\end{itemize}


\begin{vbframe}{Decision Trees}

\begin{columns}
  \begin{column}{0.6\textwidth}
      \begin{center}
        \includegraphics[width = \textwidth]{figure/rpart_plot.png}     
      \end{center}
  \end{column}
  \begin{column}{0.7\textwidth}
     \tiny
     \verbatiminput{figure/rpart_output.txt}
  \end{column}
\end{columns}

\end{vbframe}


%     \begin{vbframe}{Bike Rentals (Regression)}
        
%         \begin{itemize}
%             \item Target: number of rented bikes
%             \item Source: bicycle rental company Capital-Bikeshare in Washington D.C.,
%             \item Reference: \footnote{Fanaee-T and Gama. 2014}{https://link.springer.com/article/10.1007/s13748-013-0040-3}
%             \item Exemplary features:
%             \begin{itemize}
%                 \item season: spring, summer, fall or winter.
%                 \item holiday or not.
%                 \item working day or weekend
%                 \item weather situation on that day 
%                 \item temperature
%             \end{itemize}
%         \end{itemize}
        
%     \end{vbframe}
    
%     \begin{vbframe}{Regression on Bike Rentals}
        
%         \centering
%         \includegraphics[width=0.55\textwidth]{figure/lin_bikesharing.png}        

%     \end{vbframe}
    
%     \begin{vbframe}{Decision Tree on Bike Rentals}
        
%         \centering
%         \includegraphics[width=0.55\textwidth]{figure/dt_bike_sharing.png}        

%     \end{vbframe}
    
%     \begin{vbframe}{Risk Factors for Cervical Cancer (Classification)}
        
%         \begin{itemize}
%             \item Target: patient will get cancer or not?
%             \item Reference: \footnote{Fernandes et al. 2017, \url{https://link.springer.com/chapter/10.1007/978-3-319-58838-4_27}}
%             \item Exemplary features:
%             \begin{itemize}
%                 \item Age in years
%                 \item Number of sexual partners
%                 \item First sexual intercourse (age in years)
%                 \item Number of pregnancies
%                 \item Smoking yes or no
%                 \item Smoking (in years)
%                 \item Hormonal contraceptives yes or no
%                 \item Hormonal contraceptives (in years)
%                 \item Intrauterine device yes or no (IUD)
%             \end{itemize}
%         \end{itemize}
        
%     \end{vbframe}
	
% 	\begin{vbframe}{Logistic Regression on  Cancer (Classification)}
        
%         \centering
%         \includegraphics[width=0.55\textwidth]{figure/lin_cancer.png}     
        
%     \end{vbframe}
	
% 	\begin{vbframe}{Decision Tree on  Cancer (Classification)}
        
%         \centering
%         \includegraphics[width=0.55\textwidth]{figure/dt_cancer.png}     
%     \end{vbframe}
	
% 	\begin{vbframe}{Predictive Performance}
        
%         \begin{itemize}
%             \item Bike Rental (normalized MSE):
%             \begin{itemize}
%                 \item Linear Regression: 0.0276
%                 \item Decision Tree: 0.0328
%                 \item GLM: 0.0244
%             \end{itemize}
            
%             \medskip
%             \item Cancer (Accuracy)
%             \begin{itemize}
%                 \item Logistic Regression: 0.58
%                 \item Decision Tree: 0.62
%             \end{itemize}
            
%             \smallskip
%             \item[$\leadsto$] although easy to interpret, not really well-performing 
%             \begin{itemize}
%                 \item Boosting: 0.79
%             \end{itemize}
%         \end{itemize}
        
%     \end{vbframe}
	

\endlecture
\end{document}
