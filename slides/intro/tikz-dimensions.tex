\documentclass[11pt,compress,t,notes=noshow, aspectratio=169, xcolor=table]{beamer}

\usepackage{../../style/lmu-lecture}
% Defines macros and environments
\input{../../style/common.tex}
\newcommand\tab[1][1cm]{\hspace*{#1}}

\tikzset{main node/.style={rectangle,draw,minimum size=1cm,inner sep=4pt},}

\title{Interpretable Machine Learning}
\date{}

\begin{document}
\newcommand{\titlefigure}{figure/open_blackbox}
\newcommand{\learninggoals}{
\item What is interpretable machine learning (IML) and Explainable Artificial Intelligence (XAI)?
\item What is interpretability?
\item What are the fundamental terms and concepts of IML?}

\lecturechapter{Fundamental Terms and Concepts}
\lecture{Interpretable Machine Learning}

\begin{vbframe}{Intrinsic vs. Model-Agnostic}
	\begin{tikzpicture}[every path/.style={->,line width=0.35mm,thick},
                        every label/.append style={align=left, font=\footnotesize, text width=3.2cm}]
        \node[main node] (1) { Model Interpretation };
        \node[main node,
            label={below:Decision trees \\ Decision rules \\ Generalized regression models}
            ] (2) [below left = 1cm and 1cm of 1]  { Interpretable Models };
        \node[main node] (3) [below right = 1cm and -0.5cm of 1] { Black Box Models };
        \node[main node,
            label={below:Random Forest Explainer \\ Visualizing activations of neural networks}
            ] (4) [below left = 1cm and 0cm of 3] { Model-specific Methods };
        \node[main node, fill=gray,
            label={below:\textbf{Advantage: Flexibility} \\ Underlying models can be exchanged\\ Several IML Visualizations can be used}
            ] (5) [below right = 1cm and 0cm of 3] { Model-agnostic Methods };
        \draw (1) -- (2);
        \draw (1) -- (3);
        \draw (3) -- (4);
        \draw (3) -- (5);

    \end{tikzpicture}
\end{vbframe}

\begin{vbframe}{Types of Explanations}
    \begin{center}
        \begin{tikzpicture}[every path/.style={->,line width=0.35mm,thick},
                            every label/.append style={align=left, font=\footnotesize, text width=4cm}]
            \node[main node] (1) { Model Interpretation };
            \node[main node,
                label={below:Saliency Maps\\ Hard Masking \\ Model-agnostic methods \\ \tab SHAP\\ \tab LIME}
                ] (2) [below left = 2.3cm and 1.5cm of 1]  { Feature Attribution };
            \node[main node,
                label={below:Influence Functions\\ ... }
                ] (3) [below = 2.3cm of 1] { Data Attribution };
            \node[main node,
                label={below:Contrastive explanations\\ Diverse counterfactuals \\ Feasible \& actionable explanations}
                ] (4) [below right = 2.3cm and 1.5cm of 1] { Counterfactual Explanations };
            \draw (1) -- (2);
            \draw (1) -- (3);
            \draw (1) -- (4);
    
        \end{tikzpicture}  
    \end{center}
\end{vbframe}


\begin{vbframe}{Global vs. Local}

\begin{center}
    \begin{tikzpicture}[every path/.style={->,line width=0.35mm,thick},
                        every label/.append style={align=left, font=\footnotesize, text width=4cm}]
        \node[main node] (1) { Model Interpretation };
        \node[main node,
            label={below:Instance wise feature selections\\ ALE \\ Model-agnostic methods \\ \tab SHAP\\ \tab LIME}
            ] (2) [below left = 2.3cm and 1.5cm of 1]  { Local Explanations };
        \node[main node,
            label={below:Rule-based Explanations\\ Permutation-based Feature importance \\ Functional Anova \\ PDP}
            ] (3) [below right = 2.3cm and 1.5cm of 1] { Global Explanations };
        \draw (1) -- (2);
        \draw (1) -- (3);

    \end{tikzpicture}  
\end{center}

\end{vbframe}



\endlecture
\end{document}
