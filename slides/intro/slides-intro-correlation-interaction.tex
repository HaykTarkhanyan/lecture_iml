%TODO: Chapter needs to be improved a lot
\documentclass[11pt,compress,t,notes=noshow, aspectratio=169, xcolor=table]{beamer}

\usepackage{../../style/lmu-lecture}
% Defines macros and environments
\input{../../style/common.tex}

\usepackage[export]{adjustbox}
\usepackage[most]{tcolorbox}

\newtcolorbox{BlueBox}[2][]{%
   enhanced,
   colback   = blue!5!white,
   colframe  = blue!65!black, 
   arc       = 1mm, 
   outer arc = 1mm, 
   fonttitle = \Large\slshape\textbf,
   center title, 
   title     = #2,
   #1}
   
\title{Interpretable Machine Learning}
% \author{LMU}
%\institute{\href{https://compstat-lmu.github.io/lecture_iml/}{compstat-lmu.github.io/lecture\_iml}}
\date{}

\begin{document}

\newcommand{\titlefigure}{figure/open_blackbox}
\newcommand{\learninggoals}{
\item Understand feature correlations and interactions
\item What are general pitfalls of interpretation methods?}

\lecturechapter{General Pitfalls}
\lecture{Interpretable Machine Learning}
%
% \begin{frame}{Interpretable ML}
% \begin{itemize}
% %\itemsep2em
% \item ML algorithms algorithmically train predictive models with no or little pre-specifications and assumptions about the data.
% \item Several algorithms such as decision tree learning create interpretable models. However, most algorithms create models which can be considered a black box.
% \item We use the term black box, although the internal workings of the model are in fact accessible, but too complex for the human mind to comprehend.
% \end{itemize}
% \end{frame}
%
% \begin{frame}{Explainable AI}
% \begin{itemize}
% %\itemsep1em
% \item IML is often used synonymously with Explainable AI (XAI).
% \item There is no unified standard for these terminologies. We find that XAI often is specifically concerned with the interpretation of neural networks, whereas IML is used as an encompassing term for everything related to model interpretability.
% \item The nature of (deep) neural networks allows for powerful model-specific interpretation techniques, e.g., layer-wise relevance propagation (LRP) and saliency maps.
% \item Also covering model-specific NN methods would exceed the timeframe of this lecture. This lecture will concentrate on model-agnostic techniques, as they are both versatile, and receive a lot of attention in industry and academia.
% \end{itemize}
% \end{frame}
%
% \begin{frame}{XAI - Saliency Maps}
%
% A saliency map is a heatmap indicating pixel influence on the prediction (e.g., a classification of an image): \footnote[frame]{Mundhenk, T., Chen, B.Y., Friedland, G. (2019). Efficient Saliency Maps for Explainable AI. ArXiv, abs/1911.11293.
% }
% \medskip
% \begin{figure}
% \includegraphics[width = 0.8 \textwidth]{figure/saliencymap}
% \end{figure}
% \end{frame}
%
% \begin{frame}{What is Interpretability?}
% \begin{itemize}
% %\itemsep1em
% \item There is no scientific consensus on the definition of interpretability.
% \item We need to differentiate between interpretations of a model or reality. The latter is distorted by all modeling fallacies involved in predictive modeling, e.g., data quality, under- and overfitting, or model extrapolations.
% \item We use a practical definition of interpretability.
% Think back to the foundations of statistical modeling:  the linear regression model (LM). The LM, with its known equation of beta coefficients, represents a paradigm for statistical interpretability.
% \item It follows that it would be beneficial to create techniques that give us an interpretation similar to the one of an LM.
%
% \end{itemize}
% \end{frame}


% \begin{frame}{Correlation and Dependence}
% \begin{itemize}
% \item \textbf{Correlation:} Often linear correlation (Pearson $\rho$) between pairs of features is meant
% \item \textbf{Dependence:} More general dependence structure (e.g., non-linear relationships)
% \item $X_j$, $X_k$ independent $\Leftrightarrow$ joint probability density function (PDF) is product of marginal PDFs:
% %$$\text{PDF}_{X_j, X_k}(x_j, x_k) = \text{PDF}_{X_j}(x_j) \cdot \text{PDF}_{X_k}(x_k)$$
% $$\P(X_j, X_k) = \P(X_j) \cdot \P(X_k)$$


% $\P(X_j|X_k) = \P(X_j)$ knowing $X_k$ does not tell us anything about $X_j$ and vice versa
% \item $X_j$, $X_k$ independent $\Rightarrow$ $X_j$, $X_k$ uncorrelated \textbf{but} $X_j$, $X_k$ uncorrelated $\nRightarrow$ $X_j$, $X_k$  independent
% %\item Auf einer slide eklären, visuell und mathematisch
% \end{itemize}

% \centering

% \only<1>{
% \textbf{Example:} $X_1$, $X_2 \sim N(0,1)$ independent ($\rho = 0$) \hspace{10pt} $X_1$, $X_2 \sim N(0,1)$ dependent ($\rho = 0.8$) \hspace{30pt}

% \includegraphics[width = 0.4\textwidth]{figure/independent}
% \includegraphics[width = 0.4\textwidth]{figure/dependent}
% }


% \only<2>{
% \includegraphics[width = 0.5\textwidth]{figure/dependence_2}
% }
% % correlation reflects the noisiness and direction of a linear relationship (top row), but not the slope of that relationship (middle), nor many aspects of nonlinear relationships (bottom). N.B.: the figure in the center has a slope of 0 but in that case the correlation coefficient is undefined because the variance of Y is zero.
% \end{frame}


\begin{frame}{Correlation and Dependence}

\textbf{Correlation:} Often Pearson correlation $\rho$ is meant, which only measures linear relationship %between pairs of features

\textbf{Example:} Scatterplot with multivariate distribution (contour lines) and their marginal densities
\vspace{5pt}
\begin{columns}[totalwidth=\textwidth]
\begin{column}{0.5\linewidth}
\centering
$X_1$, $X_2 \sim N(0,1)$ with $\rho(X_1, X_2) = 0$

\includegraphics[width = 0.7\textwidth]{figure/independent}
\end{column}
\begin{column}{0.5\linewidth}
\centering
$X_1$, $X_2 \sim N(0,1)$ with $\rho(X_1, X_2) = 0.8$

\includegraphics[width = 0.7\textwidth]{figure/dependent}
\end{column}
\end{columns}

% \hspace{40pt} $X_1$, $X_2 \sim N(0,1)$ with $\rho(X_1, X_2) = 0$ \hspace{20pt} $X_1$, $X_2 \sim N(0,1)$ with $\rho(X_1, X_2) = 0.8$

% \centering
% \includegraphics[width = 0.4\textwidth]{figure/independent}
% \hspace{20pt}
% \includegraphics[width = 0.4\textwidth]{figure/dependent}

\textbf{But:} Pearson's $\rho$ does not measure non-linear dependencies, e.g., $\rho = 0$ but dependent:

\centering
\includegraphics[width = 0.6\textwidth, trim=0 0 0 190px, clip]{figure/dependence_2}
\end{frame}



\begin{frame}{Correlation and Dependence}
\textbf{Dependence:} Describes general dependence structure of features (e.g., non-linear relationships)

%\textbf{Definition:}
\begin{itemize}
\item Definition: $X_j$, $X_k$ independent $\Leftrightarrow$ joint distribution is product of marginals:\\ \vspace{5pt}
\centerline{$\P(X_j, X_k) = \P(X_j) \cdot \P(X_k)$}
\vspace{5pt}
\item Equivalent definition (knowing $X_k$ does not tell us anything about $X_j$ and vice versa): \\
\vspace{5pt}
\phantom{AAA} $\P(X_j|X_k) = \P(X_j) \text{ and } \P(X_k|X_j) = \P(X_k)$ \hfill (follows from conditional probability) \phantom{AAA}
\vspace{5pt}
%$$\P(X_j|X_k) = \P(X_j) \hfill \text{(knowing $X_k$ does not tell us anything about $X_j$ and vice versa)}$$
%\medskip

\only<1>{
\textbf{Example:} Independent $\Rightarrow$ Conditional distributions at different vertical and horizontal slices (after normalizing area under curve to 1) match their marginal distributions
\begin{columns}[c, totalwidth=\linewidth]
\begin{column}{0.5\textwidth}
\includegraphics[width=\textwidth]{slides/intro/figure/independent_slice.pdf}
\end{column}
\begin{column}{0.5\textwidth}
\includegraphics[width=\textwidth]{slides/intro/figure/dependent_slice.pdf}
\end{column}
\end{columns}
}

\pause
\item \textbf{N.B.:} $X_j$, $X_k$ independent $\Rightarrow$ $\rho(X_j, X_k) = 0$ \textbf{but} $\rho(X_j, X_k) = 0$ $\nRightarrow$ $X_j$, $X_k$  independent
%\item Auf einer slide eklären, visuell und mathematisch
\item General dependence structure difficult to measure but many different measures exist, e.g., \\
Spearman correlation (monotonic), mutual information, or Kernel-based measures (HSCI)
\end{itemize}


\end{frame}



\begin{frame}{Interpretations with Dependent Features}
\begin{itemize}
\item Many interpretation methods are based on 
%rely on varying feature values and create
artificially created data points \\
$\leadsto$ Points can be out-of-distribution or located in low-density regions if features are dependent\\
$\leadsto$ May produce biased explanations that rely on predictions where model extrapolated\\
\only<1>{\centering\includegraphics[width=0.8\textwidth]{figure/extrapolation}}
\pause
\item Highly correlated features contain similar information \\
$\leadsto$ May confuse model so that only one feature is used (even if it is a causally irrelevant one)
%Some models might use only one correlated feature (even if it is causally not relevant)
\\
%Can confuse a model so that only one correlated feature is used (even if it is not relevant) \\
$\leadsto$ Produced explanations can be misleading (may be true to the model but not true to the data) % generating process
\\
$\leadsto$ Even for causally and equally relevant features, produced explanations can be very different
%Even for two causally and equally relevant correlated features it is not guaranteed that their explanations will be similar
% \pause
% \item Assume two causally and equally relevant but highly correlated features \\
% $\leadsto$ One might expect that their explanations (e.g., feature importance) will be similar \\
% %One might expect that explanations of two correlated features (e.g., feature importance) are similar as they share similar information \\
% $\leadsto$ Generally not true as the model can learn different relationships


% \begin{tikzpicture}[remember picture,overlay,shift=(current page.center)] %opacity=.2,text opacity=1
% \node[fill=white, opacity=1,text opacity=1] at (current page.center) {\includegraphics[width=0.6\textwidth]{figure/ridge_lasso}};
% \end{tikzpicture}
\pause 

% \only<3>{
%  \begin{BlueBox}{Example}
%   \begin{columns}[c, totalwidth=\textwidth]
% \begin{column}{0.5\textwidth}
% \includegraphics[trim=0px 110px 30px 0px, clip, width=\textwidth]{figure/ridge_lasso}
% \end{column}
% \begin{column}{0.5\textwidth}
% \includegraphics[trim=0px 0px 0px 115px, clip, width=\textwidth]{figure/ridge_lasso}
% \end{column}
% \end{columns}}
% \end{BlueBox}
% }

\only<3>{\colorbox{blue!20}{\begin{columns}[c, totalwidth=\linewidth]
\begin{column}{0.5\linewidth}
\includegraphics[trim=0px 110px 30px 0px, clip, width=\linewidth]{figure/ridge_lasso}
\end{column}
\begin{column}{0.5\linewidth}
\includegraphics[trim=0px 0px 0px 115px, clip, width=\linewidth]{figure/ridge_lasso}
\end{column}
\end{columns}}}



\pause

\item Be clear about interpretation goal: Explaining the model or underlying relationship in data?
%We need to differentiate between interpretations of a model or reality
\\
$\leadsto$ Beware of the latter which might be distorted by modeling fallacies, \\
e.g., due to data quality, under- and overfitting, or model extrapolation

\end{itemize}
\end{frame}



\begin{frame}{Extrapolation}
\begin{itemize}
%\itemsep1em
\item Predictions in regions with no or few training points are subject to a high uncertainty\\
%\item Essentially, we are interested in the prediction uncertainty\\
%, i.e., the less the training data on a feature subspace, the higher the prediction uncertainty. 
$\leadsto$ Models are rarely capable of quantifying prediction uncertainty
\item There is no definition of when a model extrapolates and to what degree \\
$\leadsto$ Severity of extrapolation depends on model, some extrapolate more than others %\\
%$\leadsto$ Some models might extrapolate more than others
\item Density of training data might serve as proxy to identify regions where extrapolation is likely\\
$\leadsto$ Density estimation in many dimensions is often infeasible
\end{itemize}

\end{frame}
% \begin{frame}{Extrapolation}
% \begin{itemize}
% %\itemsep1em
% \item Predictions of an ML model in regions with a low density of training points are subject to a high variance.
% \item Essentially we are interested in the prediction uncertainty, i.e., the less the training data on a feature subspace, the higher the prediction uncertainty. Unfortunately, virtually no ML models are capable of quantifying prediction uncertainty.
% \item There is no consensus definition of when a model extrapolates and to what degree. Furthermore, the severity of the problem depends on the model itself. Some models might extrapolate more reliably than others.
% \item Theoretically, we could use the training density as a proxy. However, density estimation in many dimensions is often infeasible.
% \end{itemize}

% \end{frame}

%%% dann das gleiche für interactions

%% wir können auch ein bsp mit feature importannce machen, wo korrelation alles schwieriger macht. im guyon paper und im i2ml featsel teil ist es drin

%% was kann mindestens zu korrel sagen:
%
% a) das extrapol problem. haben wir folien zu
% b) bei korrel kann man sagen: die vars haben die gkleich info, also breaucht man nur eine. das ist eine explanation. die kann aber falsch sein. siehe guyon bsp
% c) bei korrel könnte man sagen: die erklärungen zu vars sind die gleichen. zb deren importance. das ist aber auch nicht gegeben
% ---> generell: vorsicht!

%--------------------------------

% bei interactions:
% 1) definition davon bringen
% man kann das operationals / an der modell struktur erklären. als bsp lin modell und tree
% allgemeine definition von fanoava
% sagen dass eine funktin keine IA hat, wenn sie separierbar. verbindung zur optimierung bringen
% sagen dass man aus der fanbova die interactioons ablesen kann und auf die H-statistic für weiteres vereweisen



% \begin{frame}{Correlation vs. Interaction}
% \begin{itemize}
% %\itemsep2em
% \item Correlated (or dependent) features and feature interactions can cause misleading explanations. %One needs to be careful not to confuse them.
% \item Correlated features implies joint is not product of marginals: $\text{PDF}(x_1, \dots, x_p) \neq \text{PDF}(x_1) \cdot \ldots \cdot \text{PDF}(x_p)$.
% \item
% Many interpretation methods rely on varying feature values and may create data points that are out-of-distribution or located in low-density regions if features are correlated.\\
% $\leadsto$ May produce biased explanations that rely on predictions where the model extrapolated.
% % \item An interaction is a product term between features inside the prediction function. As such, interactions are detached from the constitution of the data, i.e., regardloss of the degree of correlation, the effect of a feature on the target will depend on the values of one or multiple other features.
% \end{itemize}
% \end{frame}

% \begin{frame}{Correlation vs. Interaction}

% 1000 randomly sampled observations with positive correlation between $x_1$ and $x_2$:
% \medskip
% \begin{figure}
% \includegraphics[width = 0.7\textwidth]{figure/correlation}
% \end{figure}
% \end{frame}

\begin{frame}{Feature Interactions}
\begin{itemize}
%\itemsep2em
\item While feature dependencies concern data distribution, feature interactions occur in structure of model or DGP (e.g., functional relationship between $X$ and $\fh(X)$ or $X$ and $f(X)$)\\
$\leadsto$ Feature dependencies may lead to feature interactions in a model

%\item Although correlation concerns the data and interactions the model, they are often connected as correlations in the training data are identified by the learning algorithm.
\item Number of potential interactions in a model increases exponentially with number of features \\
$\leadsto$ Interactions are difficult to identify, especially if feature dependencies are also present

\item With interactions present, a feature's effect on the prediction depends on other features\\
$\leadsto$
$\fh(x) = x_1 x_2$ $\Rightarrow$ Effect of $x_1$ on $\fh$ depends on $x_2$ and vice versa

\begin{columns}[T, totalwidth=\textwidth]
\begin{column}{0.5\textwidth}
\centering
\begin{tikzpicture}[scale=0.75, transform shape]
   \usetikzlibrary{arrows}
    \usetikzlibrary{shapes}
     \tikzset{treenode/.style={draw, circle, font=\small}}
     \tikzset{line/.style={draw, thick}}
     \node [treenode, draw=red] (a0) {};
     \node [treenode, below=0.75cm of a0, xshift=-1cm]  (a1) {};
     \node [treenode, below=0.75cm of a0, xshift=1cm]  (a2) {};
     
     %\node [treenode, below=0.75cm of a2, xshift=-1cm] (a3) {};
     %\node [treenode, below=0.75cm of a2, xshift=1cm]  (a4) {};
 
     \path [line] (a0.south) -- + (0,-0.4cm) -| (a1.north) node [midway, above] {$x_1<3$};
     \path [line] (a0.south) -- +(0,-0.4cm) -|  (a2.north) node [midway, above] {$x_1\geq3$};
     
     \path (a1.south) -- +(0,0) -|  (a1.south) node [midway, below] {$f_1(x_1)$};
     \path (a2.south) -- +(0,0) -|  (a2.south) node [midway, below] {$f_1(x_1)$};
    %  \path [line] (a2.south) -- + (0,-0.4cm) -| (a3.north) node [midway, above] {$x_2<6$};;
    %  \path [line] (a2.south) -- +(0,-0.4cm) -|  (a4.north) node [midway, above] {$x_2\geq6$};
     
   \end{tikzpicture}
\end{column}
\begin{column}{0.5\textwidth}

\centering
\begin{tikzpicture}[scale=0.75, transform shape]
   \usetikzlibrary{arrows}
    \usetikzlibrary{shapes}
     \tikzset{treenode/.style={draw, circle, font=\small}}
     \tikzset{line/.style={draw, thick}}
     \node [treenode, draw=red] (a0) {};
     \node [treenode, below=0.75cm of a0, xshift=-1.5cm]  (a1) {};
     \node [treenode, below=0.75cm of a0, xshift=1.5cm]  (a2) {};
     
     \node [treenode, below=0.75cm of a2, xshift=-0.75cm] (a3) {};
     \node [treenode, below=0.75cm of a2, xshift=0.75cm]  (a4) {};
     
    \node [treenode, below=0.75cm of a1, xshift=-0.75cm] (a5) {};
    \node [treenode, below=0.75cm of a1, xshift=0.75cm]  (a6) {};
     
     \path [line] (a0.south) -- + (0,-0.4cm) -| (a1.north) node [midway, above] {$x_1<3$};
     \path [line] (a0.south) -- +(0,-0.4cm) -|  (a2.north) node [midway, above] {$x_1\geq3$};
     
     \path [line] (a2.south) -- + (0,-0.4cm) -| (a3.north) node [midway, above] {$x_2<6$};;
     \path [line] (a2.south) -- +(0,-0.4cm) -|  (a4.north) node [midway, above] {$x_2\geq6$};
     
    \path [line] (a1.south) -- + (0,-0.4cm) -| (a5.north) node [midway, above] {$x_3<2$};;
    \path [line] (a1.south) -- +(0,-0.4cm) -|  (a6.north) node [midway, above] {$x_3\geq2$};
    
    %\path (a5.south) -- +(0,0) -|  (a5.south) node [midway, below] {$f_{1,3}(x_1,x_3)$};
    %\path (a6.south) -- +(0,0) -|  (a6.south) node [midway, below] {$f_{1,3}(x_1,x_3)$};
     
    %\path (a3.south) -- +(0,0) -|  (a3.south) node [midway, below] {$f_{1,2}(x_1,x_2)$};
    %\path (a3.south) -- +(0,0) -|  (a4.south) node [midway, below] {$f_{1,2}(x_1,x_2)$};
    \path (a3) edge [bend right, draw=white] node [below] {$f_{1,2}(x_1,x_2)$} (a4);
    \path (a5) edge [bend right, draw=white] node [below] {$f_{1,3}(x_1,x_3)$} (a6);
   \end{tikzpicture}
\end{column}
\end{columns}
\end{itemize}
\end{frame}

\begin{frame}{Feature Interactions \citebutton{Friedman and Popescu (2008)}{https://doi.org/10.1214/07-AOAS148}}

%A function $f(\xv)$ is said to exhibit an interaction between two of its variables $x_j$ and $x_k$ if the difference in the value of $f(\xv)$ as a result of changing the value of $x_j$ depends on the value of $x_k$. For numeric variables, this can be expressed as
A function $f(\xv)$ contains an interaction between $x_j$ and $x_k$ if changes in $f(\xv)$ due to changes of values of $x_j$ also depend on $x_k$. For numeric variables, this can be expressed as

$$\mathbb{E}_X \left[ \frac{\partial^2 f(\xv)}{\partial x_j \partial x_k} \right]^2 > 0$$

$\leadsto$ No interaction: 

Higher-order interactions:

Shape of the prediction function for a two-way interaction:

\begin{figure}
\includegraphics[width = 0.5\textwidth]{figure/interaction}
\end{figure}
\end{frame}


\begin{frame}{Feature Interactions \citebutton{Friedman and Popescu (2008)}{https://doi.org/10.1214/07-AOAS148}}


Higher-order interactions:

Shape of the prediction function for a two-way interaction:

\begin{figure}
\includegraphics[width = 0.5\textwidth]{figure/interaction}
\end{figure}
\end{frame}


% \begin{frame}{Pitfalls of Interpretation Methods}
% \begin{itemize}
% %\itemsep1em
% \item Many methods in IML are theoretically defined for uncorrelated features, e.g., the PD or the PFI.
% \item In practice, the features are usually correlated, but methods are applied regardless of potential misinterpretations.
% \item Many people call for a more careful approach to conduct model interpretations, either by using intrinsically interpretable models (Rudin, 2019), or by avoiding feature permutations (Hooker, 2021).
% \item There are many potential pitfalls to consider when interpreting ML models (Molnar, 2021).
% \\
% $\rightarrow$ Know the theory and be careful!

% \footnote[frame]{Molnar, Christoph \& König, Gunnar \& Herbinger, Julia \& Freiesleben, Timo \& Dandl, Susanne \& Scholbeck, Christian \& Casalicchio, Giuseppe \& Grosse-Wentrup, Moritz \& Bischl, Bernd. (2020). Pitfalls to Avoid when Interpreting Machine Learning Models.}
% \footnote[frame]{Rudin, C. Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead. Nat Mach Intell 1, 206–215 (2019).}
% \footnote[frame]{Hooker, Giles \& Mentch, Lucas \& Zhou, Siyu. (2021). Unrestricted permutation forces extrapolation: variable importance requires at least one more model, or there is no free variable importance. Statistics and Computing. 31. 10.1007/s11222-021-10057-z.}
% \end{itemize}

% \end{frame}

\begin{frame}[t]{Pitfalls and Best Practices \citebutton{Molnar et. al (2021)}{https://arxiv.org/abs/2007.04131v2}}
\begin{itemize}
    \item \textbf{Proper training and evaluation}: 
    To gain insights into data generating process, deployed model should at least generalize well to unseen data (garbage in, garbage out)
    %Model interpretation is only as good as the underlying model (garbage in, garbage out).
    \item \textbf{Avoid unnecessary complexity}: Prefer simple interpretable models and use them as baseline
    \item \textbf{Quantify uncertainty}: Interpretation methods are often (statistical) estimators \\
    $\leadsto$ Beware of uncertainty, we may need confidence intervals
    %To avoid interpretation of noise, include uncertainty estimates for the interpretations, e.g.,  confidence intervals for feature importance.
    \item \textbf{Careful with causality}: 
     Do you want to understand the model or the nature of DGP?\\
     $\leadsto$ Your goal should guide the choice of interpretation method
     %Causal interpretation requires assumptions about relationships in the data and a corresponding model considerations (e.g., including confounders into the model).
    \item \textbf{Consider feature dependence}: Some interpretation methods suffer when features are dependent. Check presence of dependencies and use suitable methods.
    \item \textbf{Be aware of simplifications}: Interpretation methods map from complex models to low-dim. explanations. $\Rightarrow$ Information loss, e.g., some interpretation methods hide interactions.
\end{itemize}

\end{frame}

\endlecture
\end{document}
