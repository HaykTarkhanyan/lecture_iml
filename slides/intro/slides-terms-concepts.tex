\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\usepackage[]{graphicx}\usepackage[]{color}
% maxwidth is the original width if it is less than linewidth
% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
\ifdim\Gin@nat@width>\linewidth
\linewidth
\else
\Gin@nat@width
\fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%
\newcommand{\predvar}{Var\left[\hat{f}(x)\right]}
\let\hlipl\hlkwb

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
\def\at@end@of@kframe{}%
\ifinner\ifhmode%
\def\at@end@of@kframe{\end{minipage}}%
\begin{minipage}{\columnwidth}%
\fi\fi%
\def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
\colorbox{shadecolor}{##1}\hskip-\fboxsep
% There is no \\@totalrightmargin, so:
\hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
\MakeFramed {\advance\hsize-\width
\@totalleftmargin\z@ \linewidth\hsize
\@setminipage}}%
{\par\unskip\endMakeFramed%
\at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}
\newcommand{\SweaveOpts}[1]{}  % do not interfere with LaTeX
\newcommand{\SweaveInput}[1]{} % because they are not real TeX commands
\newcommand{\Sexpr}[1]{}       % will only be parsed by R

\usepackage[english]{babel}
\usepackage[utf8]{inputenc}

\usepackage{dsfont}
\usepackage{verbatim}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{bm}
\usepackage{csquotes}
\usepackage{multirow}
\usepackage{longtable}
\usepackage{booktabs}
\usepackage{enumerate}
\usepackage[absolute,overlay]{textpos}
\usepackage{psfrag}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{eqnarray}
\usepackage{arydshln}
\usepackage{tabularx}
\usepackage{placeins}
\usepackage{tikz}
\usepackage{setspace}
\usepackage{colortbl}
\usepackage{mathtools}
\usepackage{wrapfig}
\usepackage{bm}

\usetikzlibrary{shapes,arrows,automata,positioning,calc,chains,trees, shadows}
\tikzset{
%Define standard arrow tip
>=stealth',
%Define style for boxes
punkt/.style={
rectangle,
rounded corners,
draw=black, very thick,
text width=6.5em,
minimum height=2em,
text centered},
% Define arrow style
pil/.style={
->,
thick,
shorten <=2pt,
shorten >=2pt,}
}

\usepackage{subfig}

% Defines macros and environments
\input{../../style/common.tex}

%\usetheme{lmu-lecture}
\usepackage{../../style/lmu-lecture}

\let\code=\texttt
\let\proglang=\textsf

\setkeys{Gin}{width=0.9\textwidth}

\title{Interpretable Machine Learning}
% \author{Bernd Bischl, Christoph Molnar, Daniel Schalk, Fabian Scheipl}
\institute{\href{https://compstat-lmu.github.io/lecture_iml/}{compstat-lmu.github.io/lecture\_iml}}
\date{}

\setbeamertemplate{frametitle}{\expandafter\uppercase\expandafter\insertframetitle}

\begin{document}

\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}
\newcommand{\titlefigure}{figure/open_blackbox}
\newcommand{\learninggoals}{
\item What is interpretable machine learning (IML) and Explainable Artificial Intelligence (XAI)?
\item What is interpretability?
\item What is the purpose of IML?
\item What are the fundamental terms and concepts of IML?}

\lecturechapter{Fundamental Terms and Concepts}
\lecture{Interpretable Machine Learning}

\begin{vbframe}{Interpretable Machine Learning}
\begin{itemize}
\itemsep2em
\item Machine learning (ML) algorithms algorithmically train predictive models with no or little pre-specifications and assumptions about the data.
\item Several algorithms such as decision tree learning create interpretable models. However, most algorithms create models which can be considered a black box.
\item We use the term black box, although the internal workings of the model are in fact accessible, but too complex for the human mind to comprehend. 
\item Interpretable machine learning (IML) is an umbrella term for all models and methods that allow for some kind of interpretation.
\end{itemize}
\end{vbframe}

\begin{vbframe}{Explainable AI}
\begin{itemize}
\itemsep2em
\item IML is often used synonymously with Explainable AI (XAI). There is no unified standard for these terminologies. We find that XAI often is specifically concerned with the interpretation of neural networks, whereas IML is used as an encompassing term for everything related to model interpretability, i.e., interpretable models such as generalized additive models, model-agnostic techniques, as well as interpretations of neural networks.
\item The nature of neural networks allows for powerful model-specific interpretation techniques, e.g., layer-wise relevance propagation (LRP) and saliency maps. They have in common that influence on the output layer is backpropagated through the entire network layer by layer up to the input layer.
%\item Covering XAI would exceed the timeframe of a lecture. As model-agnostic techniques are both versatile, and receive a lot of attention in industry and academia, this lecture will concentrate on model-agnostic techniques.
\end{itemize}
\end{vbframe}

\begin{vbframe}{XAI - Saliency Maps}

\begin{itemize}
\item For visual data, i.e., pixels being represented as a matrix, deep learning has proven to deliver remarkable outperformance over other model types.
\item A saliency map is a heatmap indicating pixel influence on the prediction (e.g., a classification of an image): \footnote[frame]{Mundhenk, T., Chen, B.Y., Friedland, G. (2019). Efficient Saliency Maps for Explainable AI. ArXiv, abs/1911.11293.
}

\begin{figure}
\includegraphics[width = 0.8 \textwidth]{figure/saliencymap}
\end{figure}
\end{itemize}
\end{vbframe}

\begin{vbframe}{What is Interpretability?}
\begin{itemize}
\itemsep1em
\item There is no scientific consensus on the definition of interpretability.
\item We need to differentiate between interpretations of a model or reality. The latter is distorted by all modeling fallacies involved in predictive modeling, e.g., data quality, under- and overfitting, or model extrapolations. 
\item For practical purposes, we decide to define interpretability such that it is beneficial to our modeling tasks. Think back to the foundations of statistical modeling:  the linear regression model (LM). The LM, with its known equation of beta coefficients, represents a paradigm for statistical interpretability.
\item It follows that it would be beneficial to create techniques that give us an interpretation similar to the one of an LM.
\end{itemize}
\end{vbframe}

\begin{vbframe}{Fundamental Terms and Concepts}
\begin{itemize}
\itemsep1em
\item \textbf{Feature effects and importance:} An effect indicates the direction and magnitude of a change in predicted outcome due to changes in feature values. The importance indicates the contribution of one or multiple features to the model performance, or variance in predicted outcome.
\item \textbf{Local and global:} A local interpretation is an interpretation for a single observation, or data instance, i.e., a single location in multidimensional feature space. A global interpretation is an interpretation for the entire feature space. Furthermore, increasing emphasis is put on semi-global interpretations, which are interpretations on feature subspaces.
\item \textbf{Model-specific and model-agnostic:} A model-specific method is tied to the use of a specific model. A model-agnostic one can be applied to any predictive model.
\end{itemize}
\end{vbframe}

\begin{vbframe}{What tools do we have?}
 \begin{center}
  \includegraphics[width=0.9\textwidth]{figure/overview}
 \end{center}
 $\Rightarrow$ Due to their versatility, both industry and academia are highly interested in model-agnostic methods, which we will focus on.
\end{vbframe}

\begin{vbframe}{Correlation versus Interaction}
\begin{itemize}
\item A lot of problems in IML arise due to correlated features and feature interactions. One needs to be careful not to confuse them.
\item Correlated features imply that the joint probability density function (PDF) of a set of $p$ features is not a product of their marginal distributions, i.e., $\mathcal{F}(x_1, \dots, x_p) \neq \mathcal{F}(x_1) \cdot \ldots \cdot \mathcal{F}(x_p)$. This often becomes a problem as many interpretation methods rely on model evaluations with recombinations of feature values. If the training data is correlated, these recombinations are located in low training density areas of the feature space, which results in model extrapolations.
\item An interaction is a product term between features inside the prediction function. As such, interactions are detached from the constitution of the data, i.e., regardloss of the degree of correlation, the effect of a feature on the target will depend on the values of one or multiple other features.
\end{itemize}
\end{vbframe}

\begin{vbframe}{Correlation}

1000 randomly sampled observations with correlation between both features:

\begin{figure}
\includegraphics[width = 0.7\textwidth]{figure/correlation}
\end{figure}
\end{vbframe}

\begin{vbframe}{Interaction}

Shape of the prediction function for a two-way interaction:

\begin{figure}
\includegraphics[width = 0.6\textwidth]{figure/interaction}
\end{figure}
\end{vbframe}


\begin{vbframe}{Extrapolation}
\begin{itemize}
\itemsep2em
\item As we do not assume any data generating process, ML models cannot extrapolate reliably in areas where the model was not trained with a sufficient amount of training data. The resulting predictions are subject to a high variance.
\item If we solely wish to interpret the model, this is not an issue. However, in most applications we wish to use model interpretations to evaluate some kind of real-world phenomenon that generated the data, i.e., conduct inference. In such a case, interpretations based on model extrapolations can be misleading.
\item In IML, extrapolation issues often arise due to recombining values of correlated features.
\item Model extrapolation essentially corresponds to prediction uncertainty, i.e., the less the training data on a feature subspace, the higher the prediction uncertainty and thus model extrapolation. Unfortunately, virtually no ML models are capable of quantifying prediction uncertainty.
\item There is no consensus definition of when a model extrapolates and to what degree. Furthermore, the severity of the problem depends on the model itself. Some models might extrapolate more reliably than others.
\item Theoretically, we could use the training density as a proxy. However, density estimation in high dimensions is extremely infeasible.
\end{itemize}

\end{vbframe}

% \begin{vbframe}{Intrinsic and Model-Agnostic Interpretation}
% \begin{itemize}
%   \item Intrinsically interpretable models:
%   \begin{itemize}
%   \item Examples are linear models and decision trees.
%   \item They are interpretable because of their simple structures, e.g.,
%   a weighted combination of feature values or a tree structure. 
%   \item However, they are difficult to interpret with many features or complex interaction terms.
%   \end{itemize}
%   \lz
%   \item Model-agnostic interpretation methods:
%   \begin{itemize}
%   \item They are applied after training (post-hoc).
%   \item They also work for more complex black box models.
%   \item They can also be applied to intrinsically interpretable models, e.g.
%     feature importance for decision trees. 
%   \end{itemize}
% \end{itemize}
% \end{vbframe}
% 
% \begin{vbframe}{Model-Agnostic Interpretability}
%  \begin{itemize}
%   \itemsep2em
%   \item Model-agnostic interpretability methods work for \textbf{any} kind of machine learning model.
%   \item Explanation type is not tied to the underlying model type.
%   \item Often, only access to data and fitted predictor is required. No further knowledge about the model itself is necessary.
%   \item We usually distinguish between \textbf{feature effect} and \textbf{feature importance} methods.
%  \end{itemize}
% \end{vbframe}
% 
% 
% \begin{vbframe}{Feature Effects vs. Feature Importance}
% \textbf{Feature effects} indicate the direction and magnitude of a change in predicted outcome due to changes in feature values.
% \begin{center}
% \includegraphics[page=1, width=\textwidth]{figure/feature-effects}
% \end{center}
%   \begin{itemize}
%     \item Methods include: Partial Dependence Plots, Individual Conditional Expectation, Accumulated Local Effects (ALE)
%     \item Pendant in linear models: Regression coefficient $\hat{\theta}_j$
%   \end{itemize}
% \framebreak
% 
% \textbf{Feature importance} methods rank features by how much they contribute to the predictive performance or prediction variance of the model.
% \begin{columns}
% \begin{column}{0.6\textwidth}
% \begin{itemize}
%     \itemsep1em
%     \item Methods include: Permutation Feature Importance, Functional Anova
%     \item Analog in linear models: Absolute t-statistic $\left|\frac{\hat{\theta}_j}{SE(\hat{\theta}_j)}\right|$
% \end{itemize}
% \end{column}
% \begin{column}{0.4\textwidth}
% \begin{center}
% \includegraphics[page=1, width=\textwidth]{figure/feature-importance}
% \end{center}
% \end{column}
% \end{columns}
% \end{vbframe}
% 
% 
% \begin{vbframe}{Global and Local Interpretability}
% Global interpretability methods explain the expected model behavior for the entire feature space by considering all available observations (or representative subsets). For example:
%   \begin{itemize}
%     \item Permutation Feature Importance
%     \item Partial Dependence Plot
%     \item Functional Anova
%     \item ...
%   \end{itemize}
% \lz
% Local interpretability methods explain single predictions or a group of similar observations. For example:
%  \begin{itemize}
%   \item Individual Conditional Expectation (ICE) Plots
%   \item Local Interpretable Model-Agnostic Explanations (LIME)
%   \item Shapley Values
%   \item ...
%  \end{itemize}
% \end{vbframe}
% 
% 
% \begin{vbframe}{Fixed model vs. refits}
%   \begin{itemize}
%      \itemsep2em
%      \item Most methods presented in this lecture analyze a fixed, trained model 
%      (e.g., permutation feature importance).
%      \item Some methods require refitting the model (e.g., PIMP).
%      \item Trained model $\Rightarrow$ Model is the object of analysis.
%      \item Refitting $\Rightarrow$ Learning process is the object of analysis.
%      \item The advantage of refitting is that it includes information about the variability in the learning process.
%   \end{itemize}
% \end{vbframe}


\endlecture
\end{document}
