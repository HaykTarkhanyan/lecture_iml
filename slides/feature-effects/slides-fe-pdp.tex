\documentclass[11pt,compress,t,notes=noshow, aspectratio=169, xcolor=table]{beamer}

\usepackage{../../style/lmu-lecture}
% Defines macros and environments
\input{../../style/common.tex}

\title{Interpretable Machine Learning}
% \author{LMU}
%\institute{\href{https://compstat-lmu.github.io/lecture_iml/}{compstat-lmu.github.io/lecture\_iml}}
\date{}

\begin{document}

\newcommand{\titlefigure}{figure/pdp_bike}
\newcommand{\learninggoals}{
\item PD plots and relation to ICE plots
\item Interpretation of PDP
}

\lecturechapter{Partial Dependence (PD) plot}
\lecture{Interpretable Machine Learning}

\begin{frame}{Partial Dependence (PD) \citebutton{Friedman (2001)}{https://www.jstor.org/stable/2699986}}

\textbf{Definition:} PD function is expectation of $\fh(x_S, \xv_C)$ w.r.t. marginal distribution of features $\xv_C$:
$$f_{S, PD}(x_S) = \E_{\xv_C} \left( \fh(x_S, \xv_C) \right) = \int_{-\infty}^{\infty} \fh(x_S, \xv_C) \, d\P(\xv_C)$$

\textbf{Estimation:} For a grid value $x_S^*$, average ICE curves point-wise at $x_S^*$ over all observed $\xv_C^{(i)}$:
$$\fh_{S, PD}(x_S^*) = \frac{1}{n} \sum_{i=1}^n \fh(x_S^*, \xv_C^{(i)})$$

%Within the SIPA framework, the partial dependence builds the \textbf{Aggregation} step.

%\footnote[frame]{Friedman, Jerome H. (2001). Greedy Function Approximation: A Gradient Boosting Machine. Annals of Statistics: 1189-1232.}
%\footnote[frame]{Scholbeck, C. A., Molnar, C., Heumann, C., Bischl, B., and Casalicchio, G. (2019). Sampling, Intervention, Prediction, Aggregation: A Generalized Framework for Model Agnostic Interpretations. ECML PKDD 2019. (pp. 205-216).}
\end{frame}


\frame{
\frametitle{Example: Partial Dependence for LM}

Assume a linear regression model with two features:

$$\fh(\xv) = \fh(\xv_1, \xv_2) = \hat\beta_1 \xv_1 + \hat\beta_2 \xv_2 + \hat\beta_0$$

PD function for feature of interest $S = \{1\}$ (with $C = \{2\}$) is:

$$
\begin{aligned}
f_{1, PD}(\xv_1) = \E_{\xv_2} \left( \fh(\xv_1, \xv_2) \right) &= \int_{-\infty}^{\infty} \left( \hat\beta_1 \xv_1 + \hat\beta_2 \xv_2 + \hat\beta_0 \right) \, d\P(\xv_2) \\
&= \hat\beta_1 \xv_1 + \hat\beta_2 \cdot \int_{-\infty}^{\infty} \xv_2 \, d\P(\xv_2) + \hat\beta_0 \\
&= \hat\beta_1 \xv_1 + \underbrace{\hat\beta_2 \cdot \E_{\xv_2} (\xv_2) + \hat\beta_0}_{:= const}
\end{aligned}
$$

$\Rightarrow$ PD plot visualizes the function $f_{1, PD}(\xv_1) = \hat\beta_1 \xv_1 + const$ ($\hat =$ feature effect of $\xv_1$).

}

\frame{
\frametitle{Partial Dependence}
%\begin{onlyenv}
\begin{columns}[T]
\begin{column}{0.5\textwidth}
\centering
\only<1>{
\includegraphics[page=8, width=0.8\textwidth]{figure_man/ice_pd_plot_demo}
\end{column}
\begin{column}{0.5\textwidth}

\begin{center}
\includegraphics[page=1, width=\textwidth]{figure/PD}
\end{center}
}

\only<2>{
\includegraphics[page=9, width=0.8\textwidth]{figure_man/ice_pd_plot_demo}
\end{column}
\begin{column}{0.5\textwidth}

\begin{center}
\includegraphics[page=2, width=\textwidth]{figure/PD}
\end{center}
}

\only<3>{
\includegraphics[page=10, width=0.8\textwidth]{figure_man/ice_pd_plot_demo}
\end{column}
\begin{column}{0.5\textwidth}

\begin{center}
\includegraphics[page=3, width=\textwidth]{figure/PD}
\end{center}
}
\end{column}
\end{columns}
%\end{onlyenv}

\only<1>{
Estimate PD function by \textbf{point-wise} average of ICE curves at grid value \fcolorbox{red}{white}{$x_S^* = x_1^* = 1$}:
$$\textstyle\fh_{1, PD}(x_1^*) = \frac{1}{n} \sum_{i=1}^n \fh(x_1^*, \xv_{2, 3}^{(i)})$$
}
\only<2>{
Estimate PD function by \textbf{point-wise} average of ICE curves at grid value \fcolorbox{red}{white}{$x_S^* = x_1^* = 2$}:
$$\textstyle \fh_{1, PD}(x_1^*) = \frac{1}{n} \sum_{i=1}^n \fh(x_1^*, \xv_{2, 3}^{(i)})$$
}
\only<3>{
Estimate PD function by \textbf{point-wise} average of ICE curves at grid value \fcolorbox{red}{white}{$x_S^* = x_1^* = 3$}:
$$\textstyle\fh_{1, PD}(x_1^*) = \frac{1}{n} \sum_{i=1}^n \fh(x_1^*, \xv_{2, 3}^{(i)})$$
}
}










\frame{
\frametitle{Interpretation: PD and ICE}
\begin{center}
\includegraphics[width=0.65\textwidth]{figure/pdp_bike}
\end{center}


\begin{onlyenv}
\only<1>{
If feature varies:\\
\begin{itemize}
\item
  \textbf{ICE:} How does \textbf{prediction of
  individual observation} change? \(\Rightarrow\) \textbf{local} interpretation
\item
  \textbf{PD:} How does \textbf{average effect / expected prediction} change? \(\Rightarrow\) \textbf{global} interpretation
\end{itemize}
}

\only<2>{
Insights from bike sharing data:
\begin{itemize}
%  \item Averaging ICE curves yields PD plot (yellow curve)
  \item Parallel ICE curves = homogeneous effect
  \item Warmer $\Rightarrow$ more rented bikes
  \item Too hot $\Rightarrow$ slightly less bikes 
\end{itemize}
}
\end{onlyenv}
}


\frame{
\frametitle{Categorical Features}

\begin{center}
\includegraphics[width=0.9\textwidth]{figure/pdp_ice_cat}
\end{center}

\begin{itemize}
\item PDP with boxplots and ICE with parallel coordinates plots
%lot visualizes expected prediction if all observations would belong to a certain category (and shows distribution of predictions in a box plot)
\item NB: Categories can be unordered, if so, rather compare pairwise
\end{itemize}
%ICE plot connects predictions of individual observations (might identify heterogenous effects between categories if many lines are crossing)\\
%to identify heterogenous effects between categories (here: lines between SPRING and SUMMER are heterogenous).\\

}

\begin{frame}{2D Partial Dependence}

\begin{center}
\includegraphics[width=0.5\textwidth]{figure/pdp2d_bike}
\end{center}

\begin{itemize}
 \item Humidity and temperature interact with each other
 \item Low humidity + high temp < 27 $^{\circ}$C $\Rightarrow$ many rented bikes
 %Many rented bikesThe number of bike rentals is especially high when humidity is below 75 percent and temperature is between 15 $^{\circ}$C and 27 $^{\circ}$C
\end{itemize}


% \framebreak
%
% add datapoints to previous figure and mention convex hull (see pdp package)

\end{frame}

\endlecture
\end{document}
