\documentclass[11pt,compress,t,notes=noshow, aspectratio=169, xcolor=table]{beamer}

\usepackage{../../style/lmu-lecture}
% Defines macros and environments
\input{../../style/common.tex}

\title{Interpretable Machine Learning}
% \author{LMU}
%\institute{\href{https://compstat-lmu.github.io/lecture_iml/}{compstat-lmu.github.io/lecture\_iml}}
\date{}

\begin{document}

\newcommand{\titlefigure}{figure_man/me_movement}
\newcommand{\learninggoals}{
\item Why parameter-based interpretations are not always possible for parametric models
\item How marginal effects can be used in such cases
\item Drawbacks of marginal effects
\item Model-agnostic applicability}

\lecturechapter{Marginal Effects}
\lecture{Interpretable Machine Learning}

\begin{vbframe}{Interpretations of Linear Models}

\begin{itemize}
\itemsep2em
\item The LM can be directly interpreted by evaluating the model coefficients:
\begin{equation*}
y = \beta_0 + \beta_1 x_1 + \dots + \beta_p x_p + \dots + \epsilon
\end{equation*}
\item A change in $x_1$ by $\Delta x_1$ results in a change in $y$ by $\Delta y = \Delta x_1 \cdot \beta_1$.
\item Default interpretations correspond to $\Delta x_1 = 1$, i.e., $\Delta y = \beta$.
\item All interpretations are done ceteris paribus, i.e., all remaining features are kept constant.
\end{itemize}
\end{vbframe}

\begin{vbframe}{Interpretations of Polynomial Models}


\begin{itemize}
\itemsep2em
\item If higher-order terms or interactions are present, parameter-based interpretations are not possible anymore:
\begin{equation}
y = \beta_0 + \beta_{1} x_1^2 + \beta_{2} x_2^2 + \beta_{1, 2} x_1, x_2 + \epsilon
\label{eq:poly_model}
\end{equation}
\item The isolated main effects of both features vary across different values
\item The interaction depends on values of the remaining feature
\item The marginal effect (ME) allows us to determine a feature effect nonetheless.
\end{itemize}

\end{vbframe}

\begin{vbframe}{Marginal Effects}

\begin{itemize}
\itemsep2em
\item
The most common definition of the marginal effect (ME) corresponds to the derivative of the prediction function w.r.t. a feature. In this lecture, we refer to this variant as the derivative ME (dME). The dME can be computed model-agnostically via numeric differentiation, e.g., with a symmetric difference quotient:
\begin{align*}
dME_j(x) &= \frac{\partial f(x)}{\partial x_j} \\
dME_j(x) &\approx \frac{f(x + h) - f(x - h)}{2h} \\
\end{align*}
\item For Eq. (\ref{eq:poly_model}), the dME corresponds to:
\begin{equation*}
dME_j(x) = 2\beta_1 x_1 +  \beta_{1, 2} x_2
\end{equation*}
\item
A less commonly  known definition corresponds to the change in predicted outcome due to an intervention in the data, e.g., by increasing a feature value by one unit. As this variant corresponds to a forward difference, we refer to it as a forward ME (fME) in this lecture:
\begin{equation*}
fME_j(x, h_j) = f(x_1, \dots, x_j + h_j, \dots, x_p) - f(x)
\end{equation*}
\item For Eq. (\ref{eq:poly_model}), the fME of $x_1$ with step size 1 correspond to:
\begin{equation*}
fME_j(x, h_j) = 2\beta_1 h_1^2 +  \beta_{1, 2} x_2 h_1
\end{equation*}
\end{itemize}
\end{vbframe}

\begin{vbframe}{Derivative versus Forward Difference}

\begin{itemize}
\itemsep2em
\item The dME is not suited to interpret non-linear prediction functions, as the derivative of the prediction function at one point may be substantially different at another.
\item The fME is better suited for non-linear prediction functions. It essentially corresponds to a movement on the prediction function, indicating changes in predicted outcome regardless of the function's shape.
\item However, with both variants, we lose information about the prediction function along the finite difference.
\end{itemize}
\end{vbframe}

\begin{vbframe}{Derivative versus Forward Difference}
\begin{figure}
  \includegraphics[width = 0.65\textwidth]{figure_man/derivative_me_error.png}
\end{figure}
\end{vbframe}

\begin{vbframe}{Additive Recovery}

\begin{itemize}
\itemsep2em
\item Due being based on a finite difference, both variants only recover terms within the prediction function that depend on the feature(s) of interest.
\item Consider a prediction function $\widehat{f}(x) = ax_1 + bx_2$. It follows that:
\begin{align*}
dME_1(x) &= a \\
fME_1(x, h_1) &= ah_1
\end{align*}
\item The ME removes effects of other features that are linked additively, regardless of how many remaining features exist, and their effect structure.
\end{itemize}

\end{vbframe}


\begin{vbframe}{Model-Agnostic Applicability}

\begin{itemize}
\itemsep2em
\item MEs are traditionally used to interpret parametric models such as GLMs. However, both dME and fME can be used as model-agnostic interpretation tools for non-parametric models. As the fME is better suited for non-linear prediction functions, it is the natural choice for black box ML models.
\item The fME essentially correspond to an exploration of the prediction function. We can also use multivariate changes in feature values to explore the prediction surface in various directions simultaneously.
\item One needs to keep in mind that the shape of the prediction function may vary considerably along the forward difference, i.e., an fME with half the step size may not result in half the fME.
\end{itemize}

\end{vbframe}


\begin{vbframe}{Model-Agnostic Applicability}

\begin{itemize}
\itemsep2em
\item MEs are traditionally used to interpret parametric models such as GLMs. However, both dME and fME can be used as model-agnostic interpretation tools for non-parametric models. As the fME is better suited for non-linear prediction functions, it is the natural choice for black box ML models.
\item The fME essentially correspond to an exploration of the prediction function. We can also use multivariate changes in feature values to explore the prediction surface in various directions simultaneously.
\item One needs to keep in mind that the shape of the prediction function may vary considerably along the forward difference, i.e., an fME with half the step size may not result in half the fME.
\end{itemize}

\end{vbframe}





\endlecture
\end{document}
