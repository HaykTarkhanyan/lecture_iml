\documentclass[11pt,compress,t,notes=noshow, aspectratio=169, xcolor=table]{beamer}

\usepackage{../../style/lmu-lecture}
% Defines macros and environments
\input{../../style/common.tex}

\title{Interpretable Machine Learning}
% \author{LMU}
%\institute{\href{https://compstat-lmu.github.io/lecture_iml/}{compstat-lmu.github.io/lecture\_iml}}
\date{}

\begin{document}

\newcommand{\titlefigure}{figure/h-statistic}
\newcommand{\learninggoals}{
\item Understand the Friedman's H-statistic
\item How to measure the overall interaction strength
\item How to measure 2-way interaction strengths
%\item Understand how to interpret ICE curves and PD plots
}

\lecturechapter{Friedman's H-Statistic}
\lecture{Interpretable Machine Learning}

\begin{frame}{Idea}

	\textbf{Idea}: If two features do not interact, we can decompose the PD function (assuming it is centered at zero) by:

	$$\fh_{jk, PD}(x_j, x_k) = \fh_{j, PD}(x_j) + \fh_{k, PD}(x_k)$$

\begin{itemize}
	\item $\fh_{jk, PD}(x_j, x_k)$ is the 2-dim PD function of both features $j$ and $k$.
	\item $\fh_{j, PD}(x_j)$ and $\fh_{k, PD}(x_k)$ are the PD functions of the single features.
\end{itemize}

\end{frame}
\begin{frame}{Idea}

	Similarly, if a feature does not interact with any other feature, we can decompose the prediction function by:

	$$\fh(x) = \fh_{j, PD}(x_j) +  \fh_{-j, PD}(x_{-j}).$$

\begin{itemize}
	\item $\fh(x)$ is the prediction function of the considered model.
	\item $\fh_{j, PD}(x_j)$ is the PD function of feature $j$.
	\item $\fh_{-j, PD}(x_{-j})$ is the PD function of all features except feature $j$.
\end{itemize}
\end{frame}

\begin{frame}{2-way Interaction Strength}
Based on this idea, the H-statistic proposed by Friedman and Popescu measures the interaction strength between feature $j$ and $k$ by:

	$$H^2_{jk} = \frac{\sum_{i=1}^n\left[\fh_{jk,PD}(x_j^{(i)}, x_k^{(i)}) - \fh_{j, PD}(x_j^{(i)}) - \fh_{k, PD}(x_k^{(i)})  \right]^2}{\sum_{i=1}^n \left[\fh_{jk,PD}(x_j^{(i)}, x_k^{(i)}) \right]^2}$$

\textbf{Note}: The numerator is $0$ if the two features $x_j$ and $x_k$ do not interact, i.e., $\fh_{jk, PD}(x_j, x_k) - \fh_{j, PD}(x_j) - \fh_{k, PD}(x_k) = 0$.

$\Rightarrow$ The smaller the values of $H^2_{jk}$, the weaker the interaction between $x_j$ and $x_k$.


\footnote[frame]{Friedman, Jerome H., and Bogdan E. Popescu (2008). Predictive learning via rule ensembles. The Annals of Applied Statistics. JSTOR, 916â€“54.}

\end{frame}


\begin{frame}{Overall Interaction Strength}

Similarly, it is possible to measure whether a feature $j$ interacts with any other feature (Overall interaction strength):


$$H^2_{j} = \frac{\sum_{i=1}^n\left[\fh(x^{(i)}) - \fh_{j, PD}(x_j^{(i)}) - \fh_{-j, PD}(x_{-j}^{(i)})  \right]^2}{\sum_{i=1}^n \left[\fh(x^{(i)}) \right]^2}$$

\textbf{Example}: Inspect interactions of a random forest for the bike data

\begin{center}
	\includegraphics[width=0.85\textwidth]{figure/h-statistic}
\end{center}
\end{frame}


\endlecture
\end{document}
