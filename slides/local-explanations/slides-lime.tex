
\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\usepackage[]{graphicx}\usepackage[]{color}
% maxwidth is the original width if it is less than linewidth
% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
	\ifdim\Gin@nat@width>\linewidth
	\linewidth
	\else
	\Gin@nat@width
	\fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%
\let\hlipl\hlkwb

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
	\def\at@end@of@kframe{}%
	\ifinner\ifhmode%
	\def\at@end@of@kframe{\end{minipage}}%
\begin{minipage}{\columnwidth}%
	\fi\fi%
	\def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
		\colorbox{shadecolor}{##1}\hskip-\fboxsep
		% There is no \\@totalrightmargin, so:
		\hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
	\MakeFramed {\advance\hsize-\width
		\@totalleftmargin\z@ \linewidth\hsize
		\@setminipage}}%
{\par\unskip\endMakeFramed%
	\at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}
\newcommand{\SweaveOpts}[1]{}  % do not interfere with LaTeX
\newcommand{\SweaveInput}[1]{} % because they are not real TeX commands
\newcommand{\Sexpr}[1]{}       % will only be parsed by R

\usepackage[english]{babel}
\usepackage[utf8]{inputenc}

\usepackage{dsfont}
\usepackage{verbatim}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{bm}
\usepackage{csquotes}
\usepackage{multirow}
\usepackage{longtable}
\usepackage{booktabs}
\usepackage{enumerate}
\usepackage[absolute,overlay]{textpos}
\usepackage{psfrag}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{eqnarray}
\usepackage{arydshln}
\usepackage{tabularx}
\usepackage{placeins}
\usepackage{tikz}
\usepackage{setspace}
\usepackage{colortbl}
\usepackage{mathtools}
\usepackage{wrapfig}
\usepackage{bm}

\usetikzlibrary{shapes,arrows,automata,positioning,calc,chains,trees, shadows}
\tikzset{
	%Define standard arrow tip
	>=stealth',
	%Define style for boxes
	punkt/.style={
		rectangle,
		rounded corners,
		draw=black, very thick,
		text width=6.5em,
		minimum height=2em,
		text centered},
	% Define arrow style
	pil/.style={
		->,
		thick,
		shorten <=2pt,
		shorten >=2pt,}
}

\usepackage{subfig}

% Defines macros and environments
\input{../../style/common.tex}

%\usetheme{lmu-lecture}
\newcommand{\titlefigure}{figure/lime5}
\newcommand{\learninggoals}{
	\item Understand motivation for LIME
	\item Develop a mathematical intuition
	\item See various applications
	\item Know its pitfalls}
\usepackage{../../style/lmu-lecture}

\let\code=\texttt
\let\proglang=\textsf

\setkeys{Gin}{width=0.9\textwidth}

\title{Local Explanations}
% \author{Bernd Bischl, Christoph Molnar, Daniel Schalk, Fabian Scheipl}
\institute{\href{https://compstat-lmu.github.io/lecture_i2ml/}{compstat-lmu.github.io/lecture\_i2ml}}
\date{}

\setbeamertemplate{frametitle}{\expandafter\uppercase\expandafter\insertframetitle}

% additional math commands
\newcommand{\Gspace}{\mathcal{G}}
\newcommand{\neigh}{\pi_{\xv}}
\newcommand{\zv}{\mathbf{z}}
\begin{document}
	
	% Set style/preamble.Rnw as parent.
	
	% Load all R packages and set up knitr
	
	% This file loads R packages, configures knitr options and sets preamble.Rnw as 
	% parent file
	% IF YOU MODIFY THIS, PLZ ALSO MODIFY setup.Rmd ACCORDINGLY...
	
	% Defines macros and environments
	
	\input{../../latex-math/basic-math.tex}
	\input{../../latex-math/basic-ml.tex}
	
	\lecturechapter{LIME}
	\lecture{Interpretable Machine Learning}
	
	% ------------------------------------------------------------------------------

\begin{vbframe}{LIME}
\begin{itemize}
		\item Local Interpretable Model-agnostic Explanations (LIME) assume that even if a machine learning model is very complex, the local prediction can be described with a simpler model.
		\item  Therefore, LIME explains \textbf{individual} predictions of \textbf{any} black-box model by approximating the model \textbf{locally} with an interpretable models.
		\item These approximations are called local surrogate models. Typically, they are linear models or trees.
		\item They should answer why a machine learning model predicted $Y$ for input $\xv$.
		\item Since they can be applied to any black-box model they are model-agnostic.  
		\item They can handle tabular, image and text data. 
\end{itemize}
\end{vbframe}

\begin{vbframe}{Formal Definition}
	LIME provides a local explanation for a black-box model $f$ in form of model $g \in \Gspace$ with $\Gspace$ as the class of potential (interpretable) models. This model $g$ should have two characteristics:
	\begin{enumerate}
		\item It should be \textbf{interpretable} i.e. provide qualitative understanding between the input variables and the response that are easy to understand.  
		\item It should be \textbf{locally faithful}, i.e. it should behave similarly to $f$ in the vicinity of the instance being predicted. This characteristic is also called local fidelity. 
	\end{enumerate}
	Formally, we want to receive a model $\tilde{g}$ with minimal complexity and maximal local-fidelity. 
\end{vbframe}

\begin{vbframe}{Model Complexity}
We can measure the complexity of a model $g$ using $\Omega(g)$. \\ 
\vspace{0.5cm}
 	\textbf{Example: Linear regression model}\\
 	Let $\Gspace = \left\{g: \Xspace \to \R ~|~g(\xv) = \thetab^\top \xv\right\}$ be the class of linear models then $\Omega(g) = \sum_{j = 1}{p}^{max}\I_{[\theta_j \neq 0]}$ could be the number of non-zero coefficients. 
 	\vspace{0.5cm}
 	
 	\textbf{Example: Tree}\\
 	Let $\Gspace = \left\{g:\Xspace \to \R ~|~g(\xv) = \sum_{m=1}^M c_m \I(x \in Q_m)\right\}$ be the class of trees (i.e., class of additive model over the leaf-rectangles) then $\Omega(g)$ could measure the number of terminal/leaf nodes.\\
 	\end{vbframe}
 
 	\begin{vbframe}{Local model fidelity}
 		\begin{itemize}
 			\item A model $g$ is locally faithful to $f$ with respect to an instance $\xv$ if for instances $\zv$ in the vicinity of $\xv$ the predictions of $g(\zv)$ are close to $f(\zv)$. 
 			 \item We can rephrase that in an optimization task: the closer $\zv$ is to $\xv$, the closer $g(\xv)$ should be to $f(\zv)$.  
 			\item For this definition we need two distance measures:
 			\begin{enumerate}
 				\item $\neigh(\zv)$ to measure how close instance $\zv$ is to $\xv$. \label{first}
 				\item $d_{g, f}(\zv)$to measure how close the predictions of $f(\zv)$ and $g(\zv)$ are. \label{second}
 			\end{enumerate}
 			%\item For the former, we can use a proximity measure $\neigh$. The nearer $\zv$ is to $\xv$ the higher should be $\neigh(\zv)$. 
 	
 			%\item For the latter, we can use the squared loss $$d_{g,f}(\zv) = (g(\zv) - f(\zv))^2$$  
 			\item Given points $\zv \in Z$, we can measure local fidelity of $g$ with respect to $f$ now in terms of a weighted loss
 			$$L(f, g, \neigh) = \sum_{z \in Z} \neigh(\zv) d_{g,f}(\zv)$$
% 			\item Example: $L(f, g, \neigh)$ could be for example a locally weighted square loss. 
% 			A common proximity measure between two points $\xv$ and $\zv$ is the exponential kernel 
% 			$$\pi_x(\zv) = exp(-D(\xv, \zv)^2/\sigma^2)$$ 
% 			with $D$ as a distance measure (e.g., L2-loss). 
% 			\item $L(f, g, \neigh)$ could be, for example, the square loss weighted by $\pi_{\xv})$.
% 			$$ 
 		\end{itemize}
%	 \vspace{0.5cm}
%	 \textbf{Example: Linear regression model} \\
%	 Let $\Gspace = \left\{g: \Xspace \to \R ~|~g(\xv) = \thetab^\top \xv\right\}$ be the class of linear models. We can use a locally weighted square loss to measure local fidelity: 
%	 $L(f, g, \neigh) = \sum_
%	An explanation $\tilde{g}(\xv)$ for the prediction of $\xv$ produced by LIME could now be obtained by  
%	$$ \tilde{g}(\xv) = \argmin_{g \in G} L(f, g, \neigh) + \Omega(g)$$
\end{vbframe}

\begin{vbframe}{Minimization task}
	\begin{itemize}
		\item Formally, an explanation produced by LIME is obtained by the following: 
		$$ \argmin_{g \in \Gspace} L(f, g, \neigh) + \Omega(g)$$
		\item In practice, LIME only optimizes $L(f, g, \neigh)$ (model-fidelity). 	
		\item The complexity is determined by users beforehand by restricting the class $\Gspace$, for example, by only considering linear models with a specific number of coefficients. 
		\item Since we want a \textbf{model-agnostic} explainer, we need to optimize $L(f, g, \neigh)$ without making any assumptions about $f$. 
		\item Therefore, we learn $g$ only approximately with the following algorithm.  
		\end{itemize}
\end{vbframe} 

\begin{vbframe}{LIME Algorithm}
		For the algorithm, we need a pre-trained model $f$, $\xv$ whose prediction we want to explain and model class $\Gspace$.\\ \vspace{0.5cm}
		We illustrate the steps of the algorithm with a classification example: 
		\begin{itemize}
			\item The light/dark gray background represents the prediction surface of classifier $f: \R^2 \to \{0, 1\}$.
			\item The yellow point displays $\xv$ we are interested in. 
			\item $\Gspace$ is restricted to the class of logistic regression models 
		\end{itemize}
		\begin{center}
			\includegraphics[width=0.4\textwidth]{figure/lime2}
		\end{center}
	
		\begin{enumerate}
		\framebreak 
		\item Sample new instances $\zv \in Z$. 
		\item Retrieve predictions $f(\zv)$ for the perturbed points $\zv \in Z$. \\[0.2cm]
		
		\hspace{-0.7cm} Strategies for sampling: 
		\begin{itemize}
			\item Randomly sample new instances. 
			\item Perturb the training dataset.
			\item Draw samples from the estimated univariate distribution of each feature.
			\item Create an equidistant grid over the supported feature range.  
		\end{itemize}
		\begin{center}
			\includegraphics[width=0.4\textwidth]{figure/lime3} \hspace{0.1cm}
			\includegraphics[width=0.4\textwidth]{figure/lime3a}
		\end{center}
		
		\framebreak
		\item Weight $\zv \in Z$ by their proximity $\neigh(\zv)$.
		\\[0.2cm]
		\end{enumerate}
		 A common proximity measure between two points $\xv$ and $\zv$ is the exponential kernel defined on some distance function $d$
		 $$\pi_x(\zv) = exp(-d(\xv, \zv)^2/\sigma^2)$$ 
		 with $\sigma$ as the kernel width. $d$ could be for example the Euclidean distance (numeric features) or the Gower distance (mixed features).
		 \textcolor{red}{Why kernel?}
		\begin{center}
			\includegraphics[width=0.4\textwidth]{figure/lime4}
		\end{center}
		
		\framebreak
		\begin{enumerate}
			\setcounter{enumi}{3}
		\item Train a weighted, interpretable model $g$ on $Z$ with the obtained predictions as the target.
		\item Return the interpretable model $g$ as the explainer. \\[0.3cm]
			\end{enumerate}
		Popular interpretable models: linear/logistic regression models, LASSO, classification/regression trees, decision rules
					\vspace{1cm}
		\begin{center}
			\includegraphics[width=0.4\textwidth]{figure/lime5}
		\end{center}
\end{vbframe}

\begin{vbframe}{Bike Sharing Dataset}
\vspace{-.3cm}

\begin{center}
\includegraphics[width=0.7\textwidth]{figure/bike-figure.png}
\end{center} 

\footnotesize \textbf{Figure:} LIME for two example instances of the bike sharing dataset.

\normalsize
\vspace{0.2cm}
The plots show the feature effect of the sparse linear model, i.e. the model coefficients times the feature value of the instance.
Warmer temperature has a positive effect on the prediction, 
while the year 2011 has a large negative effect as well as the springtime.
\end{vbframe}

\begin{vbframe}{LIME for Text Data}
	So far, we focused on tabular data but LIME could also be applied to text data: 
	\begin{itemize}
		\item Each instance is represented as a binary vector indicating the presence or absence of a word.
		\item Starting from the original text, new samples $\zv$ are created by randomly removing words from the original text.
		\item An exponential kernel with cosine distance could be used as a proximity measure. 
	\end{itemize}
	
	\begin{figure}
		\begin{center}
			%\captionsetup{font = scriptsize, labelfont = {bf, scriptsize}}
			\includegraphics[width=0.9\textwidth]{figure/lime_movier}
		\end{center}
	\end{figure}
	
	
	\scriptsize{\textbf{Figure:} LIME for two instances of a labeled movie review dataset. One half is labeled as positive reviews (\textcolor{orange}{1}), 
		the other halfs as negative reviews (\textcolor{blue}{0}). Words like ``worst`` or ``waste`` indicate a negative review while words like ``best`` or ``great`` indicate a positive review.}
	
	{\tiny{Shen, Ian, (2019, March). Explain sentiment prediction with LIME.
			\url{https://medium.com/just-another-data-scientist/explain-sentiment-prediction-with-lime-f90ae83da2da}}\par}
	
	\end{vbframe}
	
	\begin{vbframe}{LIME for image data}
	LIME also works for image data:  
	\begin{itemize}
		\item Each instance is represented as a binary vector indicating the presence or absence of superpixels. 
		\item Superpixels are interconnected pixels with similar colors. They are used because a single pixel would probably not change a prediction by much.
		\item Starting from the original image, new samples $\zv$ are created by randomly turning ``superpixels" off or on (uniformly coloring). 
		\item An exponential kernel with L2 distance could be used as a proximity measure.
	\end{itemize}
	\vspace{-0.3cm}
	\begin{center}
		\includegraphics[width=0.8\textwidth]{figure/lime-images}
	\end{center}
	\vspace{-0.3cm}
	\tiny{Ribeiro, M. T., (2016, August). Why should i trust you?: Explaining the predictions of any classifier. Retrieved from \url{https://github.com/marcotcr/lime}\par}
\end{vbframe}


\begin{vbframe}{Pitfalls}
  \begin{itemize}
  	\item LIME is one of the best known interpretable machine learning methods but several papers caution to be careful in their use. 
  	\item Problems can occur in several places. These are discussed in more detail below. 
  \end{itemize}
	\textbf{Pitfall 1: Sampling}
	\begin{itemize}
	  \item The most common sampling strategies for $\zv \in Z$ do not take the correlation between features into account. 
      \item This can lead to unlikely data points which can then be used to learn local explanation models. 
    \end{itemize}
\framebreak
	\textbf{Pitfall 2: Locality}
	\begin{itemize} 
     \item It is difficult to define locality (= how samples are weighted locally). It has a huge influence on the local model, but there is no automatic procedure for choosing the neighborhood. 
     \item Current implementations use an exponential smoothing kernel to define the neighborhood. 
     Until now there is not a good way to find a good kernel and kernel width. 
     \item A small kernel width means that an instance must be very close to influence the local model, a larger kernel width means that instances that are farther away also influence the model. 
   \end{itemize}
\end{vbframe}


\frame{
  \frametitle{Pitfalls}
  \begin{onlyenv}
  \begin{columns}
  \begin{column}{0.5\textwidth}
  \begin{itemize}
  \only<1>{
    \item In certain scenarios, you can easily turn your explanation around by changing the kernel width. 
    \item In the figure, the predictions of the black box model depending on a single feature $x$ is shown as a thick line. The distribution of the data is shown with rugs. 
  }
  \only<2>{
    \item Three local surrogate models with different kernel widths are computed. 
    \item The resulting linear regression model depends on the kernel width. 
    \item The instability of the explanations is a really big issue. 
    It implies that explanations should be taken with a grain of salt.
  }
  \end{itemize}
  \end{column}
  \begin{column}{0.5\textwidth}  
        \begin{center}
        \includegraphics[width=1\textwidth]{figure/lime-fail-1}
      \end{center}
  \end{column}
\end{columns}
\end{onlyenv}
\vspace{1cm}
\tiny{Molnar, C, (2019). Interpretable machine learning. A Guide for Making Black Box Models Explainable. \url{https://christophm.github.io/interpretable-ml-book/.}\par}
}

\begin{vbframe}{Pitfalls}

\textbf{Pitfall 3: Local vs. global features}
\begin{itemize}
	\item There exist two types of features: Features with a \textbf{global} influence that influence the global shape of the black-box model and features with a \textbf{local} influence that impact predictions for a small area of the whole features space. 
	\item Im most LIME implementations, new instances are sampled from the whole input space and are not taken around the instance of interest $\xv$.
	\item Laugel et al. (2018) showed that this tends to hide the features with a local influence for the benefit of features with a global influence in classification problems. 
	\item They propose to sample new instances $\zv$ precisely around the decision boundary closest from point $\xv$.
\end{itemize}
{\tiny Laugel et al. (2018). Defining Locality for Surrogates in Post-hoc Interpretability. \url{https://arxiv.org/pdf/1806.07498.pdf}.}
 \begin{center}
	\includegraphics[width=1\textwidth]{figure/lime_bordersample}
	{\tiny Source: \href{http://webia.lip6.fr/~laugel/files/WHI_ICML_slides.pdf}{ICML WHI 2018: Defining Locality for Surrogates in Post-hoc Interpretability}}
\end{center}
\framebreak

\textbf{Pitfall 4: Faithfulness}
\begin{itemize}
	\item There exists a trade-off between local fidelity vs. sparsity 
	\item If the local fidelity of our interpretable model is low, we do not receive reliable explanations.
	\item On the other hand, high fidelity is only possible with a more complex model bearing the risk to substitute a black-box by another.
	\item It should also be noted that identifying \textbf{locally} faithful explanations that are interpretable is less of a challenge than identifying \textbf{globally} faithful explanations. 
	\item Yet, global fidelity implies local fidelity but not vice versa. 
\end{itemize}

\textbf{Pitfall 5: Possibility to hide biases}
\begin{itemize}
	\item Slack et al. (2020) showed that data scientist could manipulate their model to hide biases. 
	\item They make use of the fact that sampled instances to build the surrogate model are out of the input data distribution if we do not account for feature dependencies. 
	\item They built a classifier that detects whether a given data point is an out-of-distribution sample or not. 
	\item If the sample is out-of-distribution (usually samples for the surrogate models), an unbiased predictor is used, otherwise a biased on. 
	\item Since the surrogate model is trained on samples whose target values are derived from an unbiased predictor, the explanations do not detect any bias although the original predictor is biased. 
\end{itemize}

\textbf{Pitfall 6: Robustness}
\begin{itemize}
	\item Another really big problem is the instability of the explanations. 
	\item Explanations of two very close points could vary greatly. 
	\item But also if $\xv$ is fixed and only new sampled data sets $Z$ are used, the resulting explanations could differ.  
\end{itemize}

\textbf{Pitfall 7: Definition of superpixels}
\begin{itemize}
	\item Another source of instability concerns the definition of superpixels for image data. 
	\item Multiple definitions of superpixels exist and these definitions influence both the shape and size. 
	\item The definition of superpixel has a large influence on the explanations. 
	\item Furthermore, if superpixel are only slightly changed (adversarial attack), the definitions could still differ greatly.  
\end{itemize}
\vspace{0.5cm}
\textbf{Conclusion: LIME should only be used with great caution.}

\end{vbframe}

\begin{vbframe}{Literature}
	\begin{itemize}
		\small
		\item Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. 2016. "Why Should I Trust You?": Explaining the Predictions of Any Classifier. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD '16). Association for Computing Machinery, New York, NY, USA, 1135–1144. 
		\item Dylan Slack, Sophie Hilgard, Emily Jia, Sameer Singh, and Himabindu Lakkaraju. 2020. Fooling LIME and SHAP: Adversarial Attacks on Post hoc Explanation Methods. In Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society (AIES '20). Association for Computing Machinery, New York, NY, USA, 180–186. 
		\item Alvarez-Melis, D., \& Jaakkola, T. (2018). On the Robustness of Interpretability Methods. ArXiv, abs/1806.08049.
		\item Laugel et al. (2018). Defining Locality for Surrogates in Post-hoc Interpretability. \url{https://arxiv.org/pdf/1806.07498.pdf}.
	\end{itemize}
	
\end{vbframe}


\endlecture
\end{document}

