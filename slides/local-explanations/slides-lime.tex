
\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\usepackage[]{graphicx}\usepackage[]{color}
% maxwidth is the original width if it is less than linewidth
% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
	\ifdim\Gin@nat@width>\linewidth
	\linewidth
	\else
	\Gin@nat@width
	\fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%
\let\hlipl\hlkwb

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
	\def\at@end@of@kframe{}%
	\ifinner\ifhmode%
	\def\at@end@of@kframe{\end{minipage}}%
\begin{minipage}{\columnwidth}%
	\fi\fi%
	\def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
		\colorbox{shadecolor}{##1}\hskip-\fboxsep
		% There is no \\@totalrightmargin, so:
		\hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
	\MakeFramed {\advance\hsize-\width
		\@totalleftmargin\z@ \linewidth\hsize
		\@setminipage}}%
{\par\unskip\endMakeFramed%
	\at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}
\newcommand{\SweaveOpts}[1]{}  % do not interfere with LaTeX
\newcommand{\SweaveInput}[1]{} % because they are not real TeX commands
\newcommand{\Sexpr}[1]{}       % will only be parsed by R

\usepackage[english]{babel}
\usepackage[utf8]{inputenc}

\usepackage{dsfont}
\usepackage{verbatim}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{bm}
\usepackage{csquotes}
\usepackage{multirow}
\usepackage{longtable}
\usepackage{booktabs}
\usepackage{enumerate}
\usepackage[absolute,overlay]{textpos}
\usepackage{psfrag}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{eqnarray}
\usepackage{arydshln}
\usepackage{tabularx}
\usepackage{placeins}
\usepackage{tikz}
\usepackage{setspace}
\usepackage{colortbl}
\usepackage{mathtools}
\usepackage{wrapfig}
\usepackage{bm}

\usetikzlibrary{shapes,arrows,automata,positioning,calc,chains,trees, shadows}
\tikzset{
	%Define standard arrow tip
	>=stealth',
	%Define style for boxes
	punkt/.style={
		rectangle,
		rounded corners,
		draw=black, very thick,
		text width=6.5em,
		minimum height=2em,
		text centered},
	% Define arrow style
	pil/.style={
		->,
		thick,
		shorten <=2pt,
		shorten >=2pt,}
}

\usepackage{subfig}

% Defines macros and environments
\input{../../style/common.tex}

%\usetheme{lmu-lecture}
% \newcommand{\titlefigure}{figure/sample-dgp-2d.pdf}
\newcommand{\learninggoals}{
	\item }
\usepackage{../../style/lmu-lecture}

\let\code=\texttt
\let\proglang=\textsf

\setkeys{Gin}{width=0.9\textwidth}

\title{Local Explanations}
% \author{Bernd Bischl, Christoph Molnar, Daniel Schalk, Fabian Scheipl}
\institute{\href{https://compstat-lmu.github.io/lecture_i2ml/}{compstat-lmu.github.io/lecture\_i2ml}}
\date{}

\setbeamertemplate{frametitle}{\expandafter\uppercase\expandafter\insertframetitle}

% additional math commands
\newcommand{\Gspace}{\mathcal{G}}
\newcommand{\neigh}{\pi_{\xv}}
\newcommand{\zv}{\mathbf{z}}
\begin{document}
	
	% Set style/preamble.Rnw as parent.
	
	% Load all R packages and set up knitr
	
	% This file loads R packages, configures knitr options and sets preamble.Rnw as 
	% parent file
	% IF YOU MODIFY THIS, PLZ ALSO MODIFY setup.Rmd ACCORDINGLY...
	
	% Defines macros and environments
	
	\input{../../latex-math/basic-math.tex}
	\input{../../latex-math/basic-ml.tex}
	
	\lecturechapter{LIME}
	\lecture{Interpretable Machine Learning}
	
	% ------------------------------------------------------------------------------

\begin{vbframe}{LIME}
\begin{itemize}
		\item  Local Interpretable Model-agnostic Explanations (LIME) explain \textbf{individual} predictions of \textbf{any} black-box model by approximating the model \textbf{locally} with an interpretable models.
		\item These approximations are called local surrogate models. Typically, they are linear models or trees.
		\item They should answer why a machine learning model predicted $Y$ for input $\xv$.
		\item Since they can be applied to any black-box model they are model-agnostic.  
		\item They can handle tabular, image and text data. 
\end{itemize}
\end{vbframe}

\begin{vbframe}{Formal Definition}
	LIME provides a local explanation for a black-box model $f$ in form of model $g \in \Gspace$ with $\Gspace$ as the class of potential (interpretable) models. This model $g$ should have two characteristics:
	\begin{enumerate}
		\item It should be \textbf{interpretable} i.e. provide qualitative understanding between the input variables and the response that are easy to understand.  
		\item It should be \textbf{locally faithful}, i.e. it should behave similarly to $f$ in the vicinity of the instance being predicted. This characteristic is also called local fidelity. 
	\end{enumerate}
	Formally, we want to receive a model $\tilde{g}$ with minimal complexity and maximal local-fidelity. 
\end{vbframe}

\begin{vbframe}{Model Complexity}
We can measure the complexity of a model $g$ using $\Omega(g)$. \\ 
\vspace{0.5cm}
 	\textbf{Example: Linear regression model}\\
 	Let $\Gspace = \left\{g: \Xspace \to \R ~|~g(\xv) = \thetab^\top \xv\right\}$ be the class of linear models then $\Omega(g) = \sum_{j = 1}{p}^{max}\I_{[\theta_j \neq 0]}$ could be the number of non-zero coefficients. 
 	\vspace{0.5cm}
 	
 	\textbf{Example: Tree}\\
 	Let $\Gspace = \left\{g:\Xspace \to \R ~|~g(\xv) = \sum_{m=1}^M c_m \I(x \in Q_m)\right\}$ be the class of trees (i.e., class of additive model over the leaf-rectangles) then $\Omega(g)$ could measure the number of terminal/leaf nodes.\\
 	\end{vbframe}
 
 	\begin{vbframe}{Local model fidelity}
 		\begin{itemize}
 			\item A model $g$ is locally faithful to $f$ with respect to an instance $\xv$ if for instances $\zv$ in the vicinity of $\xv$ the predictions of $g(\zv)$ are close to $f(\zv)$. 
 			 \item We can rephrase that in an optimization task: the closer $\zv$ is to $\xv$, the closer $g(\xv)$ should be to $f(\zv)$.  
 			\item For this definition we need two distance measures:
 			\begin{enumerate}
 				\item $d(\zv, \xv)$ to measure how close instance $\zv$ is to $\xv$. \label{first}
 				\item $d_{g, f}(\zv)$to measure how close the predictions of $f(\zv)$ and $g(\zv)$ are. \label{second}
 			\end{enumerate}
 			\item For the former, we can use a proximity measure $\neigh$. The nearer $\zv$ is to $\xv$ the higher should be $\neigh(\zv)$. A common proximity measure between two points $\xv$ and $\zv$ is the exponential kernel defined on some distance function $D$
 			$$\pi_x(\zv) = exp(-d(\xv, \zv)^2/\sigma^2)$$ 
 			with $\sigma$ as the kernel width.  
 			\item $d$ could be for example the L2-distance.
 			\item For the latter, we can use the squared loss $$d_{g,f}(\zv) = (g(\zv) - f(\zv))^2$$  
 			\item Given some points $\zv \in Z$ we can measure local fidelity now in terms of a weighted loss
 			$$L(f, g, \neigh) = \sum_{z \in Z} \neigh(\zv) d_{g,f}(\zv)$$
% 			\item Example: $L(f, g, \neigh)$ could be for example a locally weighted square loss. 
% 			A common proximity measure between two points $\xv$ and $\zv$ is the exponential kernel 
% 			$$\pi_x(\zv) = exp(-D(\xv, \zv)^2/\sigma^2)$$ 
% 			with $D$ as a distance measure (e.g., L2-loss). 
% 			\item $L(f, g, \neigh)$ could be, for example, the square loss weighted by $\pi_{\xv})$.
% 			$$ 
 		\end{itemize}
%	 \vspace{0.5cm}
%	 \textbf{Example: Linear regression model} \\
%	 Let $\Gspace = \left\{g: \Xspace \to \R ~|~g(\xv) = \thetab^\top \xv\right\}$ be the class of linear models. We can use a locally weighted square loss to measure local fidelity: 
%	 $L(f, g, \neigh) = \sum_
%	An explanation $\tilde{g}(\xv)$ for the prediction of $\xv$ produced by LIME could now be obtained by  
%	$$ \tilde{g}(\xv) = \argmin_{g \in G} L(f, g, \neigh) + \Omega(g)$$
\end{vbframe}

\begin{vbframe}{Minimization task}
	\begin{itemize}
		\item Formally, an explanation produced by LIME is obtained by the following: 
		$$ \argmin_{g \in \Gspace} L(f, g, \neigh) + \Omega(g)$$
		\item In practice, LIME only optimizes $L(f, g, \neigh)$ (model-fidelity). 	
		\item The complexity is determined by users beforehand by restricting the class $\Gspace$, for example, by only considering linear models with a specific number of coefficients. 
		\framebreak 
		\item Since we want a \textbf{model-agnostic} explainer, we need to optimize $L(f, g, \neigh)$ without making any assumptions about $f$. 
		\item 	Given $\xv$ whose prediction we want to explain and model class $\Gspace$, we learn $g$ by the following: 
			\vspace{1cm}
		\begin{center}
			\includegraphics[width=0.5\textwidth]{figure/lime2}
		\end{center}
	\end{itemize}
	
		\begin{enumerate}
		\framebreak 
		\item Perturb your dataset and get black box predictions $\zv \in Z$ for these new points.
		\vspace{1cm}
		\begin{center}
			\includegraphics[width=0.5\textwidth]{figure/lime3}
		\end{center}
		
		\framebreak
		\item Weight the new samples by their proximity $\neigh(\zv)$.
			\vspace{1cm}
		\begin{center}
			\includegraphics[width=0.5\textwidth]{figure/lime4}
		\end{center}
		
		\framebreak
		\item Train a weighted, sparse, interpretable model on this new dataset (e.g. LASSO) with the obtained predictions as the target.
		
					\vspace{1cm}
		\begin{center}
			\includegraphics[width=0.5\textwidth]{figure/lime5}
		\end{center}
		
		\framebreak
		\item Explain the prediction by interpreting the local model. 
		\end{enumerate}
\framebreak


\begin{center}
 \includegraphics[width=0.5\textwidth]{figure/lime}
\end{center}
\begin{itemize}
  \item The blue/pink background represents the complex decision function $\fh$.
  \item The red bold cross is our instance of interest $\xv$.
  \item Other crosses and points represent the sampled instances. The size of the symbols corrresponds to the proximity to $\xv$.
  \item The dashed line corresponds to the learned model used as a local explanation.
\end{itemize}
{\tiny{Ribeiro, M. T., (2016, August). Why should i trust you?: Explaining the predictions of any classifier}}
\end{vbframe}

\begin{vbframe}{Bike Sharing Dataset}
\vspace{-.3cm}

\begin{center}
\includegraphics[width=0.7\textwidth]{figure/bike-figure.png}
\end{center} 

\footnotesize \textbf{Figure:} LIME for two example instances of the bike sharing dataset.

\normalsize
\vspace{0.2cm}
The plots show the feature effect of the sparse linear model, i.e. the model coefficients times the feature value of the instance.
Warmer temperature has a positive effect on the prediction, 
while the year 2011 has a large negative effect as well as the springtime.
\end{vbframe}

\begin{vbframe}{Remarks}
  \begin{itemize}
      \item LIME creates new samples by perturbing each feature individually. For numerical features, samples are drawn from a normal distribution with mean and standard deviation based on the observed feature values.   
      \item LIME samples are not taken around the instance of interest $\xv$, but from the training data's mass center.
      \item Any interpretable model (e.g. LASSO, decision tree) can be used as a local model. 
      \item LIME assumes that even if a machine learning model is very complex, the local prediction can be described with a simpler model.
     \item It is difficult to define locality (= how samples are weighted locally). It has a huge influence on the local model, but there is no automatic procedure for choosing the neighbourhood. 
     \item Current implementations use an exponential smoothing kernel to define the neighborhood. 
     Until now there is not a good way to find a good kernel and kernel width. 
     \item A small kernel width means that an instance must be very close to influence the local model, a larger kernel width means that instances that are farther away also influence the model. 
   \end{itemize}
\end{vbframe}


\frame{
  \frametitle{Remarks}
  \begin{onlyenv}
  \begin{columns}
  \begin{column}{0.5\textwidth}
  \begin{itemize}
  \only<1>{
    \item In certain scenarios, you can easily turn your explanation around by changing the kernel width. 
    \item In the figure, the predictions of the black box model depending on a single feature $x$ is shown as a thick line. The distribution of the data is shown with rugs. 
  }
  \only<2>{
    \item Three local surrogate models with different kernel widths are computed. 
    \item The resulting linear regression model depends on the kernel width. 
    \item The instability of the explanations is a really big issue. 
    It implies that explanations should be taken with a grain of salt.
  }
  \end{itemize}
  \end{column}
  \begin{column}{0.5\textwidth}  
        \begin{center}
        \includegraphics[width=1\textwidth]{figure/lime-fail-1}
      \end{center}
  \end{column}
\end{columns}
\end{onlyenv}
\vspace{1cm}
\tiny{Molnar, C, (2019). Interpretable machine learning. A Guide for Making Black Box Models Explainable. \url{https://christophm.github.io/interpretable-ml-book/.}\par}
}

\begin{vbframe}{LIME for Images and Text}
LIME for images and texts differs from LIME for tabular data. \\[0.2cm]
Text classification: 
\begin{itemize}
    \item Each instance is represented as a binary vector indicating the presence or absence of a word.
  \item Starting from the original text, new texts are created by randomly removing words from the original text. 
\end{itemize}

\begin{figure}
\begin{center}
%\captionsetup{font = scriptsize, labelfont = {bf, scriptsize}}
 \includegraphics[width=0.9\textwidth]{figure/lime_movier}
\end{center}
\end{figure}


 \scriptsize{\textbf{Figure:} LIME for two instances of a labeled movie review dataset. One half is labeled as positive reviews (\textcolor{orange}{1}), 
the other halfs as negative reviews (\textcolor{blue}{0}).Words like ``worst`` or ``waste`` indicate a negative review while words like ``best`` or ``great`` indicate a positive review.}

\vspace{0.3cm}
{\tiny{Shen, Ian, (2019, March). Explain sentiment prediction with LIME.
\url{https://medium.com/just-another-data-scientist/explain-sentiment-prediction-with-lime-f90ae83da2da}}\par}

\framebreak

\normalsize
LIME for images and texts differs from LIME for tabular data. \\[0.2cm]
Image classification: 
  \begin{itemize}
  \item Variations of the images are created by segmenting the image into $"$superpixels$"$ and turning superpixels off or on. 
  \item Superpixels are interconnected pixels with similar colors. They are used because a single pixel would probably not change a prediction by much.
  \item A binary vector indicates the presence or absence of these superpixels.
\end{itemize}
\vspace{-0.3cm}
\begin{center}
 \includegraphics[width=0.8\textwidth]{figure/lime-images}
\end{center}
\vspace{-0.3cm}
\tiny{Ribeiro, M. T., (2016, August). Why should i trust you?: Explaining the predictions of any classifier. Retrieved from \url{https://github.com/marcotcr/lime}\par}
\end{vbframe}


\begin{vbframe}{Example}
	\begin{itemize}
		\item We trained a model to predict if an image shows a wolf or a husky. 
		\item Below the predictions on six test images are given. 
		\item Do you trust our predictor? 
	\end{itemize}
	\begin{center}
		\includegraphics[width=0.45\textwidth]{figure/lime-wolfhusky.png}\\
		\includegraphics[width=0.45\textwidth]{figure/lime-wolfhusky2.png}\\
		{\tiny \textbf{Source:} \href{http://www.facweb.iitkgp.ac.in/~niloy/COURSE/Spring2018/IntelligentSystem/PPT_2018/why_should_i_trust_ppt.pdf}{www.facweb.iitkgp.ac.in}}
	\end{center}
\end{vbframe}

\begin{vbframe}{Example}
	\begin{itemize}
		\item We can use local interpretable model-agnostic explanations (LIME) to highlight the parts of an image which led to the prediction.
		\item We can see that our predictor is actually a snow detector. 
	\end{itemize}
	\begin{center}
		\includegraphics[width=0.75\textwidth]{figure/lime-wolfhusky3.png}\\
		{\tiny \textbf{Source:} \href{http://www.facweb.iitkgp.ac.in/~niloy/COURSE/Spring2018/IntelligentSystem/PPT_2018/why_should_i_trust_ppt.pdf}{www.facweb.iitkgp.ac.in}}
	\end{center}
\end{vbframe}

\endlecture
\end{document}

