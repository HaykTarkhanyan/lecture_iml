\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\usepackage[]{graphicx}\usepackage[]{color}
% maxwidth is the original width if it is less than linewidth
% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%
\let\hlipl\hlkwb

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}
\newcommand{\SweaveOpts}[1]{}  % do not interfere with LaTeX
\newcommand{\SweaveInput}[1]{} % because they are not real TeX commands
\newcommand{\Sexpr}[1]{}       % will only be parsed by R

\usepackage[english]{babel}
\usepackage[utf8]{inputenc}

\usepackage{dsfont}
\usepackage{verbatim}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{bm}
\usepackage{csquotes}
\usepackage{multirow}
\usepackage{longtable}
\usepackage{booktabs}
\usepackage{enumerate}
\usepackage[absolute,overlay]{textpos}
\usepackage{psfrag}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{eqnarray}
\usepackage{arydshln}
\usepackage{tabularx}
\usepackage{placeins}
\usepackage{tikz}
\usepackage{setspace}
\usepackage{colortbl}
\usepackage{mathtools}
\usepackage{wrapfig}
\usepackage{bm}

\usetikzlibrary{shapes,arrows,automata,positioning,calc,chains,trees, shadows}
\tikzset{
  %Define standard arrow tip
  >=stealth',
  %Define style for boxes
  punkt/.style={
    rectangle,
    rounded corners,
    draw=black, very thick,
    text width=6.5em,
    minimum height=2em,
    text centered},
  % Define arrow style
  pil/.style={
    ->,
    thick,
    shorten <=2pt,
    shorten >=2pt,}
}

\usepackage{subfig}

% Defines macros and environments
\input{../../style/common.tex}

%\usetheme{lmu-lecture}
 \newcommand{\titlefigure}{figure/counterfactuals_heat.png}
\newcommand{\learninggoals}{
\item Understand the motivation behind CEs
\item Know how to generate CEs
\item Know problems and limitations of CEs}
\usepackage{../../style/lmu-lecture}

\let\code=\texttt
\let\proglang=\textsf

\setkeys{Gin}{width=0.9\textwidth}

\title{Local Explanations}
% \author{Bernd Bischl, Christoph Molnar, Daniel Schalk, Fabian Scheipl}
\institute{\href{https://compstat-lmu.github.io/lecture_i2ml/}{compstat-lmu.github.io/lecture\_i2ml}}
\date{}

\setbeamertemplate{frametitle}{\expandafter\uppercase\expandafter\insertframetitle}

\begin{document}

	
% Set style/preamble.Rnw as parent.

% Load all R packages and set up knitr

% This file loads R packages, configures knitr options and sets preamble.Rnw as 
% parent file
% IF YOU MODIFY THIS, PLZ ALSO MODIFY setup.Rmd ACCORDINGLY...

% Defines macros and environments

\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}

\lecturechapter{Counterfactual Explanations}
\lecture{Interpretable Machine Learning}

% ------------------------------------------------------------------------------

\begin{vbframe}{Counterfactual Explanations}
	\begin{itemize}
	    \item \textbf{Counterfactual Explanations} (CEs) explain particular decisions of an ML model by presenting an alternative input that is predicted to a desired outcome.
		\item These alternatives are called \textbf{counterfactuals} and represent close neighbors of a data point we are interested in, but belonging to the \textbf{desired outcome class}. 
		\item CEs reveal what minimal changes to the input are sufficient to receive a different response.
		\item Due to their natural interpretaion, the targeted audience of CEs are often end-users not ML engineers.
%		\item While we focus here exclusively on their application to tabular data, they can also be applied to image and text data. 
	\end{itemize}
\end{vbframe}

\begin{vbframe}{Example: Credit Risk Application} 
	\begin{itemize}
		\item $\textbf{x}$: customer and credit information
		\item $y$: Grant or reject credit
	\end{itemize}
	\begin{center}\includegraphics[width=0.65\linewidth, page=1]{figure/counterfactuals_credit.pdf} \end{center}
	
	Questions: 
	\begin{itemize}
		\item Why was the credit rejected? 
		\item Is it a fair decision? 
		\item How should $\xv$ be changed so that the credit is accepted?  
	\end{itemize}
	
	\framebreak
	Counterfactual Explanations provide answers in the form of "What-If"-scenarios. 
	\begin{center}\includegraphics[width=0.65\linewidth, page=2]{figure/counterfactuals_credit.pdf} \end{center}
	
	``If the person was more skilled and the credit amount had been reduced to \$8000, his credit would have been granted."  \\[0.2cm]
	
%	 \begin{center}\includegraphics[width=1\linewidth, page=3]{figure/counterfactuals_credit.pdf} \end{center}
%	Input: Desired target, original data point, predictor, observed data\\[0.1cm]
%	Output: Data points which minimize  
%	\begin{enumerate}
%		\item Distance of prediction to desired prediction $|\fh(x) - y'|$
%		\item Number of feature changes $\sum_{j = 1}^p I_{x_j \neq x^*_j}$
%		\item Distance to original data point $d(x, x^*)$ (Gower)
%		\item Distance to observed data points $d(x, X^{obs})$
%	\end{enumerate}
	
\end{vbframe}



\begin{vbframe}{Aims \& Roles}
	CE can serve various purposes, the user can decide what to learn from them. Consider the following example:\newline
	``If the person was one year older and the credit amount were increased to \$12.000, her credit would have been granted."  \\[0.2cm]
	\begin{itemize}
		\itemsep1.3em
		\item \textbf{Guidance for future actions:} \textit{Ok, I'll apply again next year for the higher amount.}
		\item \textbf{Provide Reasons:} \textit{Interesting, I did not know that age plays a role in loan applications.}
		\item \textbf{Provide grounds to contest the decision:} \textit{How dare you, I do not want to be discriminated for my age in an application.}
		\item \textbf{Detect Model Biases:} \textit{Oh, there is a bug, an increase in amount should not increase approval rates.}
	\end{itemize}
\end{vbframe}

\begin{vbframe}{Philosophical Basis: CEs \& Lewis}
Interestingly, counterfactuals have a long-standing tradition in philosophy and, in fact the IML discussion of CEs is based on the work of Lewis (1973). 

A \textbf{Counterfactual Conditional} is a statement of the form:	
\begin{equation}
		\textnormal{``If $S$ was the case, $Q$ would have been the case."}
		\label{eq:sent}
\end{equation}
	\begin{itemize}
		\item $S$ must relate to an event in the past that did in fact not occur.
		\item In that sense counterfactuals run \textbf{contrary} to the \textbf{facts}.
		\item How can Equation~(\ref{eq:sent}) be true if $S$ is actually false? According to Lewis's (1973), Equation~(\ref{eq:sent}) is true, if in all worlds most similar to the actual world where $S$ is the case, $Q$ is the case. 
		\item A world is similar to another if the laws are maximally preserved and only a few facts from the actual world are changed.
		\item Lewis's proposal is hotly debated in philosophy, particularly his notion of similarity between worlds remains controversial. 
	\end{itemize}
\end{vbframe}

\begin{vbframe}{Philosophical Basis: CEs and Causality}
Counterfactuals have mainly been studied to give an account of causal dependence. According to Lewis:\newline
``$Q$ causally depends on $S$ iff, if $S$ were not the case $Q$ would not have been the case.''
	\begin{itemize}
		\item Causal dependence underlies the explanatory power CEs offer. Good CEs point to the critical causal factors that made the algorithm decide against $Q$ and thereby reveal information about the underlying mechanism.
		\item This is also the reason why counterfactuals must be maximally close to initial inputs, otherwise changes are only sufficient but some of them might be unnecessary.
		\item Current research on causality is based on Pearl's (2009) causal graphs. Instead of defining causality in terms of counterfactuals, Pearl's approach turns the story around and necessitates a representation of causal mechanisms to define CEs. 
	\end{itemize}
\footnote[frame]{Lewis, David (1973). Counterfactuals. Cambridge, MA: Harvard University Press. ISBN 9780631224952. }
\end{vbframe}

\begin{vbframe}{Philosophical Basis: CEs as Contrastive}
CEs are or can be easily translated into contrastive explanations and often vice versa. Contrastive explanations are answers to a question of the form ``why did Q' occur instead of Q?''.
	\begin{itemize}
	    \item  Contrastive Explanations answer these questions by \textbf{contrasting} the actual scenario with a different scenario $S$ in which $Q$ had occurred.
		\item According to psychologists [Miller (2019)], Contrastive Explanations are the gold standard of explanations in human-to-human interaction.
		\item A CE becomes contrastive if its antecedent $S$ is presented in contrast to the actual scenario.
	\end{itemize}
\end{vbframe}

\begin{vbframe}{Mathematical Perspective}
	Terminology: 
	\begin{itemize}
		\item $\xv$ as original/factual datapoint whose prediction we want to explain
		\item $y' \in \Yspace$ as desired outcome 
	\end{itemize}
	\vspace{0.3cm}
	A \textbf{valid} counterfactual $\xv'$ is a datapoint: 
	\begin{enumerate}
		\item whose prediction $\fh(\xv')$ is equal to the desired prediction $y'$. 
		\item that is maximally close to the original datapoint $\xv$.
	\end{enumerate}
	Wachter et al. (2017) reformulated these requirements into an optimization problem: 
	\begin{equation}
		\argmin_{\xv'} \max_{\lambda} \lambda o_1(\fh(\xv'), y') + o_2(\xv', \xv).
		\label{eq:wachter}
	\end{equation}
	\begin{itemize}
		\item $\lambda$ balances the two objectives.
		\item $o_1$ and $o_2$ are distances, their choice is critical.
		\framebreak
		\item $o_1$ could be, for example, L1-distance $o_1(\fh(\xv'), y') = |\fh(\xv')-y'|$
		\item $o_2$ could be the Gower distance that is suitable for mixed features: 
		$$o_2(\xv', \xv) = \frac{1}{p}\sum_{j = 1}^{p} \delta_G(\xv'_j, \xv_j)\in [0, 1]$$
		The value of $\delta_G$ depends on the feature type:
		\begin{equation*}
		\delta_G(x_j, x^*_j) = 
		\begin{cases}
		\frac{1}{\widehat{R}_j}|x_j- x^*_j| & \text{if $x_j$ is numerical} \\
		\mathbb{I}_{x_j \neq x_j^*} & \text{if $x_j$ is categorical}
		\end{cases}
		\end{equation*}
		with $\widehat{R}_j$ as the value range of feature $j$, which is extracted from the training dataset. 
	\end{itemize}

\footnote[frame]{Wachter et al. (2017). Counterfactual Explanations Without Opening the Black Box: Automated Decisions and the GDPR. Harvard Journal of Law \& Technology, 31 (2), 2018. \url{http://dx.doi.org/10.2139/ssrn.3063289}.}
\end{vbframe}

\begin{vbframe}{Further Objectives}
	While validity is a necessary condition for counterfactuals, additional constraints can improve the explanation quality of the corresponding CEs. Popular constraints include sparsity and plausibility.
	
	\textbf{Sparsity:}
	\begin{itemize}
		\item End-users often prefer short over long explanations. For CEs this means that the changes made to obtain counterfactuals must be \textbf{sparse}. 
		\item The distance $o_2$ can but does not necessarily take the number of changed features into account. Prominent examples that do so are the L0- and the L1-norm.
%		\item There could be a trade-off between the number of features changed and the total amount of change made to obtain a certain prediction. 
        \item We can also account for sparsity by adding an extra term to our objective that counts the number of changed features via the L0-norm $$o_3(\xv', \xv) = \sum_{j = 1}^p \mathbb{I}_{x'_j \neq x_j}.$$ 
	\end{itemize}
	\framebreak
	\textbf{Plausibility:}
	\begin{itemize}
	    \item CEs should suggest alternatives that are plausible. For example, it is a bad idea to suggest a loan applicant to raise her income and get unemployed at the same time. 
	    \item Thus, it is desirable to generate counterfactuals that are realistic in the sense that they originate from the distribution of $\Xspace$ or adhere to the data manifold. 
		\item Estimating a joint distribution over the training data is a complex problem, especially for mixed feature spaces. As a proxy, we could desire that $\xv'$ is close to the training data $\Xmat$. For example, $o_4$ could then be the Gower distance of $\xv'$ to the nearest data point of the training dataset $\xv^{[1]}$
		$$o_4(\xv, \Xmat) =  \frac{1}{p} \sum_{j = 1}^{p}  \delta_G(x_j, x^{[1]}_j).$$
	\end{itemize}	
	Overall, we could update Eq.~(\ref{eq:wachter}) to be: 
	\begin{equation}
		\argmin_{\xv'} \max_{\lambda_1} \lambda_1 o_1(\fh(\xv'), y') + \lambda_2 o_2(\xv', \xv) + \lambda_3 o_3(\xv', \xv) + \lambda_4 o_4(\xv', \Xmat).
	\end{equation}
	
	\begin{center}
		\includegraphics[width=0.5\textwidth]{figure/counterfactuals_obj}
	\end{center}

\scriptsize{\textbf{Figure:} Two possible paths for a datapoint \textcolor{blue}{$\xv$},
	originally classified in the negative class. The two counterfactuals (\textcolor{red}{CF1} and \textcolor{green}{CF2}) are valid. Note that the red path $A$ for CF1 is the shortest, whereas the
	green path $B$ for CF2 adheres closely to the manifold of the training data, but is longer.}
\vspace{0.3cm}


\footnote[frame]{Verma et al. (2020). \href{https://arxiv.org/pdf/2010.10596.pdf}{Counterfactual Explanations for Machine Learning: A Review.}}

\end{vbframe}

\begin{vbframe}{Remarks: The Rashomon Effect}
The solution to the optimization problem might not be unique, there can be many equally close inputs that obtain the desired classification. Correspondingly, there can be many different equally good explanations for the same decision. This is called the \textbf{Rashomon Effect}.
	\begin{itemize}
		\item We could present all CEs for a given case, however, time is limited and so is the human processing capacity.
		\item Another solution is to focus on one or few CEs, however, by which criterion should they be selected?
		\item Worse, as the model is generally non-linear, incompatible CEs can arise e.g. suggesting either an increase or decrease in credit duration, which confuses the explainee.
		\item How to deal with the Rashomon Effect is considered an open problem in IML.
	\end{itemize}
\end{vbframe}

\begin{vbframe}{Remarks: Model or Real-World}
Most CEs provide explanations of model-predictions. However, to end-users, these explanations appear to explain the process in which the model is employed. Unfortunately, the transfer from the model to the real-world is generally not permitted.
	\begin{itemize}
	\item Consider, e.g., a CE that proposes to increase the feature age by 5 to obtain the loan. A loan applicant takes this information and applies 5 years later for the loan. 
	\item However, by then, many of her properties have changed not only the age since they are causally dependent on age like income or the job status.
	%\item Also, the algorithm can change and CEs are not applicable anymore.
	%\item Even worse, the user might have typos in the application and in fact no changes are necessary to obtain the loan.
	\item Karimi et al. (2020) avoid this shortcoming by considering causal dependencies between variables.
%		\item Actionability: We could further strengthen above's plausibility criterion by requiring counterfactuals that do not change immutable features (e.g., race, city of birth, sex). Therefore, we could  search for counterfactuals only among a defined feasible set of counterfactuals $\mathcal{A}$. 
%		\item Causality: We could also restrict the search to counterfactuals that maintain any known causal relations. In the real world, if one feature is changed it affects also other features. E.g., better skills lead to better salary, but also a higher age due to the necessary training. 
	\end{itemize}
\footnote[frame]{Karimi et al. (2021). Algorithmic Recourse: From Counterfactual Explanations to Interventions.  Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency. 353–362.}
\end{vbframe}

\begin{vbframe}{Overview of Methods}
	Currently, multiple methods exist to calculate counterfactuals. They differ greatly in: 
	\begin{itemize}
		\item \textbf{Targets:} Most methods focus on classification models, only few cover regression models. So far, all remain in the supervised learning paradigm.
		\item \textbf{Data Types:} Mainly for tabular data, few methods focus on visual/text data, none on audio data.
		\item \textbf{Feature space:} Some methods can only handle numerical features, few can process discrete and continuous feature spaces. 
		\item \textbf{Objectives:} Many methods focus on action guidance, plausibility and sparsity, few on other objectives like fairness or individual preferences.
		\framebreak
		\item \textbf{Model access:} Methods could require access to complete model internals, access to gradients or only to prediction functions. Therefore, both model-agnostic and model-specific methods exist.
		\item \textbf{Optimization tool:} Gradient-based algorithms (only for differential models), mixed-integer programming (only linear) or gradient-free algorithms e.g. Nelder-Mead, genetic algorithm. 
		\item \textbf{Rashomon Effect:} Many methods return a single counterfactual per run, some multiple counterfactuals, others prioritize CEs or let the user choose.
	\end{itemize}
\end{vbframe}

\begin{vbframe}{Method: Multi-Objective Counterfactual Explanations}
	\begin{itemize}
		\item Minimizing Eq.~(\ref{eq:wachter}) is difficult since $\lambda_1$, $\lambda_2$, $\lambda_3$ and $\lambda_4$ need to be specified a-priori. 
		\item Instead of collapsing all four objectives into a single objective by a weighted sum, we could optimize them simultaneously with a multi-objective genetic algorithm. 
		\item This approach is called Multi-Objective Counterfactual Explanations (MOC) and was developed by Dandl et al. in 2020. 
		\item They adjusted the non-dominated sorting genetic algorithm NSGA-II (see CIM1 for details) to produce a set of diverse counterfactuals for mixed discrete and continuous feature spaces.
		\item The algorithm returns a set of counterfactuals that represents different trade-offs between the objectives and are constructed to be diverse in feature space.
	\end{itemize}

	\footnote[frame]{Dandl S., Molnar C., Binder M., Bischl B. (2020) Multi-Objective Counterfactual Explanations. In: Bäck T. et al. (eds) Parallel Problem Solving from Nature – PPSN XVI. PPSN 2020. Lecture Notes in Computer Science, vol 12269. Springer, Cham.}
	
	 
\end{vbframe}

\begin{vbframe}{Example: Credit Data}
	\begin{itemize}
		\item Model: SVM with RBF kernel
		\item $\xv$ is the first data point of the dataset with $\P(y = good)  = 0.34$ of being a ``good" customer.  
		\item Our desired goal is to increase the probability to $[0.5, 1]$.
		\item MOC (with default parameters) found 82 counterfactuals after 200 iterations that met the target.
		\item All counterfactuals proposed changes to the credit duration and many of them to the credit amount.  
		\item We can visualize feature changes with a parallel plot and 2-dim surface plot. 
		\item The parallel plot reveals that all counterfactuals had values equal to or smaller than the values of $\xv$.
		\item The surface plot illustrates why these feature changes are recommended. 
		\item Counterfactuals in the lower left corner seem to be in a less favorable region far from $\xv$, but they are in high density areas close to training samples (indicated by histograms).
	\end{itemize}
	\vspace{-0.7cm}
	\begin{columns}
				\begin{column}{0.5\textwidth}  
			\begin{center}
				\includegraphics[width=0.75\textwidth]{figure/counterfactuals_credit_parallel}
			\end{center}
		\vspace{-0.2cm}
			\scriptsize{\textbf{Figure:} Parallel plot. 
				The grey lines show the feature values of the counterfactuals $\xv'$, the blue line corresponds to the values of $\xv$. Features without proposed changes are omitted. The bold numbers give minima and maxima of numeric features.} 
			
		\end{column}
		\begin{column}{0.5\textwidth}
			\begin{center}
				\includegraphics[width=1\textwidth]{figure/counterfactuals_credit_heat}
			\end{center}
		\vspace{-0.2cm}
			\scriptsize{\textbf{Figure:} Response surface plot. 
				The white dot is $\xv$, black dots are $\xv'$. The histograms display the marginal distribution of the training data $\Xmat$.} 
				
		\end{column}
	\end{columns}
\footnote[frame]{Dandl S., Molnar C., Binder M., Bischl B. (2020) Multi-Objective Counterfactual Explanations. In: Bäck T. et al. (eds) Parallel Problem Solving from Nature – PPSN XVI. PPSN 2020. Lecture Notes in Computer Science, vol 12269. Springer, Cham.}
\end{vbframe}


%\begin{vbframe}{Example: Bike Sharing Dataset}
%	\begin{itemize}
%		\item Model: Random Forest with 500 trees
%		\item $\xv$ is the first data point of the dataset with $\fh(\xv) = 1767.93$ rental bikes. 
%		\item Our desired goal is to increase the count of total rental bikes to $y' = [3000, \infty[$
%		\item MOC (with default parameters) found 56 counterfactuals after 200 iterations that met the target.
%		\item Most counterfactuals proposed to decrease the humidity (94.6 \%) and more than half to increase the temperature (55.4\%). 
%		\item Some counterfactuals proposed additional changes to the year (2012 instead of 2011) and month (December instead of Januar).
%		\framebreak 
%		\item We can visualize feature changes with a parallel plot. 
%		\item For humidity and temperature, we can additionally show a 2-dim surface plot. 
%	\end{itemize}
%	\vspace{-0.5cm}
%	\begin{columns}
%		\begin{column}{0.5\textwidth}
%			\begin{center}
%				\includegraphics[width=1\textwidth]{figure/counterfactuals_bike_sp}
%			\end{center}
%		
%			\scriptsize{\textbf{Figure:} Response surface plot. 
%				The white dot is $\xv$, black dots are $\xv'$. The histograms display the marginal distribution of the training data $\Xmat$.} 
%				
%		\end{column}
%		\begin{column}{0.5\textwidth}  
%			\begin{center}
%				\includegraphics[width=1\textwidth]{figure/counterfactuals_bike_para}
%			\end{center}
%		
%		\scriptsize{\textbf{Figure:} Parallel plot. 
%			The grey lines show the feature values of the counterfactuals $\xv'$, the blue line corresponds to the values of $\xv$. Features without proposed changes are omitted. The bold numbers give minima and minima of numeric features while character strings indicate categories of character features.} 
%		
%		\end{column}
%	\end{columns}
%\end{vbframe}

\begin{vbframe}{Problems, Pitfalls, \& Limitations}
\begin{itemize}
    \item \textbf{Illusion of model understanding:} 
    CEs explain ML decisions by pointing to few specific alternatives. This reduces complexity, but is limited in explanatory power. Psychologists showed that even though the perceived model-understanding of end-users increases, the objective model-understanding remains unchanged.
    
    \item \textbf{Finding the right metric:} Similarity is the crucial concept for finding good CEs. However, our concept of similarity is context and domain dependent. E.g. while L1 can be a reasonable notion for tabular data, it is counterintuitive for image data. Sparsity is often desirable for end-users but not for data scientists searching for biases in the model.
    
    \item \textbf{Confusing Model and Real-World:} As pointed out before, explanations of the model do not easily transfer to the process in which a model is applied. This information should be conveyed to the end-user.
    
    \framebreak
    \item \textbf{Disclosing too much information:} CEs can reveal too much information about the model and help potential attackers.
    \item \textbf{Rashomon effect:} One, few, all? Which CEs should be shown to the end-user? There is no universal solutions, the right way depends on the end-users temporal resources and knowledge. 
    \item \textbf{Actionability vs. fairness:} Some authors suggest to focus only on the actionability of CEs. However, this counteract functions like contestability. E.g., if ethnicity is not permuted in a CE since it is not actionable this could lead to hiding racial biases in the model.
    \item \textbf{Attacking CEs:} Researchers can create models with great performance, which generate arbitrary explanations specified by the ML developer. Thus, the question is how faithful CEs are to the models underlying mechanism.
    \item \textbf{Assumption of constant model:} To provide guidance for the future, CEs assume that their underlying model would not change in the future. However, in reality this assumption is often violated and CEs are not reliable anymore. 
\end{itemize}

%	\textbf{Pitfall 3:} Rashomon Effect
%	\begin{itemize}
%		\item Due to the Rashomon Effect, multiple counterfactual explanations could be found for an instance. 
%		\item If all counterfactuals are reported the user could be overwhelmed. Instead of a comprehensible explanations for a prediction, users received an even more complex explanations.
%		\item Another option is to only report the ``best" ones. But this requires a notion for ``superiority".  
%		\item Furthermore, users might not be interested in the ``best" but most ``diverse" counterfactuals.
%		\item The best option might be to report all counterfactuals but let the user decide which one to select, e.g., based on their previous knowledge. 
%	\end{itemize}
%	\textbf{Pitfall 4:} 
%	\begin{itemize}
%		\item 
%	\end{itemize}
%	\textbf{Pitfall 5:} Confusing model explanation with real data process explanations
%	\begin{itemize}
%		\item Causal dependencies
%		\item Fixed model at time $t$ 
%		\item Wrong input by user
%	\end{itemize}
\end{vbframe}
\endlecture
\end{document}