\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\usepackage[]{graphicx}\usepackage[]{color}
% maxwidth is the original width if it is less than linewidth
% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%
\let\hlipl\hlkwb

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}
\newcommand{\SweaveOpts}[1]{}  % do not interfere with LaTeX
\newcommand{\SweaveInput}[1]{} % because they are not real TeX commands
\newcommand{\Sexpr}[1]{}       % will only be parsed by R

\usepackage[english]{babel}
\usepackage[utf8]{inputenc}

\usepackage{dsfont}
\usepackage{verbatim}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{bm}
\usepackage{csquotes}
\usepackage{multirow}
\usepackage{longtable}
\usepackage{booktabs}
\usepackage{enumerate}
\usepackage[absolute,overlay]{textpos}
\usepackage{psfrag}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{eqnarray}
\usepackage{arydshln}
\usepackage{tabularx}
\usepackage{placeins}
\usepackage{tikz}
\usepackage{setspace}
\usepackage{colortbl}
\usepackage{mathtools}
\usepackage{wrapfig}
\usepackage{bm}

\usetikzlibrary{shapes,arrows,automata,positioning,calc,chains,trees, shadows}
\tikzset{
  %Define standard arrow tip
  >=stealth',
  %Define style for boxes
  punkt/.style={
    rectangle,
    rounded corners,
    draw=black, very thick,
    text width=6.5em,
    minimum height=2em,
    text centered},
  % Define arrow style
  pil/.style={
    ->,
    thick,
    shorten <=2pt,
    shorten >=2pt,}
}

\usepackage{subfig}

% Defines macros and environments
\input{../../style/common.tex}

%\usetheme{lmu-lecture}
 \newcommand{\titlefigure}{figure/lime_robustness_2.png}
\newcommand{\learninggoals}{
\item ?}
\usepackage{../../style/lmu-lecture}

\let\code=\texttt
\let\proglang=\textsf

\setkeys{Gin}{width=0.9\textwidth}

\title{Local Explanations}
% \author{Bernd Bischl, Christoph Molnar, Daniel Schalk, Fabian Scheipl}
\institute{\href{https://compstat-lmu.github.io/lecture_i2ml/}{compstat-lmu.github.io/lecture\_i2ml}}
\date{}

\setbeamertemplate{frametitle}{\expandafter\uppercase\expandafter\insertframetitle}

\begin{document}

	
% Set style/preamble.Rnw as parent.

% Load all R packages and set up knitr

% This file loads R packages, configures knitr options and sets preamble.Rnw as 
% parent file
% IF YOU MODIFY THIS, PLZ ALSO MODIFY setup.Rmd ACCORDINGLY...

% Defines macros and environments

\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}
\input{../../latex-math/ml-interpretable.tex}

\lecturechapter{Increasing Trust in Explanations}
\lecture{Interpretable Machine Learning}

% ------------------------------------------------------------------------------

\begin{vbframe}{Motivation}
	\begin{itemize}
		\item In the previous sections we saw multiple local explanation methods. 
		\item Their explanations should not only make a prediction interpretable ("why did the model came up with this decision?") but also reveal if its trustworthy ("how certain is this explanation?").
		\item Equal to the underlying model, explanations are subject to uncertainty since they are computed from data. 
		\item Uncertainty in explanations undermines users' trust in the explanations and therefore also in the model since the lack of a solid explanation raises concerns about the model's reliability and robustness. 
		\item In the following, we discuss methods to quantify the robustness of IML methods.
		% \item In the following, we discuss a few methods that try to combine IML methods with uncertainty quantification methods. 
		\item While we focus on local IML methods, note that all strategies could be expanded to global IML methods, as well.
	\end{itemize}
\end{vbframe}

\begin{vbframe}
	\begin{itemize}
		\item In general, ML models are also not robust since they rely on training data that itself could be noisy. 
		\item Differentiation between process variance (how does our explanation change if our model is changed) and explanation variance (how does our explanation change if we we use a different subset of data or slightly different parameters).
		\item We focus on the second task.  
		\item What if the ML model was trained on noisy data? 
		\item If we use an explanation model to debug the ML model, it is okay, if our explanation model also takes up the noisy patterns learned in the ML model. 
		\item If we use an explanation model to understand both the predictor but also the underlying true data generating process, we want to focus on the stable patterns learned by the ML model and receive robust explanations. 
	\end{itemize}
\end{vbframe}

\begin{vbframe}{Sources of Uncertainty}
	Two sources of uncertainty could be identified for local explanation methods: 
	\begin{itemize}
		\item Sampling variance in explaining a single data point. 
		E.g., to train a surrogate model for LIME we sample a new data set.   
		\item Sensitivity to choice of parameters. E.g., the user needs to determine the sample size and the kernel width to explain a model with LIME. 
	\end{itemize}
These sources could lead to different explanations although we analyse the same model and the same (or a very similar) data point. 
\footnote[frame]{Zhang et al. (2019). ``Why Should You Trust My Explanation?'' Understanding Uncertainty in LIME Explanations. arXiv preprint arXiv:1904.12991.} 
\end{vbframe}

\begin{vbframe}{Measure Robustness in neighborhood}
	\begin{itemize}
		\item One way to address these limitations is to examine the behavior of the model in a neighborhood of the point of interest. 
		\item Ideally, we would like to receive similar explanations for similar inputs. 
		\item In the previous chapter on the limitations of LIME, we already saw an example, where LIME produced different explanations for very similar data points (with similar predictions).
		\item For LIME and SHAP, Melis and Jaakkola (2018) derived a notion of stability based on discrete, finite-samples neighborhoods: 
		Given a dataset $X = \{\xv_i\}_{i = 1}^n$, a $\epsilon$-neighborhood for $\xv$ is defined as 
		$$ \mathcal{N}_{\epsilon}(\xv) = \{\xv_i \in X | d(\xv, \xv_i) \le \epsilon\}$$
		where $d(\cdot)$ is an  arbitrary distance measure (e.g., Euclidean or Gower distance). 
		\item Robustness for a interpretability method $g$ could then be measured by 
		$$L_{X}(\xv) = \underset{x_i \in \mathcal{N}_{\epsilon}(\xv)}{\arg \max} \frac{||g(\xv) - g(\xv_i)||_2}{d(\xv, \xv_i)}$$
		\item We cannot only impose that the explanation of one particular method is similar for similar data points, but also across multiple IML methods. 
		\item Robustness in this setting could be defined as: 
		
	\end{itemize}
\end{vbframe}

\begin{comment}
\begin{vbframe}{Robustness}
	\begin{itemize}
		\item Robustness in vicinity of data point \url{https://arxiv.org/pdf/1806.08049.pdf} (LIME and SHAP)
		\item Robustness if we repeatedly explain prediction of a data point 
		\item Levels: different seed, different permutation/bootstrap samples (only makes sense if training data is involved), different model fits (process` variance)
	\end{itemize}
\end{vbframe}
\end{comment}

\begin{vbframe}{Out-of-distribution Detection}
	\begin{itemize}
		\item Sampled data for surrogate model (LIME), counterfactuals, permuted instance to calculate marginal contribution (Shapley values), grid data points to predict on (ceteris-paribus/ICE curve)
		\item DBSCAN \url{https://www.aaai.org/Papers/KDD/1996/KDD96-037.pdf}
		\item Classifier trained on joined dataset of original training data and randomly sampled or perturbed data points \url{https://arxiv.org/pdf/1911.02508.pdf}
		\item Fancier: Bayesian NN, generative model \url{http://proceedings.mlr.press/v119/sastry20a/sastry20a.pdf}
	\end{itemize}
\end{vbframe}

\endlecture
\end{document}