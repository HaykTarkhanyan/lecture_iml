\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\usepackage[]{graphicx}\usepackage[]{color}
% maxwidth is the original width if it is less than linewidth
% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%
\let\hlipl\hlkwb

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}
\newcommand{\SweaveOpts}[1]{}  % do not interfere with LaTeX
\newcommand{\SweaveInput}[1]{} % because they are not real TeX commands
\newcommand{\Sexpr}[1]{}       % will only be parsed by R

\usepackage[english]{babel}
\usepackage[utf8]{inputenc}

\usepackage{dsfont}
\usepackage{verbatim}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{bm}
\usepackage{csquotes}
\usepackage{multirow}
\usepackage{longtable}
\usepackage{booktabs}
\usepackage{enumerate}
\usepackage[absolute,overlay]{textpos}
\usepackage{psfrag}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{eqnarray}
\usepackage{arydshln}
\usepackage{tabularx}
\usepackage{placeins}
\usepackage{tikz}
\usepackage{setspace}
\usepackage{colortbl}
\usepackage{mathtools}
\usepackage{wrapfig}
\usepackage{bm}

\usetikzlibrary{shapes,arrows,automata,positioning,calc,chains,trees, shadows}
\tikzset{
  %Define standard arrow tip
  >=stealth',
  %Define style for boxes
  punkt/.style={
    rectangle,
    rounded corners,
    draw=black, very thick,
    text width=6.5em,
    minimum height=2em,
    text centered},
  % Define arrow style
  pil/.style={
    ->,
    thick,
    shorten <=2pt,
    shorten >=2pt,}
}

\usepackage{subfig}

% Defines macros and environments
\input{../../style/common.tex}

%\usetheme{lmu-lecture}
 \newcommand{\titlefigure}{figure/lime.png}
\newcommand{\learninggoals}{
\item Understand motivation for local explanations 
\item Develop an intuition for possible use-cases
\item Know characteristics of local explanation methods}
\usepackage{../../style/lmu-lecture}

\let\code=\texttt
\let\proglang=\textsf

\setkeys{Gin}{width=0.9\textwidth}

\title{Local Explanations}
% \author{Bernd Bischl, Christoph Molnar, Daniel Schalk, Fabian Scheipl}
\institute{\href{https://compstat-lmu.github.io/lecture_i2ml/}{compstat-lmu.github.io/lecture\_i2ml}}
\date{}

\setbeamertemplate{frametitle}{\expandafter\uppercase\expandafter\insertframetitle}

\begin{document}

% Set style/preamble.Rnw as parent.

% Load all R packages and set up knitr

% This file loads R packages, configures knitr options and sets preamble.Rnw as 
% parent file
% IF YOU MODIFY THIS, PLZ ALSO MODIFY setup.Rmd ACCORDINGLY...

% Defines macros and environments

%\input{../../latex-math/basic-math.tex}
%\input{../../latex-math/basic-ml.tex}

\lecturechapter{Introduction}
\lecture{Interpretable Machine Learning}

% ------------------------------------------------------------------------------

\begin{vbframe}{Methodological Motivation}
Except for Shapley Values, we focused so far mostly on global explanation methods that explain global model behaviour (e.g. PDP, PFI, ALE, etc.). However, there are also many methods that explain the \emph{local} behaviour of a model.
	\begin{itemize}
		\item Some local methods provide insight into the driving factors for a \emph{particular decision}. Others, help to understand the model's decision making in a \emph{local environment} of the input space.
		\item Local Methods can address questions such as: 
		\begin{itemize}
		    \item \emph{Why} did the model decide $y$ for input $\xv$?
		    \item \emph{How} decides the model for cases similar to $\xv$?
		    \item \emph{What} would the ML model have decided if $\xv$ differed in $\P$?
		    \item  \emph{Where} does the model fail?
		\end{itemize}  
	\end{itemize}
\end{vbframe}

\begin{vbframe}{Social Motivation}
All these questions can indeed be relevant for the ML modeler. However, unlike in global methods that require expert ML understanding or even domain knowledge, many local methods aim to provide explanations also for laypersons. 
	\begin{itemize}
		\item Explanations for laypersons must be tailored for the \emph{explainee} (the person receiving the explanation).
		\item Thus, such explanations should be case-specific, easy for humans to understand, and faithful to the explained mechanism.
		\item In particular if algorithms make decisions in \emph{socially/safety critical domains}, end-users have justified interest in receiving explanations.
		\item Local Explanations can not only increase \emph{user trust}, but also help to detect \emph{critical biases} in algorithmic decision-making.
		\item European citizens have the legally binding \emph{right to explanation} as given in the General Data Protection Regulation (GDPR).

	\end{itemize}
\end{vbframe}


\begin{vbframe}{GDPR: The Right to Explanation}
    ``The data subject should have the right not to be subject to a decision, which may include a measure, evaluating personal aspects relating to him or her which is based solely on automated processing and which produces legal effects concerning him or her or similarly significantly affects him or her, such as automatic refusal of an online credit application or e-recruiting practices without any human intervention.

$\cdots$

In any case, such processing should be subject to suitable safeguards, which should include specific information to the data subject and the \textbf{right} to obtain human intervention, to express his or her point of view, \textbf{to obtain an explanation of the decision reached after such assessment and to challenge the decision}.
'' (Recital 71)
\end{vbframe}


\begin{vbframe}{Example: Husky or Wolf?}
	\begin{itemize}
		\item We trained a model to predict if an image shows a wolf or a husky. 
		\item Below the predictions on six test images are given. 
		\item Do you trust our predictor? 
	\end{itemize}
	\begin{center}
		\includegraphics[width=0.45\textwidth]{figure/lime-wolfhusky.png}\\
		\includegraphics[width=0.45\textwidth]{figure/lime-wolfhusky2.png}\\
		{\tiny \textbf{Source:} \href{http://www.facweb.iitkgp.ac.in/~niloy/COURSE/Spring2018/IntelligentSystem/PPT_2018/why_should_i_trust_ppt.pdf}{www.facweb.iitkgp.ac.in}}
	\end{center}

	\framebreak
	
	\begin{itemize}
		\item We can use local explanations (in this case LIME) to highlight the parts of an image which led to the prediction.
		\item We can see that our predictor is actually a snow detector. 
	\end{itemize}
	\begin{center}
		\includegraphics[width=0.75\textwidth]{figure/lime-wolfhusky3.png}\\
		{\tiny \textbf{Source:} \href{http://www.facweb.iitkgp.ac.in/~niloy/COURSE/Spring2018/IntelligentSystem/PPT_2018/why_should_i_trust_ppt.pdf}{www.facweb.iitkgp.ac.in}}
	\end{center}
\end{vbframe}

\begin{vbframe}{Example: Loan Application}
Assume you are applying at a bank's online portal for a loan and your application gets immediately rejected without reasons.
	\begin{center}
		\includegraphics[width=0.4\textwidth]{figure/IntroJudge.png}\\
		{\tiny \textbf{Source:} \href{https://www.elte.hu/content/trendfordulo-az-mi-fejlesztesekben.t.19025}{https://www.elte.hu}}
	\end{center}
	If the bank would use local explanation methods, it could e.g. provide a counterfactual explanation:
	\begin{itemize}
	    \item[] If you were older than 21, your loan application would have been accepted.
	\end{itemize}
\end{vbframe}

\begin{vbframe}{Example: Stop or Right-of-Way?}
Assume you work in a car-company and are about to use an image-classifier for autonomous driving. Then, you show your model the following image and its $99\%$ sure it describes a right-of-way sign.
	\begin{center}
		\includegraphics[width=0.4\textwidth]{figure/IntroStop.jpg}\\
		{\tiny \textbf{Source:} [Eykholt et. al]}
	\end{center}
	Would you entrust other peoples lives into the hands of this software?
\end{vbframe}

\begin{vbframe}{Characteristics}
	\begin{itemize}
		\item \textbf{Explanation Scope:} Specific prediction, local environment
		\item \textbf{Model-classes:} Mostly model-agnostic in definition but model-specific for computational reasons. Very popular also for deep learning models.
		\item \textbf{Audience:} Mostly laypersons, but also ML modellers.
		\item \textbf{Data-Types:} Many, including tabular, image, text, audio-data.
		\item \textbf{Methods:} e.g. Counterfactual Explanations, SHAP, Local interpretable model-agnostic explanations (LIME), Adversarial examples, Ceteris-paribus (single ICE curve), Anchors. 
		\item \textbf{Special:} Due to audience, strong interactions with Social Sciences. Also, strong connections to Cognitive Science and Neurosciences due to data-types.
	\end{itemize}
\end{vbframe}

\endlecture
\end{document}
