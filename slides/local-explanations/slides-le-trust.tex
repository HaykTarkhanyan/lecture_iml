\documentclass[11pt,compress,t,notes=noshow, aspectratio=169, xcolor=table]{beamer}

\usepackage{../../style/lmu-lecture}
% Defines macros and environments
\input{../../style/common.tex}

\title{Local Explanations}

\begin{document}

	
% Set style/preamble.Rnw as parent.

% Load all R packages and set up knitr

% This file loads R packages, configures knitr options and sets preamble.Rnw as 
% parent file
% IF YOU MODIFY THIS, PLZ ALSO MODIFY setup.Rmd ACCORDINGLY...

% Defines macros and environments
\newcommand{\titlefigure}{figure/dbscan.jpg}
\newcommand{\learninggoals}{
\item Understand the aspects that undermine users' trust in an explanation
\item Learn diagnostic tools that could increase trust }

\lecturechapter{Increasing Trust in Explanations}
\lecture{Interpretable Machine Learning}

% ------------------------------------------------------------------------------


% ------------------------------------------------------------------------------

\begin{vbframe}{Motivation \& Important Properties}
	\begin{itemize}
		\item Local explanations should not only make a model interpretable but also reveal if the model is trustworthy.
	    \item \textbf{Interpretable}: ``why did the model come up with this decision?''
	    \item \textbf{Trustworthy}: ``how certain is this explanation?''
	    \begin{enumerate}
	        \item accurate insights into the inner workings of our model
	        \begin{itemize}
	            \item Failure case: generation is based on inputs in areas where the model was trained with little or no training data (extrapolation).
	        \end{itemize}
	        \item robust (i.e. low variance)
	        \begin{itemize}
	            \item Expectation: similar explanations for similar data points with similar predictions
	            \item However, multiple sources of uncertainty exist.
	            \item[$\leadsto$] measure how robust an IML method is to small changes in the input data or parameters.
	            \item[$\leadsto$] Is a point out of distribution?
	        \end{itemize}
	    \end{enumerate}
		\item Failing in one of these $\leadsto$ undermining users' trust in the explanations $\leadsto$ undermining trust in the model. 
		%\item In the upcoming section, we will, therefore, discuss methods to detect if a point is out-of-distribution and to measure how robust IML methods are.
	\end{itemize}
\end{vbframe}

\begin{vbframe}[c]{Out-of-distribution Detection}
	\begin{itemize}
		\item In areas with little data support, models are unreliable and so are the explanations we receive from our local-explanation methods.
		\item For local explanation methods, the following components could be out-of-distribution (OOD): 
		\begin{itemize}
			\item the data for LIME's surrogate model
			\item Counterfactuals themselves
			\item Shapley value's permuted instances to calculate the marginal contributions 
			\item ICE curves grid data points 
		\end{itemize}
		\item Two very simple and intuitive approaches
		\begin{itemize}
		    \item classifier for out of distribution
		    \item clustering
		\end{itemize}
		\item more complicated also possible, e.g., variational autoencoders [\href{https://arxiv.org/abs/1912.05651}{Daxberger et al. 2020}]
	\end{itemize}
\end{vbframe}


\begin{vbframe}[c]{Out-of-distribution Detection: OOD-Classifier}
	\begin{itemize}
	    \item Problem: we have only in-distribution data
	    \item Idea: Hallucinate new (out-of-distribution) data by randomly sample data points
	    \item[$\leadsto$] Learn a binary classifier to distinguish between the origins of the data
	    \medskip
	    \pause
	    \item Study whether an explanation approach can be fooled [\href{https://arxiv.org/abs/1911.02508}{Dylan Slack et al. 2020}]
	    \begin{itemize}
	        \item Hide bias in the true (deployed) model, but use an unbiased model for all out-of-distribution samples.
	    \end{itemize}
	    \item[$\leadsto$] Important way to diagnose an explanation approach
	\end{itemize}
\end{vbframe}

\begin{frame}[c]{Out-of-distribution Detection: Clustering via DBSCAN}
\begin{itemize}
	\item DBSCAN is a data clustering algorithm [\href{https://www.aaai.org/Papers/KDD/1996/KDD96-037.pdf}{Martin Ester et al. 1996}] \\ (Density-Based Spatial Clustering of Applications with Noise) 
	\item For this method, we define an $\epsilon$-neighborhood: \\
	Given a dataset $X = \{\xv_i\}_{i = 1}^n$, a $\epsilon$-neighborhood for $\xv \in \Xspace$ is defined as 
	$$ \mathcal{N}_{\epsilon}(\xv) = \{\xv_i \in X | d(\xv, \xv_i) \le \epsilon\}.$$
	 $d(\cdot)$ is a distance measure (e.g., Euclidean or Gower distance). 
	\item core points $\xv$
	\begin{itemize}
	    \item have at least $m$ data points within $\mathcal{N}_{\epsilon}(\xv)$
	    \item forms an own cluster with all its neighborhood points
	\end{itemize}
    \item border points
    \begin{itemize}
        \item within $\mathcal{N}_{\epsilon}(\xv)$
        \item part of a cluster defined by a core point
    \end{itemize}
    \item noise points
    \begin{itemize}
        \item are not within $\mathcal{N}_{\epsilon}(\xv)$
        \item not part of any cluster
    \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}[c]{Out-of-distribution Detection}
\vspace{-0.6cm}
\begin{columns}
	\begin{column}{0.5\textwidth}
	
		\begin{itemize}
			\item In the right figure, the (green) points A and B are core points and form one cluster since they lie in each others neighborhood, all yellow points are border points of this cluster. 
			\item Since D is not part of the neighborhood of core points, it is a noise point. 
			\item In-distribution: new point lies within a cluster
		    \item Out-of-distribution: new point lies outside the clusters 
		\end{itemize}
	\end{column}
	\begin{column}{0.5\textwidth}
	    \vspace{-2em}
		\begin{center}
			\includegraphics[width=0.6\textwidth]{figure/dbscan.jpg}\\
			\tiny{Example for DBSCAN, circles display $\epsilon$-neighborhoods, $m = 4$}
		\end{center}
	\end{column}
\end{columns}

\begin{itemize}
		\item Disadvantages:
		\begin{itemize}
		    \item Depending on the distance metric $d(\cdot)$, DBSCAN could suffer from the ``curse of dimensionality''. 
		    \item The choice of $\epsilon$ and $m$ is not clear a-priori. 
		\end{itemize}
\end{itemize}
\end{frame}

\begin{vbframe}{Robustness}
		\begin{itemize}
		\item In general we must differentiate between explanation uncertainty and process uncertainty that lead to non-robust explanations: 
		\begin{enumerate}
			\item Explanation uncertainty tries to answer how an explanation changes if we repeat the experiment. The explanation could differ depending on which subset of data we use for the explanation method and which parameters. 
			\item Process uncertainty tries to answer how an explanation changes if the underlying model is changed. This is interesting since ML models can be non-robust, e.g., because they are trained on noisy data. 
		\end{enumerate}
		\item Here, we focus on the former - explanation uncertainty. We could receive different explanations although we analyse the same model and the same (or a very similar) data point.
	\end{itemize}
\end{vbframe}

\begin{vbframe}{Robustness Measure for LIME and SHAP}  
	\begin{itemize}
		\item One way to quantify the robustness of an explanation is to examine the behavior of the model in a neighborhood of the point of interest. We would like to receive similar explanations for similar inputs. 
		\item In the previous chapter on the limitations of LIME, we already saw an example where LIME produced different explanations for very similar data points (with similar predictions).
		\item For LIME and SHAP, Alvarez-Melis and Jaakkola (2018) derived a notion of stability based on \textbf{locally Lipschitz continuity}: \\
		An explanation method $g:\Xspace \rightarrow \R^m$ is locally Lipschitz if for every $\xv_0 \in \Xspace$ there exist $\delta > 0$ and $\omega \in \R$ such that $||\xv - \xv_0|| < \delta$ implies $||g(\xv) - g(\xv_0)|| < \omega ||\xv - \xv_0||$. \\
		\footnotesize Note that, for LIME, $g$ returns the $m$ coefficients of the surrogate model. \normalsize
		\item According to this definition we can quantify and compare the robustness of explanation models in terms of $\omega$. 
		The closer $\omega$ is to 0, the more robust our explanation method is. 
		\item $\omega$ is rarely known a-priori but it could be estimated as following: 
		$$\hat{\omega}_{X}(\xv) = \underset{x_i \in \mathcal{N}_{\epsilon}(\xv)}{\arg \max} \frac{||g(\xv) - g(\xv_i)||_2}{d(\xv, \xv_i)},$$
		where $\mathcal{N}_{\epsilon}(\xv)$ is the $\epsilon$-neighborhood of $\xv$.
	\end{itemize}
\vspace{2cm}
\footnote[frame]{Alvarez-Melis, D., \& Jaakkola, T. (2018). On the Robustness of Interpretability Methods. ArXiv, abs/1806.08049.}
\end{vbframe}


\begin{comment}
\begin{vbframe}{Sources of Uncertainty}
Two sources of uncertainty could be identified for local explanation methods: 
\begin{itemize}
\item Sampling variance in explaining a single data point. 
E.g., to train a surrogate model for LIME we sample a new data set.   
\item Sensitivity to choice of parameters. E.g., the user needs to determine the sample size and the kernel width to explain a model with LIME. 
\end{itemize}
These sources could lead to different explanations although we analyse the same model and the same (or a very similar) data point. 
\footnote[frame]{Zhang et al. (2019). ``Why Should You Trust My Explanation?'' Understanding Uncertainty in LIME Explanations. arXiv preprint arXiv:1904.12991.} 
\end{vbframe}
\end{comment}


\begin{comment}
\begin{vbframe}{Noisy data}
	\begin{itemize}
		\item What if the ML model was trained on noisy data? Should the explanation model also take up these noisy patterns? 
		\item If we use an explanation model to debug the ML model, it is okay, if our explanation model also takes up the noisy patterns learned in the ML model. 
		\item If we use an explanation model to understand both the predictor but also the underlying true data generating process, we want to focus on the stable patterns learned by the ML model and receive robust explanations. 
	\end{itemize}
\end{vbframe}

\begin{vbframe}{Addtional notes}
	\begin{itemize}
		\item We cannot only impose that the explanation of one particular method is similar for similar data points, but also across multiple IML methods. 
	\end{itemize}
\end{vbframe}

content...
\end{comment}

\endlecture
\end{document}