\documentclass[11pt,compress,t,notes=noshow, aspectratio=169, xcolor=table]{beamer}

\usepackage{../../style/lmu-lecture}
% Defines macros and environments
\input{../../style/common.tex}

\title{Interpretable Machine Learning}
% \author{LMU}
%\institute{\href{https://compstat-lmu.github.io/lecture_iml/}{compstat-lmu.github.io/lecture\_iml}}
\date{}
\newcommand{\betah}{\hat{\beta}}

\begin{document}
	
	% Set style/preamble.Rnw as parent.
	
	% Load all R packages and set up knitr
	
	% This file loads R packages, configures knitr options and sets preamble.Rnw as 
	% parent file
	% IF YOU MODIFY THIS, PLZ ALSO MODIFY setup.Rmd ACCORDINGLY...
	
	% Defines macros and environments
	
	\newcommand{\titlefigure}{figure/lime5}
    \newcommand{\learninggoals}{
    	\item Learn why LIME should be used with caution
    	\item Possible pitfalls of LIME}
	
	\lecturechapter{LIME Pitfalls}
	\lecture{Interpretable Machine Learning}
	
	% Prerequisite: le-into, le-lime
	
	% ------------------------------------------------------------------------------


\begin{vbframe}[c]{LIME Pitfalls}
  \begin{itemize}
  	\item LIME is one of the best known interpretable machine learning methods\\ but several papers caution to be careful in their use 
  	\item Problems can occur in several places: 
  	\begin{itemize}
  	    \item Sampling procedure (extrapolation)
  	    \item Definition of locality (sensitivity)
  	    \item Scope of feature effects (local vs. global)
  	    \item Faithfulness (trade-off with sparsity)
  	    \item Surrogate model (robustness, biases)
  	    \item Definition of superpixels (sensitivity)
  	\end{itemize}
  	\item These are discussed in more detail in the following 
  \end{itemize}
  
\end{vbframe}
  
\begin{vbframe}[c]{LIME Pitfalls: Sampling}
	\begin{itemize}
	\itemsep1em
	  \item \textbf{Pitfall}: Common sampling strategies for $\zv \in Z$ do not account for correlation between features 
      \item \textbf{Implication}:  Unlikely data points might be used to learn local explanation models
      \item \textbf{Solution I}: Ideally, local sampler directly on $\Xspace$\\
      $\leadsto$ derivation is particularly difficult for high dimensional or mixed feature spaces 
      \item \textbf{Solution II}: Use training data to fit surrogate model (only works well with enough data near $\xv$)
    \end{itemize}
    
\end{vbframe}

\begin{vbframe}[c]{LIME Pitfall: Locality}

	\begin{itemize} 
     \item \textbf{Pitfall}: Difficult to define locality (= how samples are weighted locally) 
     \begin{itemize}
         \item[$\leadsto$] Huge influence on local model, but there is no automatic procedure for choosing neighborhood
     \end{itemize}
     \item Originally, an exponential kernel as proximity measure between $\xv$ and $\zv$ was used:\\
     	$\neigh(\zv) = exp(-d(\xv, \zv)^2/\sigma^2)$ where $d$ is a distance measure and $\sigma$ is the kernel width 
     	 \begin{center}
     		\includegraphics[width=0.6\textwidth]{figure/lime_locality}
     		\vspace{-0.5cm}
     		
     		\scriptsize{Linear surrogate models for two observations based on the same model with one target and one feature. Each line displays one linear surrogate model with different kernel width. In the right figure, larger kernel sizes are more severe.}
     		
     	\end{center}
     \end{itemize}
\end{vbframe}

\begin{vbframe}[c]{LIME Pitfall: Locality \citebutton{Kopper et al. 2019}{https://slds-lmu.github.io/iml_methods_limitations/}}
    \begin{itemize} 
         \item \textbf{Solution I}: Kernel width strongly interacts with locality:
         \begin{itemize}
             \item Large kernel width leads to interaction with points further away (unwanted)
             \item Small kernel width leads to small neighborhood\\
             $\leadsto$ Risk of few data points $\leadsto$ potentially fitting more noise
         \end{itemize}
    	\item \textbf{Solution II}: Gower proximity where no kernel width needs to be specified 
    	\begin{itemize}
    	    \item \textbf{Problem}: data points far away receive weight $ > 0$ $\leadsto$ resulting explanations are rather global than local surrogates   
    	\end{itemize}
    \end{itemize}
\vspace{0.3cm}

\end{vbframe}

\begin{vbframe}[c]{LIME Pitfalls: Local vs. Global Features \citebutton{Laugel et al. 2018}{https://arxiv.org/pdf/1806.07498.pdf}}

\begin{itemize}
	\item \textbf{Problem}: Some features influence the \textbf{global} shape of the black-box model vs. \textbf{local} features impact predictions for a small area of $\Xspace$ 
	\item \textbf{Example}: Decision trees; splitting variables close to the root have a more global influence than the ones close to the leaves
	\item \textbf{Problem}: By sampling instances for the surrogate model from the whole input space, the influence of local features is hidden for the benefit of features with a global influence (even when the kernel width was reduced) 
\end{itemize}

\end{vbframe}


\begin{vbframe}{LIME Pitfalls: Local vs. Global Features: Example \citebutton{Laugel et al. 2018}{https://arxiv.org/pdf/1806.07498.pdf}}

\begin{columns}
	\begin{column}{0.6\textwidth}
		\begin{itemize}
		\item Binary classification model
		\item Given in figure to the right:
		\begin{itemize}
		    \item Black and white dots: training data
		    \item Green dot: To be explained
		    \item Background color: Classification of random forest
		    \item Grey curve: Decision boundary
		\end{itemize}
		\item \textbf{Observation: }The decision boundary of LIME with different kernels (blue and green lines) do not match the direction of the steeper local boundary. 
	\end{itemize}
\end{column}
\begin{column}{0.39\textwidth}
\vspace{0.3cm}

	\begin{center}
	\includegraphics[width=1\textwidth]{figure/lime-globallocal2}

	{Half-moons dataset}
	
\end{center}

	\end{column}
\end{columns}
%\footnote[frame]{Laugel et al. (2018). Defining Locality for Surrogates in Post-hoc Interpretability. \url{https://arxiv.org/pdf/1806.07498.pdf}.}
\end{vbframe}

\begin{vbframe}{LIME Pitfalls: Local vs. Global Features: Solution \citebutton{Laugel et al. 2018}{https://arxiv.org/pdf/1806.07498.pdf}}
\begin{columns}
	\begin{column}{0.6\textwidth}
	\vspace{-.5cm}
		\begin{itemize}
		\item \textbf{Solution}: Sample new instances $\zv$ around the decision boundary closest from point $\xv$ for higher local accuracy
		\item The red dot in the figure to the right corresponds to the closest instance from the other class (black cross in the figure below)
		\item This method (red line) approximates the local border direction better 
	\end{itemize}
	\begin{center}
		\includegraphics[width=1\textwidth]{figure/laugel_method}
	    {Local surrogate method by Laugel et al.}
		\vspace{-0.3cm}
		\end{center}
\end{column}
\begin{column}{0.39\textwidth}
\vspace{0.3cm}

	\begin{center}
	\includegraphics[width=1\textwidth]{figure/lime-globallocal2}
	
	{Half-moons dataset}
	
\end{center}

	\end{column}

\end{columns}
%\footnote[frame]{Laugel et al. (2018). Defining Locality for Surrogates in Post-hoc Interpretability. \url{https://arxiv.org/pdf/1806.07498.pdf}.}
\end{vbframe}


\begin{vbframe}[c]{LIME Pitfalls: Faithfulness}
\begin{itemize}
\itemsep1em
	\item \textbf{Problem}: Trade-off between local fidelity vs. sparsity
	\item \textbf{Observation I}: Low fidelity $\leadsto$ unreliable explanations
	\item \textbf{Observation II}: High fidelity requires more complex models $\leadsto$ surrogate model cannot easily be interpreted
	\item \textbf{Example: Credit data} 
	\begin{itemize}
	    \item Original prediction by random forest for one data point $\xv$: $\fh(\xv) = \hat{\P}(y = 1 ~|~ \xv) = 0.143$
	    \item Regularized linear model with only three selected features (\code{sex}, \code{checking.account}, \code{duration}) $g_{lm}(\xv) = 0.283$
	    \item Generalized additive model (all 9 features) 
    \begin{equation*} 
    \begin{split}
    g_{gam}(\xv) & = \betah_0 + s_{age}(x_{age}) +s_{credit.amount}(x_{credit.amount}) s_{duration}(x_{duration}) + \betah_{sex = male} \Ind_{sex = male}   \\
    & + \betah_{job}(x_{job}) + \betah_{housing = own} \Ind_{housing = own} +   \betah_{housing = rent} \Ind_{housing = rent} \\
    & + \betah_{saving.accounts = moderate} \Ind_{saving.accounts = moderate} + \betah_{saving.accounts = rich} \Ind_{saving.accounts = rich} \\
    & + ... + \betah_{purpose = radio/TV} \Ind_{purpose = radio/TV}  = 0.148 
    \end{split}
    \end{equation*}
	\end{itemize}
\end{itemize}

\end{vbframe}

\begin{vbframe}{LIME Pitfalls: Hiding biases \citebutton{Slack et al. 2020}{https://arxiv.org/abs/1911.02508}}

\begin{itemize}
	\item \textbf{Problem}: Developer could manipulate their model to hide biases 
	\item \textbf{Insight}: LIME can sample out-of-distribution points
	\item \textbf{Attack}:
	\begin{enumerate}
	    \item classifier to discriminate between in-distribution and out-of-distribution data points
	    \item for in-distribution points, use the original (biased) model
	    \item for out-of-distribution points, use an unbiased model 
	    \item[$\leadsto$] LIME samples out-of-distribution points and uses the unbiased model for local explanation $\leadsto$ this hides the bias of the true model
	\end{enumerate}
	\item \textbf{Example}: Credit dataset; a biased model trained on Feature `gender`; unbiased model could be trained on features uncorrelated with `gender`
\end{itemize}
\end{vbframe}

\begin{vbframe}[c]{LIME Pitfalls: Robustness \citebutton{Alvarez-Melis, D., \& Jaakkola, T. 2018}{https://arxiv.org/abs/1806.08049}}
\begin{itemize}
	\item \textbf{Problem}: Instability of explanations 
	\item \textbf{Observation}: Explanations of two very close points could vary greatly 
	\begin{itemize}
	    \item[$\leadsto$] can happen because of other sampled data points $\zv$
	\end{itemize}
\end{itemize}
\vspace{-0.7cm}
\begin{columns}
	\begin{column}{0.48\textwidth}
		\begin{center}
		
		\includegraphics[width=0.55\textwidth]{figure/lime_robustness_1.png}
		
		{Linear prediction task (logistic regression). \\Linear surrogate returns similar coefficients for similar points.}
		
		\end{center}
	\end{column}
	\begin{column}{0.48\textwidth}
		\begin{center}
	\includegraphics[width=0.55\textwidth]{figure/lime_robustness_2.png}
	
	{Circular prediction task (random forest). \\Linear surrogate returns different coefficients for similar points.}
	
	\end{center}
\end{column}
\end{columns}
\end{vbframe}

\begin{vbframe}{LIME Pitfalls: Definition of Superpixels \citebutton{Achanta et al. 2012}{https://ieeexplore.ieee.org/document/6205760}}

\begin{columns}
    
    \begin{column}{0.6\textwidth}
        
        \begin{itemize}
        	\item \textbf{Problem}: Instability because of specification of superpixels for image data 
        	\item \textbf{Observation}: Multiple specification of superpixels exist, influencing both the shape and size 
        	\item \textbf{Implication}: The specification of superpixel has a large influence on the explanations 
        	\item \textbf{Attack}: Change superpixels as part of an adversarial attack $\leadsto$ changed explanation
        \end{itemize}
        
    \end{column}
    
    \begin{column}{0.4\textwidth}
    
        \centering
        \includegraphics[width=0.7\textwidth]{figure/superpixel_woman}
        
    \end{column}
    
\end{columns}

\end{vbframe}


\endlecture
\end{document}