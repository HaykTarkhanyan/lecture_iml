\documentclass[11pt,compress,t,notes=noshow, aspectratio=169, xcolor=table]{beamer}

\usepackage{../../style/lmu-lecture}
% Defines macros and environments
\input{../../style/common.tex}

\title{Interpretable Machine Learning}
% \author{LMU}
%\institute{\href{https://compstat-lmu.github.io/lecture_iml/}{compstat-lmu.github.io/lecture\_iml}}
\date{}

\begin{document}

% Set style/preamble.Rnw as parent.
\newcommand{\vertiii}[1]{{\left\vert\kern-0.25ex\left\vert\kern-0.25ex\left\vert #1 
    \right\vert\kern-0.25ex\right\vert\kern-0.25ex\right\vert}}
% Load all R packages and set up knitr

% This file loads R packages, configures knitr options and sets preamble.Rnw as 
% parent file
% IF YOU MODIFY THIS, PLZ ALSO MODIFY setup.Rmd ACCORDINGLY...

% Defines macros and environments
 \newcommand{\titlefigure}{figure/AEturtle.jpg}
\newcommand{\learninggoals}{
\item Understand the definition of AEs and their relation to Counterfactual Explanations
\item Understand different methods that generate AEs
\item Discuss potential causes of AEs and standard defenses against them}

\lecturechapter{\Large{Local Explanations: Adversarial Examples}}
\lecture{Interpretable Machine Learning}

% ------------------------------------------------------------------------------

\begin{frame}[c]{Adversarial Machine Learning}
\begin{itemize}
    \item What happens if a computer system gets an erroneous input?
    \item Even worse:\\ What happens if someone feeds in a malicious input on purpose to attack a system?
    \item[$\leadsto$] \textbf{Robustness} is important to ensure a safe service!
    \medskip
    \item \textbf{Adversarial ML} studies the robustness of machine learning (ML) algorithms to malicious input
    \item Two different kinds of attacks:
    \begin{itemize}
        \item \textbf{Evasion attacks} mislead an employed ML model with manipulated inputs (our focus)
        \item \textbf{Data Poisoning}: Malicious inputs to the training dataset
    \end{itemize}
    \end{itemize}
\end{frame}

\begin{vbframe}[c]{Adversarial Examples}
%The inputs by which evasion attacks can be conducted are called \textbf{Adversarial Examples (AEs)}
\begin{itemize}
    \item \textbf{Informal Definition}: An AE is an input to a model that is deliberately designed to "fool" the model into misclassifying it
    \item Even possible with low generalization error
    \item Both deep Learning models (e.g., CNNs) and classical ML can be vulnerable to such attacks
    \item AEs created from a real data observation $\xv$ can be indistinguishable from $\xv$ by a human observer 
    \item Missing of a real understanding of the underlying concepts of the provided inputs?
\end{itemize}
\end{vbframe}

\begin{vbframe}{Examples: Model-Attacks \citebutton{Gong \& Poellabauer 2018}{https://arxiv.org/pdf/1803.09156.pdf}}

\begin{columns}

\begin{column}{0.6\textwidth}

\begin{figure}[h]
\centering
  \includegraphics[width=1\linewidth]{figure/AEduckSound.png}
  \label{fig:mnist}
\end{figure} 

\end{column}

\begin{column}{0.4\textwidth}

    \vspace{3em}
    \begin{itemize}
        \item Is this a duck or a horse?
        \item Small (hard-to-see) noise can change the prediction
    \end{itemize}

\end{column}

\end{columns}

\end{vbframe}

\begin{vbframe}[c]{Examples: Image Data \citebutton{Eykholt et al. (2018)}{https://arxiv.org/pdf/1807.07769.pdf}  \citebutton{Athalye et al. (2018)}{https://arxiv.org/pdf/1707.07397.pdf}}
\begin{figure}[h]
\centering
\includegraphics[width=0.46\linewidth]{figure/AEstop.png}\quad \includegraphics[width=0.45\linewidth]{figure/AEturtle.jpg}
  \label{fig:mnist}
\end{figure} 

\begin{columns}

\begin{column}{0.5\textwidth}

\begin{itemize}
    \item Stop signs can be missclassified\\ e.g., because of graffiti
    \item With some well-placed patches, the model identifies it as a ``right of way'' sign
\end{itemize}

\end{column}

\begin{column}{0.5\textwidth}

\begin{itemize}
    \item 3D-print of a turtle
    \item Misclassified as a rifle (from every angle)
\end{itemize}

\end{column}

\end{columns}


\end{vbframe}

\begin{vbframe}{Example: Tabular Data \citebutton{Ballet (2019)}{https://arxiv.org/pdf/1911.03274.pdf}}
What is imperceptibility on tabular data?
\begin{itemize}
    \item Idea: experts focus on the most important features in their judgment
    \item An AE arises from manipulating features the model deems important but experts do not
\end{itemize}
\begin{figure}[h]
\centering
\includegraphics[width=0.6\linewidth]{figure/AEloanApplication.png}\\
   \centering
  {Decision boundary of a classifier deciding loan applications. AE via ``number of pets''}
  \label{fig:mnist}
\end{figure} 

\end{vbframe}

\begin{vbframe}[c]{AE and Interpretability}

\begin{enumerate}
    \item AEs show where models fail $\leadsto$ improved model understanding
    \item Because of AEs, we need more interpretability
    \item Interpretation can lead to robustness against AEs
    \medskip
    \item Explanations can be used to construct AEs (e.g., see numer of pets on previous slide)
\end{enumerate}

\end{vbframe}

\begin{vbframe}[c]{Formal Definition}
\begin{block}{Adversarial Input}
Let $\epsilon>0$, $f:\Xspace \rightarrow \Yspace$ be an ML model and $\xv \in \Xspace$ be a real data point that is correctly classified: $f(\xv)=y_{\xv,true}$. \\\medskip
 We call $a_{\xv}$ an \textbf{adversarial input} to $\xv$ if:
\begin{equation*}
    \| a_{\xv}- \xv\|<\epsilon\text{ and } f(a_{\xv})\neq y_{a_{\xv},true}=f(\xv)
\end{equation*}
\end{block}
\begin{itemize}
    \item $a_{\xv}$ an is a data point close to a real, correctly classified input that is misclassified
    \item $a_{\xv}$ is called \textbf{targeted} if the class it is assigned to is determined\\
    $f(a_{\xv})=y'$ with $y'$ being a desired prediction
    \item Can be generalized to regression problems
\end{itemize}
\end{vbframe}


% \begin{vbframe}{AE and Counterfactual Explanations}
% It seems as if AEs and counterfactual explanations (CEs) are defined similarly. Both AEs and CEs describe inputs close to a given input $\xv$ that gets a different assignment. What are their differences?
% \begin{itemize}
%     \item Counterfactuals do not have to be misclassified.
% %    \item Counterfactuals should be maximally close to $\xv$.
%     \item Different notions of distance $\|\cdot\|$ are applied, e.g., $p_{2,\infty}$-norm for AEs or $p_{0,1}$-norm for CEs.
%     \item Informal difference I: AEs are mostly considered for high-dimensional data, while CEs are mostly considered in the context of low-dimensional data.
%     \item Informal difference II: AEs hide changes while CEs highlight them.
%     \item \textbf{Shared example:} ``If you had two more pets, your loan application would have been granted" is an example of both AEs and CEs.
% \end{itemize}
% \end{vbframe}


\begin{vbframe}[c]{Why Do AE Exist?}
    Non-exhaustive list of hypotheses:
    \begin{itemize}
        \item \textbf{Low-probability spaces hypotheses:} AEs live in low-probability yet dense spaces in the data manifold that are not well represented in the training samples \citebutton{Szegedy et al. (2013)}{https://arxiv.org/abs/1312.6199}
        \smallskip
        \item \textbf{Linearity hypotheses (most popular):} Adversarial examples are omnipresent in the data manifold $\leadsto$ occur, because commonly used models often show linear behavior $\leadsto$ small changes of $\epsilon$ in every feature cause a change of $\epsilon\|\thetab\|_1$ in prediction \citebutton{Goodfellow et al. (2014)}{https://arxiv.org/abs/1412.6572}
        \smallskip
        \item \textbf{The boundary tilting hypothesis:} Linearity is neither necessary nor sufficient to explain AEs $\leadsto$ AEs mostly result from overfitting the sampled manifold \citebutton{Tanay and Griffin (2016)}{https://arxiv.org/abs/1608.07690}
        \smallskip
        \item \textbf{Human-centric hypotheses:} ML models make use of predictive but non-robust features -- meaning they are highly correlated with the prediction target, but not used by humans \citebutton{Ilyas et al. (2019)}{https://papers.nips.cc/paper/2019/hash/e2c420d928d4bf8ce0ff2ec19b371514-Abstract.html}
    \end{itemize}

\end{vbframe}

\begin{vbframe}[c]{Ways to Generate AE}
Different ways for constructing AEs:
There exist various ways in the literature to generate AEs for a given model in feasible time
\begin{itemize}
    \item Formulate the search for AEs as an \textbf{optimization problem}, e.g. 
    \begin{equation*}
        \label{eq:optimization}
        \underset{\xv'\in \Xspace}{\text{argmin}}\; \| \xv-\xv' \|_{\Xspace} + \lambda\;    \|f(\xv')-y'\|_{\Yspace}
    \end{equation*}
    \item Use \textbf{sensitivity analysis} to identify features that influence the target class
    \item Train a generative network to generate adversarials
\end{itemize}
Moreover, depending on the attacker's model access, we can distinguish between
\begin{itemize}
    \item \textbf{Full-access attacks}: the attacker has full access to the internals of the model
    \item \textbf{Black-box attacks}: the attacker can only query the model on some inputs and receives the model's outputs
\end{itemize}
\end{vbframe}

\begin{vbframe}{Fast-Gradient-Sign-Method (FGSM) \citebutton{Goodfellow et al. (2015)}{https://arxiv.org/pdf/1412.6572.pdf}}
% Since we have seen optimization methods for CEs, we focus on a method using sensitivity analysis, namely FGSM.
\begin{itemize}
    \item FGSM is based on the linearity hypothesis
    \item FGSM finds AEs from:
    \begin{equation*}
        a_{\xv}=\xv+\epsilon\cdot\text{sign}(\nabla_{\xv} J(\theta,\xv,y_{\xv,true}))
    \end{equation*}
    where $\text{sign}(\nabla_{\xv} J(\theta,x,y_{\xv,true}))$ describes the component-wise signum of the gradient of cost function $J$ in $\xv$ with true label $y_{\xv,true}$
\end{itemize}
\begin{figure}[h]
\centering
\includegraphics[width=0.6\linewidth]{figure/AEpanda.png}
  \label{fig:mnist}
\end{figure} 

\end{vbframe}

\begin{vbframe}[c]{Notes on FGSM \citebutton{Goodfellow et al. (2015)}{https://arxiv.org/pdf/1412.6572.pdf} \citebutton{Lyu et al. 2015}{https://arxiv.org/abs/1511.06385}}
\begin{itemize}
    \item FGSM works particularly well for linear(-like) models in high-dimensional spaces,\\ e.g., LSTMs, logistic regressions or CNNs with ReLU activations
    \item Not every $a_{\xv}$ generated by FGSM is an AE, especially if $\epsilon$ is too small
    \item FGSM attacks can be also generated without model access by approximating the gradient,\\ e.g. with finite difference methods
    \item The notion of similarity in FGSM is based on $\|\cdot\|_{\infty}$ $\leadsto$ there are generalizations of FGSM to other norms
\end{itemize}
\end{vbframe}


\begin{vbframe}[c]{Black-Box Attacks with Surrogates \citebutton{Papernot et al. (2016)}{https://arxiv.org/abs/1605.07277}}

\begin{itemize}
    \item So far, we assumed full access to the predictive model
    \item Black-box attacks only assume query-access
    \item Large risk of attacks since often one can query predictive models many times
\end{itemize}

\medskip
\begin{enumerate}
    \item Query the model you aim to attack as often as allowed on data similar to the training data
    \item Use the labeled data you received to train a surrogate model
    \item Generate AEs for the surrogate model
    \item Use these AEs to attack the original model
\end{enumerate}

\medskip
\begin{itemize}
    \item[$\leadsto$] Known as the \textbf{transferability} of AEs.
\end{itemize}



\end{vbframe}

\begin{vbframe}[c]{Defenses Against AE}
There are several ways to protect your network against such attacks -- we distinguish between two broad types of defenses, differing in the position in which they act
\begin{itemize}
    \item \textbf{Guards} act on the inputs a model receives
    \begin{itemize}
        \item \textbf{Detect anomalies:} e.g., statistical testing, or discriminator networks from GANs
        \item \textbf{Conduct transformations} on inputs (e.g. PCA)
    \end{itemize}
    \item \textbf{Defense by design} act on the model itself
    \begin{itemize}
        \item \textbf{Adversarial training}: train model on adversarials
        \item \textbf{Architectural defenses}: e.g., removing low predictive features from the model)
    \end{itemize}
\end{itemize}
\end{vbframe}

%\begin{vbframe}{Regularization Against AEs}
%The use of regularization techniques against AEs is motivated by most hypotheses that explain AEs. As an example we look at a technique based on the FGSM and the linearity hypotheses.
%\begin{itemize}
%    \item Goodfellow et al. suggest to specfiy the cost function s.t.
%    \begin{equation*}
%        \tilde{J}(\theta,\xv,y):=\alpha J(\theta,\xv,y)+(1-\alpha) J(\theta,\xv+\epsilon\cdot\text{sign}(\nabla_{\xv} J(\theta,\xv,y)),y)
%    \end{equation*}
%    with $\alpha=0.5$.
%    \item This increased the model robustness against FGSM generated AEs.
%    \item However, model are not only still prone to other AEs but also to many FGSM-AEs.
%    \item Applying such regularization terms can increase test error.
%\end{itemize}
%\end{vbframe}

\begin{vbframe}[c]{Summary}
\begin{itemize}
    \item AEs are not explanations themselves but are conceptually connected to them
    \item AEs can be generated in diverse settings $\leadsto$ crucial modeling decisions are the distance measure, the local environment, and the target level (model or process)
    \item There are various hypotheses on the existence of AEs which also motivate different defense strategies
\end{itemize}
\end{vbframe}

%\begin{vbframe}{Outlook}
%\begin{itemize}
%    \item Even in highly non-linear models AEs occur. As long as their existence is not well-understood, defenses against them will have limited power.
%    \item More and more different distance measures are considered, e.g. $p_0$ for one-pixel attacks, the Wasserstein-metric or psychologically motivated measures like the Perceptual Adversarial Similarity Score (PASS).
%    \item Recent work considered AEs that fool both, humans and ML models. AEs may be a case where research on human and machine perception can profit from each other.
%\end{itemize}
%@misc{rozsa2016adversarial,
%      title={Adversarial Diversity and Hard Positive Generation}, 
%      author={Andras Rozsa and Ethan M. Rudd and Terrance E. Boult},
%      year={2016},
%      eprint={1605.01775},
%      archivePrefix={arXiv},
%      primaryClass={cs.CV}
%}
%\end{vbframe}

% \begin{vbframe}{Central References}
% \begin{itemize}
%     \item Serban, A., Poll, E., \& Visser, J. (2020). Adversarial examples on object recognition: A comprehensive survey. ACM Computing Surveys (CSUR), 53(3), 1-38.
%     \item Goodfellow, I. J., Shlens, J., \& Szegedy, C. (2014). Explaining and harnessing adversarial examples. arXiv preprint arXiv:1412.6572.
%     \item Yuan, X., He, P., Zhu, Q., \& Li, X. (2019). Adversarial examples: Attacks and defenses for deep learning. IEEE transactions on neural networks and learning systems, 30(9), 2805-2824.
% \end{itemize}
% \end{vbframe}


\endlecture
\end{document}
