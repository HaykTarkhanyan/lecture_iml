\documentclass[11pt,compress,t,notes=noshow, aspectratio=169, xcolor=table]{beamer}

\usepackage{../../style/lmu-lecture}
% Defines macros and environments
\input{../../style/common.tex}

\title{Interpretable Machine Learning}
% \author{LMU}
%\institute{\href{https://compstat-lmu.github.io/lecture_iml/}{compstat-lmu.github.io/lecture\_iml}}
\date{}

\begin{document}

\newcommand{\titlefigure}{figure/whitebox}
\newcommand{\learninggoals}{
\item Examples of popular interpretable models
\item Properties of some interpretable models
\item How can we interpret them?}

\lecturechapter{Overview of Interpretable Models}
\lecture{Interpretable Machine Learning}




\begin{frame}[c]{Linear Regression - Characteristics}

%\textbf{Model formula}
    %$$\mathbb{E}_Y(Y \vert X) = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \ldots + \beta_p x_p + \epsilon = X^T\mathbf{\beta} + \epsilon$$
\begin{align*}
y &= \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \dots + \beta_p x_p + \epsilon \\
 &= X^T\beta + \epsilon
\end{align*}

    \begin{itemize}
        %\item $\mathbb{E}_Y(Y \vert X)$ expected value of target given features $X$
        %\item $y$ target / output
        \item $\beta_j$: weight of input feature $x_j$ $\leadsto$ model consists of $p+1$ weights $\beta_j$ (including intercept $\beta_0$)
        %\item $\epsilon$ remaining error (e.g., because of noise)
        %\item Model equation is additive and identical across entire input space
        %\pause
        \item Polynomial regression extends equation above by \textbf{higher order main effects} which have their own weights (e.g., $\beta_{x_j^2} \cdot x_j^2$) and \textbf{interaction effects} (e.g., 2-way interaction: $\beta_{x_i, x_j} \cdot x_i \cdot x_j$)
    \end{itemize}
   \vspace*{0.2cm} 
   \pause
    \textbf{Model assumptions}
    \begin{itemize}
    \item Predictions are a \textbf{linear} combination of features - model equation is additive% and identical across entire input space
    \item Error term and target outcome given the features are \textbf{normally} distributed, i.e.,\\
    \centerline{$\epsilon \sim N(0, \sigma^2) \; \Rightarrow \; (y \vert x) \sim N(x^T \beta, \sigma^2)$ with \textbf{constant variance} (homoscedastic)}
    $\leadsto$ if violated, inference-based metrics (t-statistic, p-values, confidence intervals) are invalid
    %\item Error terms are assumed to have a \textbf{constant variance} over the entire feature space %(homoscedasticity)
    \item Further assumptions: Observations are independent of each other (i.e., no repeated measurements), features are fixed (free of measurement errors), no multicollinearity
    % free of measurement error assumption: https://indigo.uic.edu/articles/thesis/Measurement_Error_in_Generalized_Linear_Models/17025971/1/files/31488719.pdf
    % \item Note: For inference-based metrics (t-statistic, p-values, confidence intervals) to be valid, the error term needs to be normally distributed, i.e., $\epsilon \sim N(0, \sigma^2) \; \Rightarrow \; (y \vert x) \sim N(x^T \beta, \sigma^2)$\\
%$\leadsto$ Restricts use of LMs in practice as distribution of error is a prior assumption about data
        % \item Properties and assumptions:
        % \begin{itemize}
        %     \item linear
        %     \item normality assumption of the target % not true...
        %     \item homoscedastic (i.e., constant variance)
        %     \item independence of features
        %     \item fixed features (i.e., free of noise)
        %     \item no strong correlation of features
        % \end{itemize} 
    \end{itemize}

\end{frame}

%------------------------------------------------------------------
%------------------------------------------------------------------

\begin{frame}[c]{Linear Regression - Interpretation}

\begin{align*}
y &= \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \dots + \beta_p x_p + \epsilon \\
 &= X^T\beta + \epsilon
\end{align*}

    Interpretation of weights (\textbf{feature effects}) depend on type of feature:
    \begin{itemize}
        \item Numerical $x_j$: Increasing feature value of $x_j$ by one unit changes predicted outcome by $\beta_j$ c.p.
        \pause
        \item Binary $x_j$: Weight $\beta_j$ is active or not (multiplication with 1 or 0) where 0 is reference category
        \item %Categorical: One-hot-encoding of $L-1$ new features for $L$ categories (dummy encoding) \\
        Categorical $x_j$ with $L$ categories: Create one-hot-encoded features $x_{j,1}, \hdots, x_{j,L-1}$ for $L-1$ categories (each having its own weight), left out category is reference ($\hat =$ dummy encoding)\\
        $\leadsto$ Interpretation:
        For category $l$, prediction changes by $\beta_{j,l}$ compared to reference cat.,  c.p.\pause
        % (for any of the $L-1$ categories):
        %Predicted outcome changes for $l$-th category compared to the reference category by its weight $\beta_{j,l}$ c.p.\pause
        %Predicted outcome changes for category $x_{j,l}$ compared to the reference category by $\beta_j$ c.p.\pause
        \item Intercept $\beta_0$: Expected baseline prediction if all features are set to 0 %reflects expected features values if features were standardised (0-mean, 1-stdev)
        \item Note: For higher order or interaction effects, weights cannot be interpreted in isolation
    \end{itemize}	
    \pause
    \textbf{Feature importance}:
    \begin{itemize}
        \item Absolute t-statistic value: Estimated weight scaled with its standard error (how certain are we about correct value) -- high absolute t-values indicate important (i.e. significant) features
    \end{itemize}
    $$t_{\hat\beta_j} = \left| \tfrac{\hat\beta_j}{SE(\hat\beta_j)} \right|$$
\end{frame}

%------------------------------------------------------------------
%------------------------------------------------------------------

% \begin{frame}{Linear and Polynomial Regression}

% \begin{align*}
% \mathbb{E}_Y(Y \vert X) &= \beta_0 + \beta_1 x_1 + \dots + \beta_p x_p + \epsilon \\
%  &= X^T\beta + \mathcal{E}
% \end{align*}

% \begin{itemize}
% \itemsep1em
% \item Model equation is identical across the entire feature space.
% %\item The predictive power of LMs is determined by specifying the correct model structure.
% \item Polynomial regression extends the LM by non-linear effects.
% %A polynomial regression model is an extension of the LM that includes higher order terms or interactions.
% %This enables us to model non-linear data while making use of the entire arsenal of LM functionality.
% \item We can exactly determine feature effects (e.g., beta coefficients, effect plots) and importance scores (e.g., p-values, t-statistics).
% %By knowing the model equation, we can exactly determine feature effects (e.g., beta coefficients, effect plots) and importance scores (e.g., p-values, t-statistics).
% \item For higher order effects or interactions, beta coefficients cannot be interpreted in isolation.
% \item Note: For inference-based metrics (p-values, confidence intervals) to be valid, error term needs to be normally distributed with zero mean, i.e., $\epsilon \sim N(0, \sigma^2) \; \Rightarrow \; (y \vert x) \sim N(x^T \beta, \sigma^2)$.\\
% $\leadsto$ Restricts use of LMs in practice as distribution of error is a prior assumption about data.
% \end{itemize}
% \end{frame}

\begin{frame}{Linear Regression - Example: Main Effects}

\textbf{Bike data}: predict number of rented bikes using 4 numeric and 1 categorical feature (season)
% \begin{footnotesize}
% $$
% \hat y = \hat \beta_0 + \hat \beta_1 \mathbbm{1}_{(seas = SPRING)} + \hat \beta_2 \mathbbm{1}_{(seas = SUMMER)} + \hat \beta_3 \mathbbm{1}_{(seas = FALL)} + \hat \beta_4 temp + \hat \beta_5 hum + \hat \beta_6 windspeed + \hat \beta_7 days\_since\_2011
% $$
% \end{footnotesize}
\begin{columns}[T, totalwidth=\linewidth]
\begin{column}{0.52\textwidth}
%\vspace*{-0.3cm}
\begin{align*}
\hat y = 
& \hat \beta_0 + 
\hat \beta_1 \mathbbm{1}_{x_{season} = SPRING} +
\hat \beta_2 \mathbbm{1}_{x_{season} = SUMMER} +\\
& 
\hat \beta_3 \mathbbm{1}_{x_{season} = FALL} + 
\hat \beta_4 x_{temp} + 
\hat \beta_5 x_{hum} + \\
& 
\hat \beta_6 x_{windspeed} + 
\hat \beta_7 x_{days\_since\_2011}
\end{align*}
\end{column}
\begin{column}{0.47\textwidth}
  \centering
\begin{tiny}
\begin{table}[ht]
\begin{tabular}{rrrrr}
  \hline
 & Weights & Std. Error & t value & Pr($>$$|$t$|$) \\

  \hline
(Intercept) & 3229.3 & 220.6 & 14.6 & 0.00 \\ 
  seasonSPRING & 862.0 & 129.0 & 6.7 & 0.00 \\ 
  seasonSUMMER & 41.6 & 170.2 & 0.2 & 0.81 \\ 
  seasonFALL & 390.1 & 116.6 & 3.3 & 0.00 \\ 
  temp & 120.5 & 7.3 & 16.5 & 0.00 \\ 
  hum & -31.1 & 2.6 & -12.1 & 0.00 \\ 
  windspeed & -56.9 & 7.1 & -8.0 & 0.00 \\ 
  days\_since\_2011 & 4.9 & 0.2 & 26.9 & 0.00 \\
   \hline
\end{tabular}
\end{table}
\end{tiny}
\end{column}
\end{columns}
%\vspace*{-0.3cm}
\pause

\begin{columns}[T, totalwidth=\linewidth]
\begin{column}{0.47\textwidth}

\includegraphics[width = \textwidth]{figure/plot_lin_effect.pdf}
  \begin{center}
   % Effect of $i$-th observation $= \beta_j x_j^{(i)}$\\
    Weights multiplied by actual feature value are better comparable due to different scales
    %$\leadsto$ Better comparability due to different scales
  \end{center}
%\pause
%\verbatiminput{figure/lm_output.txt}
\end{column}\hfill
\begin{column}{0.52\textwidth}  %%<--- here

\begin{itemize}
    \item \textbf{Interpretation categorical}: Number of bike rentals in spring increases by 862 compared to winter (which is reference category) c.p.
    \item \textbf{Interpretation numerical}: If the temperature increases by 1 degree Celsius, the number of bike rentals increases by 120.5 c.p.
    \item \textbf{Interpretation intercept}:
    If all feature values are 0 (i.e., season is winter), the expected number of bike rentals is 3229.3
\end{itemize}
\end{column}
\end{columns}
\end{frame}


\begin{frame}{Linear Regression - Example: Including Interactions}
\textbf{Update}: Interaction between temperature and season affects marginal effect of temperature% with (right) and without (left) interaction.
\begin{columns}[T]
\begin{column}{0.25\textwidth}
\begin{tiny}
\begin{table}[ht]
\centering
\begin{tabular}{rr}
  \hline
 & Weights \\ 
  \hline
(Intercept) & 3453.9 \\ 
  seasonSPRING & 1317.0 \\ 
  seasonSUMMER & 4894.1 \\ 
  seasonFALL & -114.2 \\ 
  temp & 160.5 \\ 
  hum & -37.6 \\ 
  windspeed & -61.9 \\ 
  days\_since\_2011 & 4.9 \\
  \hline
  seasonSPRING:temp & -50.7 \\ 
  seasonSUMMER:temp & -222.0 \\ 
  seasonFALL:temp & 27.2 \\ 
   \hline
\end{tabular}
\end{table}
\end{tiny}
\end{column}
\begin{column}{0.7\textwidth}
\includegraphics[width = \textwidth]{figure/lm_main_vs_interaction_effects.pdf}
\end{column}
\end{columns}
\vfill
\pause
\textbf{Interpretation}: If \code{temp} increases by 1 $^{\circ}$C, the number of bike rentals increases by 160.5 in winter, by 109.8 (= 160.5 - 50.7) in spring, and decreases by -61.5 (= 160.5 - 222) in summer.\\\vspace*{0.2cm}
\textbf{Note:} Temperature ranges (on x-axis) depend on season $\leadsto$ not acknowledged by LM.
%Value ranges of temperature differ depending on the season $\leadsto$ not acknowledged by LM
\end{frame}


\begin{frame}{Linear Regression - Example: Including Polynomials}
\textbf{Update}: Add 2nd order polynomial for \code{temp} (left) and interaction with \code{season} (right).
\begin{columns}[T]
\begin{column}{0.25\textwidth}
\begin{tiny}
\begin{table}[ht]
\centering
\begin{tabular}{rr}
  \hline
 & Weights \\ 
  \hline
(Intercept) & 3094.1 \\ 
  seasonSPRING & 619.2 \\ 
  seasonSUMMER & 284.6 \\ 
  seasonFALL & 123.1 \\ 
  hum & -36.4 \\ 
  windspeed & -65.7 \\ 
  days\_since\_2011 & 4.7 \\ 
  poly(temp, 2)1 & 280.2 \\ 
  poly(temp, 2)2 & -5.6 \\ 
   \hline
\end{tabular}
\end{table}
   Table: Weights for main effect model

\end{tiny}

\end{column}
\begin{column}{0.7\textwidth}
\includegraphics[width = \textwidth]{figure/poly_main_vs_interaction_effects.pdf}
\end{column}
\end{columns}
\vfill
\pause
\textbf{Interpretation}: Not linear anymore!\\ 
$\leadsto$ Effect size depends on temperature value: $280.2 \cdot x_{temp} - 5.6 \cdot x_{temp}^2$\\
$\leadsto$ BUT: Needs to be manually specified!\\
$\leadsto$ The more non-linear and interaction effects are added, the less interpretable becomes the model
\end{frame}


\begin{frame}{Sparse Linear Regression - LASSO \citebutton{Tibshirani (1996)}{https://doi.org/10.1111/j.2517-6161.1996.tb02080.x}}
\begin{itemize}
    \item Sparser models (with fewer features) are more interpretable
    \item LASSO adds an L1-norm penalization term  ($\lambda||\beta||_1$) to the optimization problem
    \item Aim: Feature selection (sparsity) and regularization of feature weights
    \item Penalization parameter $\lambda$ must be chosen (e.g., by CV) %or depending on the number of features to keep
\end{itemize}
$$
min_{\beta} \bigg(\underbrace{\frac{1}{n} \sum_{i=1}^{n} (y^{(i)} - x^{(i)\top}\beta)^2}_\text{Least square estimate for LM} + \lambda||\beta||_1\bigg)
$$
%\vspace*{0.2cm}\\
%\vspace*{-0.4cm}
\pause
\begin{columns}[T, totalwidth=\linewidth]
\begin{column}{0.65\textwidth}
\textbf{Example}
\begin{itemize}
    \item Penalize model with main effects and interaction between \code{temp} and \code{season}
    \item $\lambda$ is chosen such that 6 features are selected (not zero)
    \item May be problematic for categorical features as only weights of single categories (due to dummy encoding) are selected out instead of the whole categorical feature\\ %, polynomials and interactions
    $\leadsto$ group LASSO \lit{Yuan and Lin (2006)}{https://doi.org/10.1111/j.1467-9868.2005.00532.x}
\end{itemize}
\end{column}
\begin{column}{0.35\textwidth}
\tiny
\begin{table}[ht]
\centering
\begin{tabular}{rr}
  \hline
 & Weights \\ 
  \hline
(Intercept) & 3135.2 \\ 
  seasonSPRING & 767.4 \\ 
  seasonSUMMER & 0.0 \\ 
  seasonFALL & 0.0 \\ 
  temp & 116.7 \\ 
  hum & -28.9 \\ 
  windspeed & -50.5 \\ 
  days\_since\_2011 & 4.8 \\ 
  seasonSPRING:temp & 0.0 \\ 
  seasonSUMMER:temp & 0.0 \\ 
  seasonFALL:temp & 30.2 \\ 
   \hline
\end{tabular}
\end{table}
% \begin{table}[ht]
% \centering
% \begin{tabular}{rr}
%   \hline
%  & Weights \\ 
%   \hline
% (Intercept) & 2665.50 \\ 
%   seasonSPRING & 489.34 \\ 
%   seasonSUMMER & 0.00 \\ 
%   seasonFALL & 0.00 \\ 
%   hum & -19.44 \\ 
%   windspeed & -35.54 \\ 
%   days\_since\_2011 & 4.71 \\ 
%   poly(temp, 2)1 & 109.25 \\ 
%   poly(temp, 2)2 & 0.00 \\ 
%   \hline
% \end{tabular}
% \end{table}
\end{column}
\end{columns}

\end{frame}
%------------------------------------------------------------------
%------------------------------------------------------------------
\begin{frame}{Generalized Linear Model (GLM) - Characteristics}

\textbf{Problem}: Target variable given the features not always normally distributed $\leadsto$ LM not suitable
\begin{itemize}
    \item Target is categorical (e.g., disease classification)
    \item Target is count variable (e.g., number of sold products)
    \item Time until an event occurs (e.g., time until death)
\end{itemize}
\medskip
\pause
\textbf{Solution}: GLMs - extend LMs by allowing other distributions from exponential family
$$g(\mathrm{E}_Y (y\mid x)) = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \ldots + \beta_p x_p$$
\vspace*{-0.4cm}
    \begin{itemize}
        \item Link function $g$ links weighted sum of features (linear predictor) to specified distribution
        \item Link function and probability distribution need to be specified by the user
        \item LM is a special case by choosing the identity function for $g$ and the Gaussian distribution
        \item Interpretation depends on specified link function and distribution
        \item Interaction and polynomial effects can be manually added in the same way as in LMs
    \end{itemize}
    
 %   \medskip
 %   Non-Gaussian outputs via Generalized Linear Models (GLMs):
    
  %  \begin{itemize}
   %     \item link function $g$ -- can be freely chosen
    %    \item exponential family defining $\mathrm{E}_Y$ -- can be freely chosen
     %   \item weighted sum $X^\top W$
    %\end{itemize}
    
    %\medskip 
    %\pause
    %Interaction effects via feature engineering:
    %\begin{itemize}
    %    \item E.g., feature expansion: $\beta_{x_i,x_j} x_i \cdot x_j$
    %\end{itemize}
    
\end{frame}
 	
\begin{frame}{Logistic Regression - Characteristics}

\begin{itemize}
    \item Logistic regression models probabilities (between 0 and 1) for binary classification tasks 
    \item GLM with Bernoulli distribution and logit link function
    \item Logistic regression formula:
    $$P(y = 1) =\frac{1}{1 + \exp(-( \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \ldots + \beta_p x_p ))} $$
    \item Typically, we set the threshold to $0.5$ to predict classes, e.g.,
        \begin{itemize}
            \item Class 1 if $P(y=1) > 0.5$
            \item Class 0 if $P(y=1) \leq 0.5$
        \end{itemize}
\end{itemize}
    

	

\end{frame}

%------------------------------------------------------------------
%------------------------------------------------------------------

\begin{frame}[c]{Logistic Regression - Interpretation}

    
    
    
    

    \begin{itemize}
        \item Weights relate to formula in log odds, effects are again linear (but w.r.t. log odds)
        $$log\text{ }odds = \log \left(\frac{P(y = 1)}{P(y=0)}\right) = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \ldots + \beta_p x_p  $$
        \pause
        \item Odds can be caulculated by 
        $$odds = \left(\frac{P(y = 1)}{P(y=0)}\right) = \exp(\beta_0 + \beta_1 x_1 + \beta_2 x_2 + \ldots + \beta_p x_p)  $$
        \item[$\leadsto$] Changing $x_j$ by one unit, changes the odds ratio by a \alert{factor} of $\exp(\beta_J)$
        $$odds\text{ }ratio = \frac{odds_{x_j+1}}{odds} = \frac{\exp(\beta_0 + \beta_1 x_1 + \ldots + \beta_j (x_j+1) + \ldots + \beta_p x_p)}{\exp(\beta_0 + \beta_1 x_1 + \ldots + \beta_j x_j + \ldots + \beta_p x_p)} = \exp{(\beta_j)} $$
        \pause
        \item Interpretation for different feature types is the same as for linear regression (however, linear interpretation only possible for log odds $\leadsto$ difficult to comprehend)
    \end{itemize}	

\end{frame}

\begin{frame}{Logistic Regression - Example}

\begin{itemize}
    \item Create a binary target variable for bike rental data (Class 1: high number of bike rentals - more than the 70\% quantile - and Class 0: low to medium number of bike rentals)
    \item Fit a logistic regression with Bernoulli distribution and logit link
\end{itemize}

\begin{columns}[T]
\begin{column}{0.45\textwidth}
\begin{table}[ht]
\centering
\tiny
\begin{tabular}{rrrrr}
  \hline
 & Weights & Std. Error & z value & Pr($>$$|$z$|$) \\ 
  \hline
(Intercept) & -8.5 & 1.2 & -7.1 & 0.00 \\ 
  seasonSPRING & 1.7 & 0.6 & 2.9 & 0.00 \\ 
  seasonSUMMER & -0.9 & 0.8 & -1.1 & 0.26 \\ 
  seasonFALL & -0.6 & 0.6 & -1.2 & 0.25 \\ 
  temp & 0.3 & 0.0 & 7.4 & 0.00 \\ 
  hum & -0.1 & 0.0 & -5.0 & 0.00 \\ 
  windspeed & -0.1 & 0.0 & -3.0 & 0.00 \\ 
  days\_since\_2011 & 0.0 & 0.0 & 11.6 & 0.00 \\ 
   \hline
\end{tabular}
\end{table}
\pause
\textbf{Interpretation}
\begin{itemize}
    %\item If the temperature increases by 1 degree Celsius then the log odds of high number of bike rentals increase linearly by 0.3 c.p.
    \item If temperature increases by 1 degree Celsius then the odds for high number of bike rentals increase by the factor $\exp (0.3) = 1.3$ c.p.
\end{itemize}
\end{column}
\hfill
\pause
\begin{column}{0.5\textwidth}
\includegraphics[width = \textwidth]{figure/logistic_maginal_temp.pdf}
\end{column}
\end{columns}

\end{frame}
%------------------------------------------------------------------
%------------------------------------------------------------------


%------------------------------------------------------------------
%------------------------------------------------------------------

\begin{frame}{Generalized Additive Models (GAMs) - Characteristics}

\textbf{Problem}: Relationship between features and target variable not linear $\leadsto$ standard LM not suitable 

\medskip
\pause
\textbf{Solution}: 
 \begin{itemize}
        \item Feature transformations (e.g., exp or log)
        \item Categorization of features (i.e., intervals / buckets of feature values)
        \item GAMs:
        $$g(\mathrm{E}_Y (y\mid x)) = \beta_0 + f_1(x_1) + f_2(x_2) + \ldots + f_p(x_p)$$
    
    \begin{itemize}
        \item Instead of $\beta_j x_j$ use flexible functions $f_j(x_j)$ $\leadsto$ splines
        \item Preserves additive structure of feature effects but does also allow nonlinear effects
        \item Smoothness parameter to control flexibility (prevent overfitting) needs to be tuned
    \end{itemize}
    \end{itemize}
    
    

   

\end{frame}


\begin{frame}{Generalized Additive Models (GAMs) - Example}
    \begin{itemize}
    \item Fit a GAM with smooth splines for four numeric features of bike rental data 
\end{itemize}

\begin{columns}[T]
\begin{column}{0.45\textwidth}
\begin{table}[ht]
\centering
\tiny
\begin{tabular}{rrrrr}
  \hline
 & edf & Ref.df & F & p-value \\ 
  \hline
s(temp) & 5.8 & 7.0 & 57.2 & 0.00 \\ 
  s(hum) & 4.0 & 5.1 & 68.0 & 0.00 \\ 
  s(windspeed) & 1.7 & 2.1 & 50.1 & 0.00 \\ 
  s(days\_since\_2011) & 8.3 & 8.8 & 154.4 & 0.00 \\ 
   \hline
\end{tabular}
\end{table}


\textbf{Interpretation}
\begin{itemize}
    %\item If the temperature increases by 1 degree Celsius then the log odds of high number of bike rentals increase linearly by 0.3 c.p.
    \item Edf (effective degrees of freedom) represents complexity of the smooth
    \item Interpretation needs to be done visually and relative to average prediction\\
    $\leadsto$ more flexibe and better model fit but less interpretable than single weights
\end{itemize}
\end{column}
\hfill
\begin{column}{0.5\textwidth}
\includegraphics[width = \textwidth]{figure/gam_effects.pdf}
\end{column}
\end{columns}
\end{frame}
%------------------------------------------------------------------
%------------------------------------------------------------------

\begin{frame}{Decision Trees - Characteristics \& Interpretation}

\textbf{Problem}: Relationship between features and target are nonlinear or feature interactions are present\\
\medskip
\pause
\textbf{Solution}: Decision tree \lit{Breiman et al. (1984)}{https://doi.org/10.1201/9781315139470} -- Split data in different subsets depending on cutoff values in features 
$$
\hat f(x) = \sum_{m=1}^M c_m \mathbbm{1}_{\{x \in \mathcal{R}_m\}}\text{ ,  where $c_m$ is a constant and $\mathcal{R}_m$ the $m$-th leaf node of the tree}
$$

\begin{itemize}
    \item Finding best split point (CART): Greedy search for the point that minimizes the variance of $y$ (regression) or the Gini index (classification)
    \item Able to handle mixed feature spaces and missing values
\end{itemize}
\medskip
\pause
\textbf{Interpretation}
\begin{itemize}
    \item Directly by following the tree structure (i.e., sequence of decision rules)
    \item Feature importance by (scaled) score of how much splitting criterion (e.g. variance) is reduced compared to a parent node
\end{itemize}



\end{frame}

\begin{frame}{Decision Trees - Example}
\begin{itemize}
    \item Fit decision tree with tree depth of 3 on bike data
    \item E.g., mean prediction for the first 105 days since 2011 is 1798
    \item Feature \code{days\_since\_2011} shows highest feature importance (explains most of variance)
\end{itemize}
\begin{columns}[T]
\begin{column}{0.4\textwidth}
\begin{table}[ht]
\centering
\begin{tabular}{lr}
  \hline
Feature & Importance \\ 
  \hline
days\_since\_2011 & 68.03 \\ 
  temp & 20.54 \\ 
  season & 6.56 \\ 
  hum & 3.58 \\ 
  windspeed & 1.29 \\ 
   \hline
\end{tabular}
\end{table}
\end{column}
\begin{column}{0.6\textwidth}
  \includegraphics[width = \textwidth]{figure/tree.pdf} 
\end{column}
\end{columns}
 
\end{frame}
%------------------------------------------------------------------
%------------------------------------------------------------------

%\begin{frame}[c]{Decision Rules}

%\texttt{IF COND$_1$ AND COND$_2$ AND ... THEN value}

%\begin{itemize}
%    \item \texttt{COND$_i$} can be of the form \texttt{feature <op> value} where \texttt{<op>} can be for example $\{=, <, > \}$
%\end{itemize}

%\pause
%\medskip

%Properties:
%\begin{description}
%    \item{Support} Fraction of observations to support appliance of rule
%    \item{Accuracy} for predicting the correct class under the condition(s)
%\end{description}

%$\leadsto$ often trade-off between these two

%\pause
%\medskip

%$\leadsto$ many different ways to learn a set of rules (incl. a default rule if none of the rules are met)

%\end{frame}

%------------------------------------------------------------------
%------------------------------------------------------------------

\begin{frame}{Componentwise Gradient Boosting - Characteristics}

\begin{itemize}
%\setlength\itemsep{2em}
\item<1-> Recall: Boosting iteratively combines weak base learners to create a powerful ensemble model
\item<1->
Idea: Boosting with gradient descent using interpretable base learners (e.g., use base learners with single features in each iteration $\leadsto$ coordinate gradient descent)
%The resulting ensemble is also interpretable.
%\pause
\item<2->
Two linear base learners $b_j(x, \theta)$ and $b_j(x, \theta^{\star})$ of the same type, but distinct parameter vectors $\theta$ and $\theta^{\star}$ can be combined in a base learner of the same type:
$$b_j(x, \theta) + b_j(x, \theta^{\star}) = b_j(x, \theta + \theta^{\star})$$
%\pause
\item<3-> In each iteration, a set of base learners is fitted on pseudo residuals. The one with the best fit is added to the previously computed model:
\medskip
\begin{align*}
\widehat{f}^{[1]}(x) &= \hat{f}_0 + \nu \textcolor{blue}{b_3(x_3, \theta^{[1]})} \\
\widehat{f}^{[2]}(x) &= \widehat{f}^{[1]}(x) + \nu \textcolor{blue}{b_3(x_3, \theta^{[2]})} 
%= \hat{f}_0 + \nu \textcolor{blue}{b_3(x_3, \theta^{[1]})} + \nu \textcolor{blue}{b_3(x_3, \theta^{[2]})}
\\
\widehat{f}^{[3]}(x) &= \widehat{f}^{[2]}(x) + \nu \textcolor{orange}{b_1(x_1, \theta^{[3]})} 
%= \hat{f}_0 + \nu \textcolor{blue}{b_3(x_3, \theta^{[1]})} + \nu \textcolor{blue}{b_3(x_3, \theta^{[2]})} + \nu \textcolor{orange}{b_1(x_1, \theta^{[3]})} 
= \hat{f}_0 + \nu \left(\textcolor{blue}{b_3(x_3, \theta^{[1]} + \theta^{[2]})} + \textcolor{orange}{b_1(x_1, \theta^{[3]})}\right) 
\\
&= \hat{f}_0 + \textcolor{blue}{\hat{f}_3(x_3)} + \textcolor{orange}{\hat{f}_1(x_1)}
\end{align*}
\item<3-> Final model has additive structure (as GAMs), where each component function is interpretable

\end{itemize}
\end{frame}


\begin{frame}{Componentwise Gradient Boosting - Example}

Simplest case: Use linear models as base learners on single features
$$
b_j(x_j, \theta) = x_j\theta \hspace*{0.2cm}\text{ for } j = 1,\ldots p \hspace*{0.3cm} \leadsto \text{ordinary linear regression}
$$

\begin{itemize}
\item Converges to same solution as least square estimate of LMs
\item Specifying loss function and link function according to exponential family 
%leads to boosting which 
is equivalent to a (regularized) GLM
\item Early stopping allows feature selection

\end{itemize}
\pause
\begin{columns}[T]
\begin{column}{0.4\textwidth}
\scriptsize
  \begin{table}[ht]
\centering
\begin{tabular}{rlrr}
  \hline
 Feature & Intercept & Slope \\ 
  \hline
days\_since\_2011\_linear & -1188.81 & 3.23 \\ 
 season\_ridge & -291.24 & 165.61 \\ 
 temp\_linear & -1045.34 & 70.24 \\ 
 offset & 4452.18 &  \\ 
   \hline
\end{tabular}
\end{table}
\begin{itemize}
    \item Linear base learners for numeric features and categorical base learner for season
    \item 3 base learners selected after 100 iterations
\end{itemize}
\end{column}
\begin{column}{0.55\textwidth}
  \includegraphics[width = .95 \textwidth]{figure/compboost_base_linear.pdf} 
\end{column}
\end{columns}
\end{frame}







\begin{frame}{Componentwise Gradient Boosting - Interpretation}

\begin{itemize}
    \item Fit a model-based boosting model on bike rental data \lit{Daniel Schalk et al. 2018}{https://doi.org/10.21105/joss.00967}
    \item Define base learners for all features - linear and centered splines for numeric features and categorical base learner for season
\end{itemize}
\begin{columns}[T]
\pause
\begin{column}{0.5\textwidth}
 \includegraphics[width = 0.9\textwidth]{figure/compboost_pfi.pdf}
 \begin{itemize}
     \item Loss-based feature importance\\
     $\leadsto$ days\_since\_2011 is most important feature
 \end{itemize}
%\scriptsize
%\verbatiminput{figure/mboost_output.txt}
\end{column}
\pause
\begin{column}{0.5\textwidth}  %%<--- here
  \includegraphics[width = 0.9 \textwidth]{figure/compboost_pfe.pdf}
  \begin{itemize}
      \item Partial feature effect for days\_since\_2011\\
      $\leadsto$ Total partial effect can be decomposed into partial effects of linear and centered spline base learners 
  \end{itemize}
\end{column}
\end{columns}
\end{frame}



\begin{frame}[c]{Other Interpretable Models}

\textbf{RuleFit} \lit{Friedman and Popescu 2008}{https://arxiv.org/abs/0811.1679}
\begin{itemize}
    \item Combination of linear models and decision trees 
    \item Allows for feature interactions and non-linearities
\end{itemize}

\textbf{Decision Rules} \lit{Holte 1993}{https://doi.org/10.1023/A:1022631118932}
\begin{itemize}
    \item Simple ``if -- then'' statements - very intuitive and easy-to-interpret
    \item Common assumption: only classification tasks and categorical features
\end{itemize}

\textbf{NaiveBayes}
%$$P (C_k \mid x ) = \frac{1}{Z} P(C_k) \prod_{i=1}^{n} P(x_i \mid C_k) $$
\begin{itemize}
    \item Product of probabilities for a class on the value of each feature
    \item Strong independence assumption
\end{itemize}


\textbf{k-Nearest Neighbor}
\begin{itemize}
    \item (Closely related to case-based reasoning)
    \item Average of of the outcome of neighbors -- local explanation
\end{itemize}

\end{frame}


\endlecture
\end{document}