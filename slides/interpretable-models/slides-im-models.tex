\documentclass[11pt,compress,t,notes=noshow, aspectratio=169, xcolor=table]{beamer}

\usepackage{../../style/lmu-lecture}
% Defines macros and environments
\input{../../style/common.tex}

\title{Interpretable Machine Learning}
% \author{LMU}
%\institute{\href{https://compstat-lmu.github.io/lecture_iml/}{compstat-lmu.github.io/lecture\_iml}}
\date{}

\begin{document}

\newcommand{\titlefigure}{figure/whitebox}
\newcommand{\learninggoals}{
\item What characteristics does an interpretable model have?
\item Why should we use interpretable models at all?
\item Examples for interpretable models.}

\lecturechapter{Interpretable Models}
\lecture{Interpretable Machine Learning}




\begin{frame}[c]{Linear Regression}

    %$$\mathbb{E}_Y(Y \vert X) = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \ldots + \beta_p x_p + \epsilon = X^T\mathbf{\beta} + \epsilon$$
\begin{align*}
y &= \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \dots + \beta_p x_p + \epsilon \\
 &= X^T\beta + \epsilon
\end{align*}

    \begin{itemize}
        %\item $\mathbb{E}_Y(Y \vert X)$ expected value of target given features $X$
        \item $y$ target / output
        \item $\beta_i$ weight/coefficient of input feature $x_i$ $\leadsto$ model consists of $p+1$ weights $\beta_i$
        \item $\epsilon$ remaining error (e.g., because of noise)
        \item Model equation is additive and identical across entire input space.
        \pause
        \item Polynomial regression extends equation above by higher order effects which have their own weights (e.g., $\beta_{x_j^2} \cdot x_j^2$) and interaction effects (e.g., 2-way interaction: $\beta_{x_i, x_j} \cdot x_i \cdot x_j$).
        \item Note: For inference-based metrics (t-statistic, p-values, confidence intervals) to be valid, the error term needs to be normally distributed, i.e., $\epsilon \sim N(0, \sigma^2) \; \Rightarrow \; (y \vert x) \sim N(x^T \beta, \sigma^2)$.\\
$\leadsto$ Restricts use of LMs in practice as distribution of error is a prior assumption about data.
        % \item Properties and assumptions:
        % \begin{itemize}
        %     \item linear
        %     \item normality assumption of the target % not true...
        %     \item homoscedastic (i.e., constant variance)
        %     \item independence of features
        %     \item fixed features (i.e., free of noise)
        %     \item no strong correlation of features
        % \end{itemize} 
    \end{itemize}

\end{frame}

%------------------------------------------------------------------
%------------------------------------------------------------------

\begin{frame}[c]{Interpretation of Linear Regression}

\begin{align*}
y &= \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \dots + \beta_p x_p + \epsilon \\
 &= X^T\beta + \epsilon
\end{align*}

    Let's consider different feature types:
    \begin{itemize}
        \item Numerical: Increase of numerical value will lead to $\beta_i$ times increased output
        \item Binary: Either weight $\beta_i$ is active or not (multiplication with 1 or 0).
        \item Categorical: One-hot-encoding of $L-1$ new features for $L$ categories (dummy encoding)
        \item Intercept $\beta_0$: Expected baseline prediction if all features are set to 0. %reflects expected features values if features were standardised (0-mean, 1-stdev)
        \item Note: For higher order or interaction effects, coefficients cannot be interpreted in isolation.
    \end{itemize}	
    \pause
    Feature importance:
    \begin{itemize}
        \item t-statistic by the estimated weight scaled with its standard error\\ (i.e., less certain about the correct value)
    \end{itemize}
    $$t_{\beta_i} = \frac{\beta_i}{SE(\beta_i)} $$
\end{frame}

%------------------------------------------------------------------
%------------------------------------------------------------------

% \begin{frame}{Linear and Polynomial Regression}

% \begin{align*}
% \mathbb{E}_Y(Y \vert X) &= \beta_0 + \beta_1 x_1 + \dots + \beta_p x_p + \epsilon \\
%  &= X^T\beta + \mathcal{E}
% \end{align*}

% \begin{itemize}
% \itemsep1em
% \item Model equation is identical across the entire feature space.
% %\item The predictive power of LMs is determined by specifying the correct model structure.
% \item Polynomial regression extends the LM by non-linear effects.
% %A polynomial regression model is an extension of the LM that includes higher order terms or interactions.
% %This enables us to model non-linear data while making use of the entire arsenal of LM functionality.
% \item We can exactly determine feature effects (e.g., beta coefficients, effect plots) and importance scores (e.g., p-values, t-statistics).
% %By knowing the model equation, we can exactly determine feature effects (e.g., beta coefficients, effect plots) and importance scores (e.g., p-values, t-statistics).
% \item For higher order effects or interactions, beta coefficients cannot be interpreted in isolation.
% \item Note: For inference-based metrics (p-values, confidence intervals) to be valid, error term needs to be normally distributed with zero mean, i.e., $\epsilon \sim N(0, \sigma^2) \; \Rightarrow \; (y \vert x) \sim N(x^T \beta, \sigma^2)$.\\
% $\leadsto$ Restricts use of LMs in practice as distribution of error is a prior assumption about data.
% \end{itemize}
% \end{frame}

\begin{frame}{Linear Regression}

\begin{columns}[T]
\begin{column}{0.55\textwidth}
\tiny
\verbatiminput{figure/lm_output.txt}
\end{column}
\begin{column}{0.45\textwidth}  %%<--- here
  \includegraphics[width = \textwidth]{figure/lm_effect_plot.png}
\end{column}
\end{columns}
\end{frame}

%------------------------------------------------------------------
%------------------------------------------------------------------
	
\begin{frame}{Logistic Regression}

    $$P(y = 1) =\frac{1}{1 + \exp(-( \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \ldots + \beta_p x_p ))} $$

    \begin{itemize}
        \item Probabilistic \alert{classification} model relates
        \item Typically, we set the threshold to $0.5$ to predict 
        \begin{itemize}
            \item Class 1 if $P(y=1) > 0.5$
            \item Class 0 if $P(y=1) \leq 0.5$
        \end{itemize}
    \end{itemize}	

\end{frame}

%------------------------------------------------------------------
%------------------------------------------------------------------

\begin{frame}[c]{Interpretation of Logistic Regression}

    $$\log \left(\frac{P(y = 1)}{P(y=0)}\right) = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \ldots + \beta_p x_p  $$

    \begin{itemize}
        \item weights relate to log odds-ratio
        \item Again linear in log odds-ratio
        \item[$\leadsto$] change by one unit changes the odds ratio by a \alert{factor} of $\exp(\beta_i)$.
        \medskip
        \pause
        \item Interpretation for different feature types is the same as for linear regression
    \end{itemize}	

\end{frame}

%------------------------------------------------------------------
%------------------------------------------------------------------

\begin{frame}{GLM and Interactions}

    \begin{itemize}
        \item Linear models are often too restrictive for many applications
    \end{itemize}
    
    \medskip
    Non-Gaussian outputs via Generalized Linear Models (GLMs):
    $$g(\mathrm{E}_Y (y\mid x)) = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \ldots + \beta_p x_p$$
    \begin{itemize}
        \item link function $g$ -- can be freely chosen
        \item exponential family defining $\mathrm{E}_Y$ -- can be freely chosen
        \item weighted sum $X^\top W$
    \end{itemize}
    
    \medskip 
    \pause
    Interaction effects via feature engineering:
    \begin{itemize}
        \item E.g., feature expansion: $\beta_{x_i,x_j} x_i \cdot x_j$
    \end{itemize}
    
\end{frame}

%------------------------------------------------------------------
%------------------------------------------------------------------

\begin{frame}{Generalized Additive Models (GAMs)}

Non-Linear relations can be addressed by:
    \begin{itemize}
        \item feature transformations (e.g., exp or log)
        \item Categorization of features (i.e., intervals / buckets of feature values)
        \item GAMs:
    \end{itemize}
    
    $$g(\mathrm{E}_Y (y\mid x)) = \beta_0 + f_1(x_1) + f_2(x_2) + \ldots + f_p(x_p)$$
    
    \begin{itemize}
        \item instead of $\beta_i x_i$ use flexible functions $f_i(x_i)$ $\leadsto$ splines
    \end{itemize}

\end{frame}

%------------------------------------------------------------------
%------------------------------------------------------------------

\begin{frame}{Decision Trees}

\begin{columns}

\begin{column}{0.3\textwidth}

   \begin{tikzpicture}
   \usetikzlibrary{arrows}
    \usetikzlibrary{shapes}
     \tikzset{treenode/.style={draw, circle, font=\small}}
     \tikzset{line/.style={draw, thick}}
     \node [treenode, draw=red] (a0) {$a_0$};
     \node [treenode, below=0.75cm of a0, xshift=-1cm]  (a1) {$a_1$};
     \node [treenode, draw=red, below=0.75cm of a0, xshift=1cm]  (a2) {$a_2$};
     
     \node [treenode, draw=red, below=0.75cm of a2, xshift=-1cm] (a3) {$a_3$};
     \node [treenode, below=0.75cm of a2, xshift=1cm]  (a4) {$a_4$};
     
     \node [treenode, below=0.75cm of a3, xshift=-1cm] (a5) {$a_5$};
     \node [treenode, below=0.75cm of a3, xshift=1cm]  (a6) {$a_6$};
     
     \path [line] (a0.south) -- + (0,-0.4cm) -| (a1.north) node [midway, above] {$x_1<0.3$};
     \path [line] (a0.south) -- +(0,-0.4cm) -|  (a2.north) node [midway, above] {$x_1\geq0.3$};
     
     \path [line] (a2.south) -- + (0,-0.4cm) -| (a3.north) node [midway, above] {$x_1<0.6$};;
     \path [line] (a2.south) -- +(0,-0.4cm) -|  (a4.north) node [midway, above] {$x_1\geq0.6$};
     
          
     \path [line] (a3.south) -- + (0,-0.4cm) -| (a5.north) node [midway, above] {$x_2<0.2$};;
     \path [line] (a3.south) -- +(0,-0.4cm) -|  (a6.north) node [midway, above] {$x_2\geq0.2$};
     
   \end{tikzpicture}

\end{column}

\begin{column}{0.7\textwidth}

Properties:
\begin{itemize}
    \item able to model non-linear effects
    \item terminal nodes (aka leaf nodes) can have several observations and predicts the mean outcome over these
    \item Applicable to regression and classification
\end{itemize}

Interpretation:
\begin{itemize}
    \item directly by following the tree (i.e., sequence of rules)
    \item Feature importance by (scaled) score of much the splitting criterion was reduced compared to the parent
\end{itemize}

\end{column}

\end{columns}

\end{frame}

%------------------------------------------------------------------
%------------------------------------------------------------------

\begin{frame}[c]{Decision Rules}

\texttt{IF COND$_1$ AND COND$_2$ AND ... THEN value}

\begin{itemize}
    \item \texttt{COND$_i$} can be of the form \texttt{feature <op> value} where \texttt{<op>} can be for example $\{=, <, > \}$
\end{itemize}

\pause
\medskip

Properties:
\begin{description}
    \item{Support} Fraction of observations to support appliance of rule
    \item{Accuracy} for predicting the correct class under the condition(s)
\end{description}

$\leadsto$ often trade-off between these two

\pause
\medskip

$\leadsto$ many different ways to learn a set of rules (incl. a default rule if none of the rules are met)

\end{frame}

%------------------------------------------------------------------
%------------------------------------------------------------------

\begin{frame}[c]{Other Interpretable Models}

\textbf{RuleFit} \lit{Friedman and Popescu 2008}{https://arxiv.org/abs/0811.1679}
\begin{itemize}
    \item Combination of linear models and decision trees 
    \item Allows for feature interactions and non-linearities
\end{itemize}

\textbf{NaiveBayes}
$$P (C_k \mid x ) = \frac{1}{Z} P(C_k) \prod_{i=1}^{n} P(x_i \mid C_k) $$
\begin{itemize}
    \item product of probabilities for a class on the value of each feature
    \item strong independence assumption
\end{itemize}


\textbf{k-Nearest Neighbor}
\begin{itemize}
    \item (closely related to case-based reasoning)
    \item Average of of the outcome of neighbors -- local explanation
\end{itemize}

\end{frame}


\endlecture
\end{document}