\documentclass[11pt,compress,t,notes=noshow, aspectratio=169, xcolor=table]{beamer}

\usepackage{../../style/lmu-lecture}
% Defines macros and environments
\input{../../style/common.tex}

\title{Interpretable Machine Learning}
% \author{LMU}
%\institute{\href{https://compstat-lmu.github.io/lecture_iml/}{compstat-lmu.github.io/lecture\_iml}}
\date{}

\def\firstrowcolor{}
\def\secondrowcolor{}
\def\thirdrowcolor{}
\def\fourthrowcolor{}

\begin{document}

\newcommand{\titlefigure}{figure/whitebox}
\newcommand{\learninggoals}{
\item Understand additive model structure
\item Combining base learners}

\lecturechapter{Model-based Boosting}
\lecture{Interpretable Machine Learning}


\begin{frame}{Model-based Boosting \citebutton{BÃ¼hlmann and Yu 2003}{https://doi.org/10.1198/016214503000125}}

\begin{itemize}
%\setlength\itemsep{2em}
\item<1-> Recall: Boosting iteratively combines weak base learners (BL) %to create a powerful ensemble model
\item<1->
Idea: Use simple linear BL to ensure interpretability \\
%$\leadsto$ e.g., linear BL with single features in each iteration
%Boosting with gradient descent using interpretable base learners (e.g., use base learners with single features in each iteration $\leadsto$ coordinate gradient descent)
%The resulting ensemble is also interpretable.
%\pause
\item<2->
Possible to combine linear BL of same type (with distinct parameters $\theta$ and $\theta^{\star}$):
%Two linear base learners $b_j(x, \theta)$ and $b_j(x, \theta^{\star})$ of the same type, but distinct parameter vectors $\theta$ and $\theta^{\star}$ can be combined in a base learner of the same type:
$$b_j(x, \theta) + b_j(x, \theta^{\star}) = b_j(x, \theta + \theta^{\star})$$
%\pause
\item<3-> %In each iteration, a set of BLs is fitted on pseudo residuals. The one with the best fit is added to the previously computed model (using step-size $\nu$), e.g.,
In each iteration, fit a set of BLs and add the best BL to previous model (using step-size $\nu$):
%\medskip
\begin{align*}
\widehat{f}^{[1]}(x) &= \hat{f}_0 + \nu \textcolor{blue}{b_3(x_3, \theta^{[1]})} \\
\widehat{f}^{[2]}(x) &= \widehat{f}^{[1]}(x) + \nu \textcolor{blue}{b_3(x_3, \theta^{[2]})} 
%= \hat{f}_0 + \nu \textcolor{blue}{b_3(x_3, \theta^{[1]})} + \nu \textcolor{blue}{b_3(x_3, \theta^{[2]})}
\\
\widehat{f}^{[3]}(x) &= \widehat{f}^{[2]}(x) + \nu \textcolor{orange}{b_1(x_1, \theta^{[3]})} 
%= \hat{f}_0 + \nu \textcolor{blue}{b_3(x_3, \theta^{[1]})} + \nu \textcolor{blue}{b_3(x_3, \theta^{[2]})} + \nu \textcolor{orange}{b_1(x_1, \theta^{[3]})} 
\\
&= \hat{f}_0 + \nu \left(\textcolor{blue}{b_3(x_3, \theta^{[1]} + \theta^{[2]})} + \textcolor{orange}{b_1(x_1, \theta^{[3]})}\right) 
\\
&= \hat{f}_0 + \textcolor{blue}{\hat{f}_3(x_3)} + \textcolor{orange}{\hat{f}_1(x_1)}
\end{align*}
\item<3-> Final model is additive (as GAMs), where each component function is interpretable

\end{itemize}
\end{frame}


\begin{frame}{Model-based Boosting - Example}

Simple case: Use linear model with single feature (including intercept) as BL
$$
b_j(x_j, \theta) = x_j\theta + \theta_0 \hspace*{0.2cm}\text{ for } j = 1,\ldots p \hspace*{0.3cm} \leadsto \text{ordinary linear regression}
$$

\begin{itemize}
\item After many iterations, it converges to same solution as least square estimate of LMs
\item Early stopping allows feature selection and might prevent overfitting (regularization)
%\item Specifying loss and link function according to exponential family leads a (regularized) GLM
\item Here: Interpretation of weights as in LM
\end{itemize}
\pause
\begin{columns}[T]
\begin{column}{0.49\textwidth}
\scriptsize
\begin{table}[ht]
\centering
\begin{tabular}{r|r|l}
  %\hline
\textbf{1000 iter. with $\nu = 0.1$} & Intercept & Weights \\ 
  \hline  \hline
days\_since\_2011 & -1791.06 & 4.9 \\ 
  \hline
  hum & 1953.05 & -31.1 \\ 
    \hline
  season & 0 &  \begin{tabular}[c]{@{}l@{}}
  WINTER: -323.4\\
  SPRING: 539.5\\
  SUMMER: -280.2\\
  FALL: 67.2
  \end{tabular}\\
    \hline
  %season &  & WINTER: -323.4, SPRING: 539.5, SUMMER: -280.2, FALL: 67.2 \\ 
  temp & -1839.85 & 120.4 \\ 
    \hline
  windspeed & 725.70 & -56.9 \\ 
    \hline
  offset & 4504.35 &  \\ 
   %\hline
\end{tabular}
\end{table}
\centering
$\Rightarrow$ Converges to solution of LM
\end{column}
\begin{column}{0.49\textwidth}
\scriptsize
\only<2>{
\begin{table}[ht]
\centering
\begin{tabular}{r|r|l}
  %\hline
 \textbf{20 iter. with $\nu = 0.1$} & Intercept & Weights \\ 
  \hline  \hline
  days\_since\_2011 & -1210.27 & 3.3 \\ 
    \hline
   season & 0 & 
   \begin{tabular}[c]{@{}l@{}}
  WINTER: -276.9\\
  SPRING: 137.6\\
  SUMMER: 112.8\\
  FALL: 20.3
   \end{tabular}\\
     \hline
  temp & -1118.94 & 73.2 \\ 
    \hline
  offset & 4504.35 &  \\ 
   %\hline
\end{tabular}
\end{table}
\centering
$\Rightarrow$ 3 BLs selected after 20 iter. (feature selection)
}
\only<3>{
Relative frequency of selected BLs across iterations
\includegraphics[width = .95 \textwidth]{figure/compboost_base_linear.pdf}}
\end{column}
\end{columns}
% \begin{itemize}
%     \item Linear base learners for numeric features and categorical base learner for season
%     \item 3 base learners selected after 100 iterations
% \end{itemize}
\end{frame}

\begin{frame}{Model-based Boosting - Interpretation}

\begin{itemize}
    \item Fit model on bike data with different BL types \citebutton{Daniel Schalk et al. 2018}{https://doi.org/10.21105/joss.00967}
    \item BLs: linear and centered splines for numeric features, categorical for season
    %and categorical base learner for season
\end{itemize}
\begin{columns}[T]
\pause
\begin{column}{0.5\textwidth}
 \includegraphics[width = 0.9\textwidth]{figure/compboost_pfi.pdf}
 \begin{itemize}
     \item Feature importance (risk reduction over iter.)\\
     $\leadsto$ \code{days\_since\_2011} most important
 \end{itemize}
%\scriptsize
%\verbatiminput{figure/mboost_output.txt}
\end{column}
\pause
\begin{column}{0.5\textwidth}  %%<--- here
  \includegraphics[width = 0.9 \textwidth]{figure/compboost_pfe.pdf}
  \begin{itemize}
      \item Partial feature effect for \code{days\_since\_2011}\\
      $\leadsto$ Total effect: Combination of partial effects of linear BL and centered spline BL 
  \end{itemize}
\end{column}
\end{columns}
\end{frame}


\endlecture
\end{document}