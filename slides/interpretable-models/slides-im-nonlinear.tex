\documentclass[11pt,compress,t,notes=noshow, aspectratio=169, xcolor=table]{beamer}

\usepackage{../../style/lmu-lecture}
% Defines macros and environments
\input{../../style/common.tex}

\title{Interpretable Machine Learning}
% \author{LMU}
%\institute{\href{https://compstat-lmu.github.io/lecture_iml/}{compstat-lmu.github.io/lecture\_iml}}
\date{}

\begin{document}

\newcommand{\titlefigure}{figure/gam_effects.pdf}
\newcommand{\learninggoals}{
\item Generalized additive model
\item Decision trees
\item Other interpretable models}

\lecturechapter{Interpretable Nonlinear Models}
\lecture{Interpretable Machine Learning}

%------------------------------------------------------------------
%------------------------------------------------------------------

\begin{frame}{Generalized Additive Model (GAM) \citebutton{Hastie and Tibshirani (1986)}{https://www.jstor.org/stable/2245459}}

\textbf{Problem}: LM not suitable if relationship between features and target variable is not linear % $\leadsto$ standard 

\medskip
\pause
\textbf{Workaround in LMs / GLMs}: 
 \begin{itemize}
        \item Feature transformations (e.g., exp or log)
        \item Including high-order effects
        \item Categorization of features (i.e., intervals / buckets of feature values)
        
    \end{itemize}
\medskip
\pause
\textbf{Idea of GAMs:}

    \begin{itemize}
        \item Instead of linear terms $\beta_j x_j$, use flexible functions $f_j(x_j)$ $\leadsto$ splines
            $$g(\E (y \mid \xv)) = \beta_0 + f_1(x_1) + f_2(x_2) + \ldots + f_p(x_p)$$
    
        \item Preserves additive structure and allows to model non-linear effects
        \item Splines have a smoothness parameter to control flexibility (prevent overfitting)\\
        $\leadsto$ Needs to be chosen, e.g., via cross-validation
    \end{itemize}
    
    

   

\end{frame}


\begin{frame}{Generalized Additive Model (GAM) - Example}
Fit a GAM with smooth splines for four numeric features of bike rental data \\
$\leadsto$ more flexible and better model fit but less interpretable than LM

\begin{columns}[T]
\begin{column}{0.5\textwidth}
\begin{table}[ht]
\centering
\scriptsize
% \begin{tabular}{rrrrr}
%   \hline
%  & edf & Ref.df & F & p-value \\ 
%   \hline
% s(temp) & 5.8 & 7.0 & 57.2 & 0.00 \\ 
%   s(hum) & 4.0 & 5.1 & 68.0 & 0.00 \\ 
%   s(windspeed) & 1.7 & 2.1 & 50.1 & 0.00 \\ 
%   s(days\_since\_2011) & 8.3 & 8.8 & 154.4 & 0.00 \\ 
%   \hline
\begin{tabular}{rrr}
  \hline
 & edf & p-value \\ 
  \hline
s(temp) & 5.8 & 0.00 \\ 
  s(hum) & 4.0 & 0.00 \\ 
  s(windspeed) & 1.7 & 0.00 \\ 
  s(days\_since\_2011) & 8.3 & 0.00 \\ 
   \hline
\end{tabular}
\end{table}


\textbf{Interpretation}
\begin{itemize}
    %\item If the temperature increases by 1 degree Celsius then the log odds of high number of bike rentals increase linearly by 0.3 c.p.
    \item Interpretation needs to be done visually and relative to average prediction
    \item Edf (effective degrees of freedom) represents complexity of smoothness
\end{itemize}
\end{column}
\hfill
\begin{column}{0.45\textwidth}
\includegraphics[width = \textwidth]{figure/gam_effects.pdf}
\end{column}
\end{columns}
\end{frame}
%------------------------------------------------------------------
%------------------------------------------------------------------


% \begin{frame}{Decision Trees}

% \begin{columns}

% \begin{column}{0.3\textwidth}

%   \begin{tikzpicture}
%   \usetikzlibrary{arrows}
%     \usetikzlibrary{shapes}
%      \tikzset{treenode/.style={draw, circle, font=\small}}
%      \tikzset{line/.style={draw, thick}}
%      \node [treenode, draw=red] (a0) {$a_0$};
%      \node [treenode, below=0.75cm of a0, xshift=-1cm]  (a1) {$a_1$};
%      \node [treenode, draw=red, below=0.75cm of a0, xshift=1cm]  (a2) {$a_2$};

%      \node [treenode, draw=red, below=0.75cm of a2, xshift=-1cm] (a3) {$a_3$};
%      \node [treenode, below=0.75cm of a2, xshift=1cm]  (a4) {$a_4$};

%      \node [treenode, below=0.75cm of a3, xshift=-1cm] (a5) {$a_5$};
%      \node [treenode, below=0.75cm of a3, xshift=1cm]  (a6) {$a_6$};

%      \path [line] (a0.south) -- + (0,-0.4cm) -| (a1.north) node [midway, above] {$x_1<0.3$};
%      \path [line] (a0.south) -- +(0,-0.4cm) -|  (a2.north) node [midway, above] {$x_1\geq0.3$};

%      \path [line] (a2.south) -- + (0,-0.4cm) -| (a3.north) node [midway, above] {$x_1<0.6$};;
%      \path [line] (a2.south) -- +(0,-0.4cm) -|  (a4.north) node [midway, above] {$x_1\geq0.6$};


%      \path [line] (a3.south) -- + (0,-0.4cm) -| (a5.north) node [midway, above] {$x_2<0.2$};;
%      \path [line] (a3.south) -- +(0,-0.4cm) -|  (a6.north) node [midway, above] {$x_2\geq0.2$};

%   \end{tikzpicture}

% \end{column}

% \begin{column}{0.7\textwidth}

% Properties:
% \begin{itemize}
%     \item able to model non-linear effects
%     \item terminal nodes (aka leaf nodes) can have several observations and predicts the mean outcome over these
%     \item Applicable to regression and classification
% \end{itemize}

% Interpretation:
% \begin{itemize}
%     \item directly by following the tree (i.e., sequence of rules)
%     \item Feature importance by (scaled) score of much the splitting criterion was reduced compared to the parent
% \end{itemize}

% \end{column}

% \end{columns}

% \end{frame}

\begin{frame}{Decision Trees \citebutton{Breiman et al. (1984)}{https://doi.org/10.1201/9781315139470}}

%\textbf{Problem}: Can we model non-linear effects and interactions automatically (without manual specification as in GLMs or GAMs)?\\
%Relationship between features and target are non-linear or interactions are present\\
%\medskip
%\pause

\begin{columns}[T]

\begin{column}{0.7\textwidth}
\textbf{Idea of decision trees}: 
%Split data in different subsets based on cut-off values in features 
Partition data into subsets based on cut-off values in features (found by minimizing a split criterion via greedy search) and predict constant mean $c_m$ in leaf node $\mathcal{R}_m$:

$$
\hat f(x) = \sum_{m=1}^M c_m \mathds{1}_{\{x \in \mathcal{R}_m\}},
$$

% \begin{itemize}
% \item where $c_m$ is a constant and 
% \item $\mathcal{R}_m$ the $m$-th leaf node of the tree
% \end{itemize}

\begin{itemize}
    %\item Finding best split point (CART): Greedy search for the point that minimizes the variance of $y$ (regression) or the Gini index (classification)
    \item Applicable to regression and classification
    \item Able to model interactions and non-linear effects
    \item Able to handle mixed feature spaces and missing values
\end{itemize}

\end{column}
\begin{column}{0.3\textwidth}

\begin{tikzpicture}[scale=0.75, transform shape]
   \usetikzlibrary{arrows}
    \usetikzlibrary{shapes}
     \tikzset{treenode/.style={draw, circle, font=\small}}
     \tikzset{line/.style={draw, thick}}
     \node [treenode, draw=red] (a0) {};
     \node [treenode, below=0.75cm of a0, xshift=-1.5cm]  (a1) {};
     \node [treenode, below=0.75cm of a0, xshift=1.5cm]  (a2) {};

     \node [treenode, below=0.75cm of a2, xshift=-0.75cm] (a3) {$c_1$};
     \node [treenode, below=0.75cm of a2, xshift=0.75cm]  (a4) {$c_2$};

    \node [treenode, below=0.75cm of a1, xshift=-0.75cm] (a5) {$c_3$};
    \node [treenode, below=0.75cm of a1, xshift=0.75cm]  (a6) {$c_4$};

     \path [line] (a0.south) -- + (0,-0.4cm) -| (a1.north) node [midway, above] {$x_1<3$};
     \path [line] (a0.south) -- +(0,-0.4cm) -|  (a2.north) node [midway, above] {$x_1\geq3$};

     \path [line] (a2.south) -- + (0,-0.4cm) -| (a3.north) node [midway, above] {$x_2<6$};;
     \path [line] (a2.south) -- +(0,-0.4cm) -|  (a4.north) node [midway, above] {$x_2\geq6$};

    \path [line] (a1.south) -- + (0,-0.4cm) -| (a5.north) node [midway, above] {$x_3<2$};;
    \path [line] (a1.south) -- +(0,-0.4cm) -|  (a6.north) node [midway, above] {$x_3\geq2$};

    %\path (a5.south) -- +(0,0) -|  (a5.south) node [midway, below] {$f_{1,3}(x_1,x_3)$};
    %\path (a6.south) -- +(0,0) -|  (a6.south) node [midway, below] {$f_{1,3}(x_1,x_3)$};

    %\path (a3.south) -- +(0,0) -|  (a3.south) node [midway, below] {$f_{1,2}(x_1,x_2)$};
    %\path (a3.south) -- +(0,0) -|  (a4.south) node [midway, below] {$f_{1,2}(x_1,x_2)$};
    % \path (a3) edge [bend right, draw=white] node [below] {$f_{1,2}(x_1,x_2)$} (a4);
    % \path (a5) edge [bend right, draw=white] node [below] {$f_{1,3}(x_1,x_3)$} (a6);
   \end{tikzpicture}
   
\end{column}
\end{columns}

\medskip
\pause
\textbf{Interpretation}
\begin{itemize}
    \item Directly by following the tree structure (i.e., sequence of decision rules)
    \item Importance of $x_j$ based on ``improvement in split criterion'' over all splits where $x_j$ was involved\\
    (e.g., variance for regression or Gini index for classification)
    %Feature importance: How much did split criterion improve compared to parent node% by (scaled) score of how much splitting criterion (e.g. variance) is reduced compared to a parent node
\end{itemize}



\end{frame}

\begin{frame}{Decision Trees - Example}
\begin{itemize}
    \item Fit decision tree with tree depth of 3 on bike data
    \item E.g., mean prediction for the first 105 days since 2011 is 1798 (applies to $\hat = 15\%$ of the data)
    \item \code{days\_since\_2011} shows highest feature importance (explains most of variance)
\end{itemize}
\begin{columns}[T]
\begin{column}{0.4\textwidth}
\begin{table}[ht]
\centering
\begin{tabular}{lr}
  \hline
Feature & Importance \\ 
  \hline
days\_since\_2011 & 68.03 \\ 
  temp & 20.54 \\ 
  season & 6.56 \\ 
  hum & 3.58 \\ 
  windspeed & 1.29 \\ 
   \hline
\end{tabular}
\end{table}
\end{column}
\begin{column}{0.6\textwidth}
  \includegraphics[width = \textwidth]{figure/tree.pdf} 
\end{column}
\end{columns}
 
\end{frame}
%------------------------------------------------------------------
%------------------------------------------------------------------

%\begin{frame}[c]{Decision Rules}

%\texttt{IF COND$_1$ AND COND$_2$ AND ... THEN value}

%\begin{itemize}
%    \item \texttt{COND$_i$} can be of the form \texttt{feature <op> value} where \texttt{<op>} can be for example $\{=, <, > \}$
%\end{itemize}

%\pause
%\medskip

%Properties:
%\begin{description}
%    \item{Support} Fraction of observations to support appliance of rule
%    \item{Accuracy} for predicting the correct class under the condition(s)
%\end{description}

%$\leadsto$ often trade-off between these two

%\pause
%\medskip

%$\leadsto$ many different ways to learn a set of rules (incl. a default rule if none of the rules are met)

%\end{frame}

%------------------------------------------------------------------
%------------------------------------------------------------------


\begin{frame}[c]{Other Interpretable Models}

\textbf{RuleFit} \citebutton{Friedman and Popescu 2008}{https://arxiv.org/abs/0811.1679}
\begin{itemize}
    \item Combination of linear models and decision trees 
    \item Allows for feature interactions and non-linearities
\end{itemize}

\textbf{Decision Rules} \citebutton{Holte 1993}{https://doi.org/10.1023/A:1022631118932}
\begin{itemize}
    \item Simple ``if -- then'' statements - very intuitive and easy-to-interpret
    \item Common assumption: only classification tasks and categorical features
\end{itemize}

\textbf{Naive Bayes}
\citebutton{Zhang 2004}{https://www.aaai.org/Papers/FLAIRS/2004/Flairs04-097.pdf}
%$$P (C_k \mid x ) = \frac{1}{Z} P(C_k) \prod_{i=1}^{n} P(x_i \mid C_k) $$
\begin{itemize}
    \item Product of probabilities for a class on the value of each feature
    \item Strong independence assumption
\end{itemize}


\textbf{k-Nearest Neighbor}
\citebutton{Cover 1967}{https://doi.org/10.1109/TIT.1967.1053964}
\begin{itemize}
    \item (Closely related to case-based reasoning)
    \item Average of the outcome of neighbors -- local explanation
\end{itemize}

\end{frame}


\endlecture
\end{document}