\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\usepackage[]{graphicx}\usepackage[]{color}
% maxwidth is the original width if it is less than linewidth
% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
	\ifdim\Gin@nat@width>\linewidth
	\linewidth
	\else
	\Gin@nat@width
	\fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%
\let\hlipl\hlkwb

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
	\def\at@end@of@kframe{}%
	\ifinner\ifhmode%
	\def\at@end@of@kframe{\end{minipage}}%
\begin{minipage}{\columnwidth}%
	\fi\fi%
	\def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
		\colorbox{shadecolor}{##1}\hskip-\fboxsep
		% There is no \\@totalrightmargin, so:
		\hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
	\MakeFramed {\advance\hsize-\width
		\@totalleftmargin\z@ \linewidth\hsize
		\@setminipage}}%
{\par\unskip\endMakeFramed%
	\at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}
\newcommand{\SweaveOpts}[1]{}  % do not interfere with LaTeX
\newcommand{\SweaveInput}[1]{} % because they are not real TeX commands
\newcommand{\Sexpr}[1]{}       % will only be parsed by R

\usepackage[english]{babel}
\usepackage[utf8]{inputenc}

\usepackage{dsfont}
\usepackage{verbatim}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{bm}
\usepackage{csquotes}
\usepackage{multirow}
\usepackage{longtable}
\usepackage{booktabs}
\usepackage{enumerate}
\usepackage[absolute,overlay]{textpos}
\usepackage{psfrag}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{eqnarray}
\usepackage{arydshln}
\usepackage{tabularx}
\usepackage{placeins}
\usepackage{tikz}
\usepackage{setspace}
\usepackage{colortbl}
\usepackage{mathtools}
\usepackage{wrapfig}
\usepackage{bm}

\usetikzlibrary{shapes,arrows,automata,positioning,calc,chains,trees, shadows}
\tikzset{
	%Define standard arrow tip
	>=stealth',
	%Define style for boxes
	punkt/.style={
		rectangle,
		rounded corners,
		draw=black, very thick,
		text width=6.5em,
		minimum height=2em,
		text centered},
	% Define arrow style
	pil/.style={
		->,
		thick,
		shorten <=2pt,
		shorten >=2pt,}
}

\usepackage{subfig}

% Defines macros and environments
\input{../../style/common.tex}

%\usetheme{lmu-lecture}
\newcommand{\titlefigure}{figure_man/feature-importance.png}
\newcommand{\learninggoals}{
	\item Understand motivation for feature importance
	\item Develop an intuition for possible use-cases
	\item Know characteristics of feature importance methods}
\usepackage{../../style/lmu-lecture}

\let\code=\texttt
\let\proglang=\textsf

\setkeys{Gin}{width=0.9\textwidth}

\title{Feature Importance}
% \author{Bernd Bischl, Christoph Molnar, Daniel Schalk, Fabian Scheipl}
\institute{\href{https://compstat-lmu.github.io/lecture_i2ml/}{compstat-lmu.github.io/lecture\_i2ml}}
\date{}

\setbeamertemplate{frametitle}{\expandafter\uppercase\expandafter\insertframetitle}

%\usepackage{Sweave}
\begin{document}
	
	% Set style/preamble.Rnw as parent.
	
	% Load all R packages and set up knitr
	
	% This file loads R packages, configures knitr options and sets preamble.Rnw as 
	% parent file
	% IF YOU MODIFY THIS, PLZ ALSO MODIFY setup.Rmd ACCORDINGLY...
	
	% Defines macros and environments
	
	\input{../../latex-math/basic-math.tex}
	\input{../../latex-math/basic-ml.tex}
	\input{../../latex-math/ml-interpretable.tex}
	
	\lecturechapter{Introduction}
	\lecture{Interpretable Machine Learning}
	
	% ------------------------------------------------------------------------------

\begin{vbframe}{Motivation}
\begin{itemize}
  \item \textbf{Feature effects} describe the relationship of features with the prediction $\yh$.
  \begin{itemize}
    \item requires one plot per feature
    \item does not take the prediction target $y$ into account
  \end{itemize}
  \item \textbf{Feature importance} methods quantify the relevance of features for the prediction performance.
  \begin{itemize}
    \item condensed to one number per feature
    \item insight into the relationship with $y$
  \end{itemize}
\end{itemize}

\end{vbframe}
\begin{vbframe}{Example}

Feature Importance offers a condensed summary of the relevance of features for the prediction.

\begin{center}
  \begin{figure}
  \includegraphics[width=0.6\linewidth]{figure_man/bike_pdp+ice} \hfill \includegraphics[width=0.35\linewidth]{figure_man/bike_pfi}
  \caption{A random forest was fit on the bike sharing dataset. Left: Feature Effect Plots (PDP) for all features. Right: Feature Importance Plot (PFI).}
\end{figure}
\end{center}

\end{vbframe}

\begin{vbframe}{Feature Importance Scheme}
In general, feature importance is computed in three steps:
\lz
\begin{enumerate}
  \item \textbf{Perturbation:} Sample a perturbed version of the dataset $\pert{\D}{}{}$.
  \begin{itemize}
    \item for example by replacing the feature of interest $x_j$ with an uninformative sample $\pert{x_j}{}{}$
    \end{itemize}
  \item \textbf{Prediction:} Make predictions for both i.e., $\D$ and $\pert{\D}{}{}$.
  \item \textbf{Aggregation:} 
    \begin{itemize}
      \item Compute the loss for each observation in both data sets.
      \item Take the difference of both losses $\Delta L$ for each observation.
      \item For global interpretation: Average this change in loss across all observations.
    \end{itemize}
\end{enumerate}
\lz
The interpretation of the feature importance methods depends on the type of perturbation and aggregation.

\footnote[frame]{Scholbeck, Christian A., et al. "Sampling, intervention, prediction, aggregation: A generalized framework for model-agnostic interpretations." arXiv preprint arXiv:1904.03959 (2019).}
\end{vbframe}


\begin{vbframe}{Potential Interpretation Goals}

Depending on type of perturbation and aggregation, different interpretation goals can (or cannot) be reached. For example, one may be interested in one of the following questions:
\lz
\begin{enumerate}
    \item Is the feature $\xj$ causal for the prediction?
    \item Does the variable $\xj$ contain prediction-relevant information about $\ydat$ (that the model exploits)?
    \item Does the model require access to $\xj$ to achieve it's prediction performance? (I.e. does the feature contribute unique information about $\ydat$?)
\end{enumerate}
\lz
Except for special cases, the answers to these questions do not coincide.
\end{vbframe}

\begin{vbframe}{Potential Interpretation Goals}

The properties do not necessarily coincide:

\begin{itemize}
  \item A feature may causal for the prediction $\yh$ (1) without containing prediction-relevant information about $\ydat$ (2).
  \item A feature may contain prediction relevant information (2) without the model exploiting it (1).
  \item A feature may contain prediction-relevant information that the model exploits (2), without the model requiring access to the feature for (optimal) prediction performance (3).
\end{itemize}

\end{vbframe}

\endlecture
\end{document}
