\documentclass[11pt,compress,t,notes=noshow, aspectratio=169, xcolor=table]{beamer}

\usepackage{../../style/lmu-lecture}
% Defines macros and environments
\input{../../style/common.tex}

\title{Interpretable Machine Learning}
% \author{LMU}
%\institute{\href{https://compstat-lmu.github.io/lecture_iml/}{compstat-lmu.github.io/lecture\_iml}}
\date{}

\bibliography{feature-importance}
%\usepackage{Sweave}

%\usepackage{Sweave}
\begin{document}
	\newcommand{\titlefigure}{figure_man/feature-importance.png}
    \newcommand{\learninggoals}{
    	\item SAGE
    	}
	% Set style/preamble.Rnw as parent.
	
	% Load all R packages and set up knitr
	
	% This file loads R packages, configures knitr options and sets preamble.Rnw as 
	% parent file
	% IF YOU MODIFY THIS, PLZ ALSO MODIFY setup.Rmd ACCORDINGLY...
	
	% Defines macros and environments

	\lecturechapter{Shapley Additive Global Importance (SAGE)}
	\lecture{Interpretable Machine Learning}
	
	% ------------------------------------------------------------------------------

% CHALLENGE
\begin{frame}{Challenge: Fair Attribution of Importance}
\begin{columns}[T]
\begin{column}{0.54\textwidth}
\textbf{Recap:} 
\begin{itemize}
\item Data: $x_1, \dots, x_4$ uniformly sampled from $[-1, 1]$ 
\item DGP: $y:= x_1 x_2 + x_3 + \epsilon_Y$ with $\epsilon_Y \sim N(0, 1)$
\item Model: $\fh(x) \approx x_1 x_2 + x_3$
\end{itemize}

\lz

Although $x_3$ alone contributes as much to the prediction as $x_1$ and $x_2$ jointly, all three are considered equally relevant by PFI.
\end{column}
\begin{column}{0.45\textwidth}
\centering
  \includegraphics[width=\linewidth]{figure_man/pfi_interactions.pdf}
\end{column}
\end{columns}
\lz
\textbf{Reason:} PFI assesses importance given that all remaining features are preserved. If we first permute $x_1$ and then $x_2$, permutation of $x_2$ would have no effect on the performance (and vice versa).
\end{frame}



\begin{frame}{Challenge: Fair attribution of importance \citebutton{Covert et al. (2020)}{https://proceedings.neurips.cc/paper/2020/file/c7bf0b7c1a86d5eb3be2c722cf2cf746-Paper.pdf}}

\textbf{Observation:} 
\begin{itemize}
    \item Feature importance attribution can be regarded as cooperative game \\
    $\leadsto$ features jointly contribute to achieve a certain model performance
    \item Player: features
    \item Payoff to be fairly distributed: model performance
    \item Marginal contribution of a feature depends on the coalition of features that were already used
\end{itemize}
%, where each feature is a player and model performance is the payoff. The surplus contribution of a feature depends on the coalition of features that are already in the room.\\
\lz
\textbf{SAGE Idea:} %Leverage game-theoretic 
Use Shapley values to compute a fair attribution of importance (via model performance)\\
$\Rightarrow$ Global feature importance technique\\
$\Rightarrow$ Compute approximate Shapley values $\phi_j$ using appropriate value function 
%\lz
%$\Rightarrow$ SAGE translates SHAP into a global feature importance technique\\
%\lz
%In order to compute SAGE, approximate Shapley values $\phi_j$ as for SHAP, replacing SHAP value functions with SAGE value functions.\\

%\footnote[frame]{\fullcite{NEURIPS2020_c7bf0b7c}}
  
\end{frame}


% SAGE IDEA
\begin{frame}{SAGE Removal and Value function}
  
 \textbf{Removal Idea:} %To deprive the model of the non-coalition features $-S$, 
 To deprive information of the non-coalition features $-S$ from the model, marginalize the prediction function over the features $-S$ to be ``dropped''.

$$\fh_S(x_S) = \E[\fh(x) | X_S = x_S]$$

\pause

The ``dropped'' features can be sampled from \\
\begin{itemize}
\item the marginal distribution $\P(x_{-S})$
\item the conditional distribution $\P(x_{-S}|x_S)$
\end{itemize}
%\lz

%\footnote[frame]{\fullcite{NEURIPS2020_c7bf0b7c}}

%For SAGE, value functions quantify the predictive power of a coalition $S$ in terms of reduction in risk over the mean prediction.\\
\pause
\lz

SAGE value function:  $v_{\fh}(S) = \risk(\fh_{\emptyset}) - \risk(\fh_{S})$\\
$\leadsto$ Quantify the predictive power of a coalition $S$ in terms of reduction in risk \\
$\leadsto$ Risk of predictor $\fh_S(x_S)$ is compared to the risk of the mean prediction $\fh_{\emptyset}$

\pause
\lz

Marginal contribution of feature $x_j$ over coalition $x_S$:  $v_{\fh}(S \cup \{j\}) - v_{\fh}(S) = \risk(\fh_{S})-\risk(\fh_{S \cup \{j\}})$\\
$\leadsto$ Quantifies the added value of feature $j$ when it is added to coalition $S$
\lz

\end{frame}

\begin{frame}{SAGE Removal and Value function}

Interpretation of SAGE value functions differ, depending on whether conditional or marginal sampling is used.

\lz\pause

\textbf{Marginal sampling:} $v(S)$ quantifies the reliance of the model on features $x_S$
\begin{itemize}
  \item features $x_S$ not being causal for the prediction is a sufficient condition for $v(S) = 0$
\end{itemize}

\lz\pause

\textbf{Conditional sampling}: $v(S)$ quantifies the reliance of the model on the information contained in variables $x_S$
\begin{itemize}
  \item features $x_S$ not being causal for the prediction is not a sufficient condition for $v(S) = 0$
  \item under model optimality, links to mutual information or the conditional variance can be drawn
\end{itemize}

\end{frame}

\begin{frame}{SAGE Removal and Value function}

When the loss-optimal model $f^*$ is inspected using \textit{conditional-sampling} based SAGE value functions, interesting links exist.

\lz\pause
\textbf{For cross-entropy loss:} 
  \begin{itemize}
      \item value function is the mutual information:  $v_{f^*}(S) = I(y;x_S)$
      \item marginal contribution of a feature $x_j$ is the conditional mutual information: $v_{f^*}(S \cup \{j\}) - v_{f^*}(S) = I(y,x_i|x_S)$
  \end{itemize}

\lz\pause

\textbf{For MSE loss:} 
    \begin{itemize}
    \item value function is the expected reduction in variance given knowledge of the features $x_S$: $v_{f^*}(S) = Var(y) - \E[Var(y|x_S)]$
    \item marginal contribution is the respective reduction over $x_S$:
    $v_{f^*}(S \cup \{j\}) - v_{f^*}(S) = \E[Var(y|x_S)] - \E[Var(y|x_{S \cup {j}})]$
    \end{itemize}
    
\end{frame}

\begin{frame}{SAGE Removal and Value function}

\textbf{Example:} $x_1 = \epsilon_1$, $x_2 = x_1 + \epsilon_2$, $x_3 = x_2 + \epsilon_3$, $y = x_3 + \epsilon_y$ with $\epsilon_j$ independent noise terms (causal DAG: $x_1 \rightarrow x_2 \rightarrow x_3 \rightarrow y$). The prediction model is defined as  $\fh \approx 0.95 x_3 + 0.05 x_2$. 
%
\begin{figure}
  \includegraphics[width=0.6\linewidth]{figure_man/sage_variants}
\end{figure}
%
While the conditional $v(j)$ are nonzero for features that are not used by $\fh$, marginal $v(j)$ are only nonzero for features that are used by $\fh$. 
Since for conditional $v$ the difference $v(-j \cup j) - v(-j)$ quantifies the unique contribution of $x_j$ over remaining variables $-j$ and $y \indep x_1, x_2 | x_3$, only $v(\{1,2,3\}) - v(\{1, 2\})$ is nonzero.
%
\end{frame}

\begin{frame}{SAGE interpretation}

The Shapley axioms can be translated into properties of SAGE. The interpretation depends on whether conditional or marginal sampling is used.
%
\begin{table}
  \centering
  \begin{tabular}{l | l }
  Shapley property $\implies$ & conditional SAGE property \\
  \hline
  efficiency & $\sum_{i=1}^d \phi_j(v) = \risk(\fh_\emptyset) - \risk(\fh)$\\
  symmetry & $x_j = x_i \implies \phi_i = \phi_j$ \\
  linearity & $\phi_j$ expecation of per-instance\\
  & conditional SHAP applied to model loss\\
  monotonicity & given models $f, f'$, if  $\forall S:$\\
  &$v_f(S \cup j) - v_f(S) \geq v_{f'}(S \cup j) - v_{f'}(S)$ \\
  &then $\phi_j(v_f) \geq \phi_j(v_{f'})$\\
  dummy & if $\forall S: \fh(x) \indep x_j | x_S \Rightarrow \phi_j = 0$
  \end{tabular}
\end{table}

\end{frame}

\begin{frame}{SAGE interpretation}
%
The Shapley axioms can be translated into properties of SAGE. The interpretation depends on whether conditional or marginal sampling is used.
%
\begin{table}
  \centering
  \begin{tabular}{l | l }
  Shapley property $\implies$ & marginal SAGE property \\
  \hline
  efficiency & $\sum_{i=1}^d \phi_j(v) = \risk(\fh_\emptyset) - \risk(\fh)$\\
  symmetry & no intelligible implication \\
  linearity & $\phi_j$ expecation of per-instance\\
  & marginal SHAP applied to model loss\\
  monotonicity & given models $f, f'$, if  $\forall S:$\\
  &$v_f(S \cup j) - v_f(S) \geq v_{f'}(S \cup j) - v_{f'}(S)$ \\
  &then $\phi_j(v_f) \geq \phi_j(v_{f'})$\\
  dummy & model invariant to $x_j \Rightarrow \phi_j = 0$\\
  \end{tabular}
\end{table}
%
\end{frame}

\begin{frame}{Interaction Example Revisited}

\textbf{Recap:} Data: $x_1, \dots, x_4$ uniformly sampled from $\{-1, 1\}$ and $y:= x_1 x_2 + x_3 + \epsilon_Y$ with $\epsilon_Y \sim N(0, 1)$. Model: $\fh(x) \approx x_1 x_2 + x_3$.\\
%
\begin{figure}
  \includegraphics[width=0.7\linewidth]{figure_man/sage_pfi_interactions}
\end{figure}
%
While PFI regards $x_1, x_2$ to be equally important as $x_3$, marginal SAGE fairly divides the contribution of the interaction $x_1$ and $x_2$.
  
\end{frame}


%\begin{frame}{Implications marginal SAGE}
%
%Can we gain insight into whether...
%
%\begin{enumerate}
%    \item the feature $x_j$ is causal for the prediction?
%    \begin{itemize}
%      \item yes, because of the dummy property nonzero $\phi_j$ implies $x_j \rightarrow \fh(x)$
%    \end{itemize}
%    \item the variable $x_j$ contains prediction-relevant information?
%    \begin{itemize}
%      \item No, whether a model relies on the feature (to improve the performance).
%    \end{itemize}
%    \item the model requires access to $x_j$ to achieve it's prediction performance?    
%    \begin{itemize}
%      \item 
%\end{itemize}
%\end{enumerate}
%
%\end{frame}
%
%\begin{frame}{Implications conditional SAGE}
%
%Can we gain insight into whether...
%
%\begin{itemize}
%    \item the feature $\xj$ is causal for the prediction?
%    \begin{itemize}
%      \item no
%    \end{itemize}
%    \item the variable $\xj$ contains prediction-relevant information about $\ydat$?
%    \begin{itemize}
%      \item yes
%    \end{itemize}
%    \item the model requires access to $x_j$ to achieve it's prediction performance?  
%    \begin{itemize}
%        \item no
%    \end{itemize}
%\end{itemize}
%
%\end{frame}


% \begin{frame}
%   \printbibliography
% \end{frame}

\endlecture
\end{document}
