\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\usepackage[]{graphicx}\usepackage[]{color}
% maxwidth is the original width if it is less than linewidth
% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
	\ifdim\Gin@nat@width>\linewidth
	\linewidth
	\else
	\Gin@nat@width
	\fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%
\let\hlipl\hlkwb

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
	\def\at@end@of@kframe{}%
	\ifinner\ifhmode%
	\def\at@end@of@kframe{\end{minipage}}%
\begin{minipage}{\columnwidth}%
	\fi\fi%
	\def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
		\colorbox{shadecolor}{##1}\hskip-\fboxsep
		% There is no \\@totalrightmargin, so:
		\hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
	\MakeFramed {\advance\hsize-\width
		\@totalleftmargin\z@ \linewidth\hsize
		\@setminipage}}%
{\par\unskip\endMakeFramed%
	\at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}
\newcommand{\SweaveOpts}[1]{}  % do not interfere with LaTeX
\newcommand{\SweaveInput}[1]{} % because they are not real TeX commands
\newcommand{\Sexpr}[1]{}       % will only be parsed by R

\usepackage[english]{babel}
\usepackage[utf8]{inputenc}

\usepackage{dsfont}
\usepackage{verbatim}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{bm}
\usepackage{csquotes}
\usepackage{multirow}
\usepackage{longtable}
\usepackage{booktabs}
\usepackage{enumerate}
\usepackage[absolute,overlay]{textpos}
\usepackage{psfrag}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{eqnarray}
\usepackage{arydshln}
\usepackage{tabularx}
\usepackage{placeins}
\usepackage{tikz}
\usepackage{setspace}
\usepackage{colortbl}
\usepackage{mathtools}
\usepackage{wrapfig}
\usepackage{bm}
\usepackage[backend=biber]{biblatex}

\usetikzlibrary{shapes,arrows,automata,positioning,calc,chains,trees, shadows}
\tikzset{
	%Define standard arrow tip
	>=stealth',
	%Define style for boxes
	punkt/.style={
		rectangle,
		rounded corners,
		draw=black, very thick,
		text width=6.5em,
		minimum height=2em,
		text centered},
	% Define arrow style
	pil/.style={
		->,
		thick,
		shorten <=2pt,
		shorten >=2pt,}
}

\usepackage{subfig}

% Defines macros and environments
\input{../../style/common.tex}

%\usetheme{lmu-lecture}
\newcommand{\titlefigure}{figure_man/feature-importance.png}
\newcommand{\learninggoals}{
	\item SAGE value functions
	\item SAGE interpretation}
\usepackage{../../style/lmu-lecture}

\let\code=\texttt
\let\proglang=\textsf

\setkeys{Gin}{width=0.9\textwidth}

\title{Feature Importance}
% \author{Bernd Bischl, Christoph Molnar, Daniel Schalk, Fabian Scheipl}
\institute{\href{https://compstat-lmu.github.io/lecture_i2ml/}{compstat-lmu.github.io/lecture\_i2ml}}
\date{}

\setbeamertemplate{frametitle}{\expandafter\uppercase\expandafter\insertframetitle}

\bibliography{feature-importance}

%\usepackage{Sweave}
\begin{document}
	
	% Set style/preamble.Rnw as parent.
	
	% Load all R packages and set up knitr
	
	% This file loads R packages, configures knitr options and sets preamble.Rnw as 
	% parent file
	% IF YOU MODIFY THIS, PLZ ALSO MODIFY setup.Rmd ACCORDINGLY...
	
	% Defines macros and environments
	
	\input{../../latex-math/basic-math.tex}
	\input{../../latex-math/basic-ml.tex}
	\input{../../latex-math/ml-interpretable.tex}
	
	\lecturechapter{Shapley Additive Global Importance (SAGE)}
	\lecture{Interpretable Machine Learning}
	
	% ------------------------------------------------------------------------------

% CHALLENGE
\begin{frame}{Challenge: Fair attribution of importance}

\begin{itemize}
  \item contribution of a feature may be shared
  \item how do we fairly attribute importance?
\end{itemize}
\lz
Copy interaction example from PFI chapter.\\
\lz
$\Rightarrow$ Regard feature importance attribution as cooperative game, where each feature is a player. Depending on the order in which they enter a game, their marginal payoff may differ.\\
$\Rightarrow$ Use Shapley values to compute a fair attribution.
  
\end{frame}


% SAGE IDEA
\begin{vbframe}{SAGE Removal}
  
SAGE translates SHAP into a global feature importance technique. For SAGE, value functions quantify the predictive power of a coalition $S$ in terms of reduction in risk over the mean prediction.\\
\lz
\textbf{Removal Idea:} In order to deprive the model of the non-coalition features $-S$, marginalize the prediction function over the dropped features.

$$\fh_S(x_S) = \E[\fh(x) | X_S = x_S]$$
  
The dropped features can either be sampled from the marginal distribution $\P(x_{-S})$ or the conditional distribution $\P(x_{-S}|x_S)$.\\
\lz

  
\footnote[frame]{\fullcite{NEURIPS2020_c7bf0b7c}}
  
\end{vbframe}

% SAGE value functions

\begin{vbframe}{SAGE value function}

Value function:  $ v_{\fh}(S) = \risk(\fh_{\emptyset}) - \risk(\fh_{S})$\\
\lz
Contribution of feature $x_j$ over coalition $x_S$:  $v_{\fh}(S \cup \{j\}) - v_{\fh}(S)$\\
\lz
Depending on model and loss function, interesting links exist:
\begin{itemize}
  \item cross-entropy loss: conditional mutual information
  \item MSE: reduction in variance
\end{itemize}


\end{vbframe}

\begin{vbframe}{SAGE}

Algorithm and closed form
  
\end{vbframe}



\begin{vbframe}{SAGE Interpretation}

\begin{itemize}
	\item indirect influence not quantified
	\begin{itemize}
		\item if the model did not mechanistically use the feature, that does not imply that the model did not make use of information that is also contained in the feature
		\item e.g. relevant for fairness audits (we do not care whether influence is direct or indirect)
	\end{itemize}
	\item no fair attribution (e.g. interactions)
\end{itemize}

\end{vbframe}


\begin{vbframe}{Interpretation goal overview}

\begin{itemize}
    \item Is the feature $\xj$ causal for the prediction?
    \begin{itemize}
      \item What if no association with the target right away?
    \end{itemize}
    \item Does the variable $\xj$ contain prediction-relevant information about $\ydat$ (that the model exploits)?
    \begin{itemize}
      \item inference possible depending on loss and model optimality
      \item always the model's perspective
    \end{itemize}
    \item Does the variable $\xj$ contain more prediction-relevant information about $\ydat$ than $\xv_S$ (that the model exploits)?
    \begin{itemize}
        \item no
    \end{itemize}
\end{itemize}

\end{vbframe}

\begin{vbframe}
  \printbibliography
\end{vbframe}

\endlecture
\end{document}
