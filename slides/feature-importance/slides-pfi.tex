\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\usepackage[]{graphicx}\usepackage[]{color}
% maxwidth is the original width if it is less than linewidth
% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
	\ifdim\Gin@nat@width>\linewidth
	\linewidth
	\else
	\Gin@nat@width
	\fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%
\let\hlipl\hlkwb

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
	\def\at@end@of@kframe{}%
	\ifinner\ifhmode%
	\def\at@end@of@kframe{\end{minipage}}%
\begin{minipage}{\columnwidth}%
	\fi\fi%
	\def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
		\colorbox{shadecolor}{##1}\hskip-\fboxsep
		% There is no \\@totalrightmargin, so:
		\hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
	\MakeFramed {\advance\hsize-\width
		\@totalleftmargin\z@ \linewidth\hsize
		\@setminipage}}%
{\par\unskip\endMakeFramed%
	\at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}
\newcommand{\SweaveOpts}[1]{}  % do not interfere with LaTeX
\newcommand{\SweaveInput}[1]{} % because they are not real TeX commands
\newcommand{\Sexpr}[1]{}       % will only be parsed by R

\usepackage[english]{babel}
\usepackage[utf8]{inputenc}

\usepackage{dsfont}
\usepackage{verbatim}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{bm}
\usepackage{csquotes}
\usepackage{multirow}
\usepackage{longtable}
\usepackage{booktabs}
\usepackage{enumerate}
\usepackage[absolute,overlay]{textpos}
\usepackage{psfrag}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{eqnarray}
\usepackage{arydshln}
\usepackage{tabularx}
\usepackage{placeins}
\usepackage{tikz}
\usepackage{setspace}
\usepackage{colortbl}
\usepackage{mathtools}
\usepackage{wrapfig}
\usepackage{bm}

\usetikzlibrary{shapes,arrows,automata,positioning,calc,chains,trees, shadows}
\tikzset{
	%Define standard arrow tip
	>=stealth',
	%Define style for boxes
	punkt/.style={
		rectangle,
		rounded corners,
		draw=black, very thick,
		text width=6.5em,
		minimum height=2em,
		text centered},
	% Define arrow style
	pil/.style={
		->,
		thick,
		shorten <=2pt,
		shorten >=2pt,}
}

\usepackage{subfig}

% Defines macros and environments
\input{../../style/common.tex}

%\usetheme{lmu-lecture}
\newcommand{\titlefigure}{figure_man/feature-importance.png}
\newcommand{\learninggoals}{
	\item Underdstand how PFI is computed
	\item Understanding strengths and weaknesses
	\item Testing Importance}
\usepackage{../../style/lmu-lecture}

\let\code=\texttt
\let\proglang=\textsf

\setkeys{Gin}{width=0.9\textwidth}

\title{Feature Importance}
% \author{Bernd Bischl, Christoph Molnar, Daniel Schalk, Fabian Scheipl}
\institute{\href{https://compstat-lmu.github.io/lecture_i2ml/}{compstat-lmu.github.io/lecture\_i2ml}}
\date{}

\setbeamertemplate{frametitle}{\expandafter\uppercase\expandafter\insertframetitle}

%\usepackage{Sweave}
\begin{document}
	
	% Set style/preamble.Rnw as parent.
	
	% Load all R packages and set up knitr
	
	% This file loads R packages, configures knitr options and sets preamble.Rnw as 
	% parent file
	% IF YOU MODIFY THIS, PLZ ALSO MODIFY setup.Rmd ACCORDINGLY...
	
	% Defines macros and environments
	
	\input{../../latex-math/basic-math.tex}
	\input{../../latex-math/basic-ml.tex}
	\input{../../latex-math/ml-interpretable.tex}
	
	\lecturechapter{Permutation Feature Importance}
	\lecture{Interpretable Machine Learning}
	
	% ------------------------------------------------------------------------------

\begin{vbframe}{Permutation Feature Importance}
\textbf{Idea:} Permuting a feature breaks the association between the feature and the target.\\
$\Rightarrow$ Model's prediction error should increase after permutation if the considered feature is important.

\begin{figure}
  \includegraphics[width=0.75\textwidth]{figure_man/feature-importance.png}
\end{figure}

\vspace{-0.2cm}
\tiny{Fisher et al. (2018). All models are wrong but many are useful: Variable importance for black-box, proprietary, or misspecified prediction models, using model class reliance. arXiv preprint arXiv:1801.01489 (2018).}

\framebreak
\normalsize

Performance-based permutation feature importance (PFI) for a feature $x_S$ using test data $\D$:
\begin{itemize}
  \item Measure the error \color{red}\textbf{with permuted feature values} \color{black} for $x_S$.
  \item Measure the error \color{blue}\textbf{without permuting features}\color{black}.
  \item Repeat permuting the feature (e.g., $m$ times) and average the difference of both errors: 
$$\widehat{PFI}_S = \tfrac{1}{m} \textstyle\sum\nolimits_{k = 1}^{m} \widehat{GE}(\fh, {\color{red}\D_{S}^{(k)}}) - \widehat{GE}(\fh, {\color{blue}\D})$$
\end{itemize}

Example of permuting feature $x_S$ with $S = \{1\}$:

\begin{center}
\includegraphics[width=0.75\textwidth]{figure_man/permuted-fv.png}
\end{center}

\vspace*{0.2cm}
{\scriptsize{Note: 
The $S$ in $x_S$ refers to a \textbf{S}ubset of features for which we are interested in their effect on the prediction.
As we calculate the feature importance for one feature at a time $|S| = 1$.}\par}

\framebreak 

Steps to calculate the PFI for a single feature $x_j$ according to the SIPA framework:
\lz
\begin{enumerate}
  \item \textbf{Perturbation:} Sample a perturbed version of the dataset $\pert{\D}{}{}$.
  \begin{itemize}
    \item replace the feature with a sample from the marginal $P(X_j)$, yielding $\pert{\D}{j}{}$
    \item marginal sampling can be approximated by permuting the feature vector $\xj$
    \end{itemize}
  \item \textbf{Prediction:} Make predictions for both i.e., $\D$ and $\pert{\D}{j}{}$.
  \item \textbf{Aggregation:} 
    \begin{itemize}
      \item Compute the loss for each observation in both data sets.
      \item Take the difference of both losses $\Delta L$ for each observation.
      \item For global interpretation: Average this change in loss across all observations.
    \end{itemize}
\end{enumerate}
\lz
\end{vbframe}


\frame{

\frametitle{Permutation Feature Importance}

  \only<1-3,5-7>{$\hspace{36pt}{\color{white}\widehat{GE}(\fh, {\color{red}\D_{S}^{(k)}}) - \widehat{GE}(\fh, {\color{blue}\D})}$}
  \only<4>{$\hspace{36pt}\widehat{GE}(\fh, {\color{red}\D_{S}^{(k)}}) - \widehat{GE}(\fh, {\color{blue}\D}),$ where 
$\scriptstyle\widehat{GE}(\fh, \D) =$ \scalebox{0.7}{$\frac{1}{n} \sum\nolimits_{(x, y) \in \D}$} $\scriptstyle L(\fh(x), y)$}
% $\scriptstyle\widehat{GE}(\fh, \D) =$ \scalebox{0.7}{$\frac{1}{n} \sum\nolimits_{(x, y) \in \D}$} $\scriptstyle L(\fh(x), y)$}
%   \only<5-6>{$\hspace{36pt}{\color{white}\widehat{GE}(\fh, {\color{red}\D_{S}^{(k)}}) - \widehat{GE}(\fh, {\color{blue}\D})}$}
  
  \begin{center}
  \only<1>{\includegraphics[page=2, trim=0pt 5pt 0 66pt, clip, width=0.88\textwidth]{figure_man/pfi_demo2}}%
  \only<2>{\includegraphics[page=3, trim=0pt 5pt 0 66pt, clip, width=0.88\textwidth]{figure_man/pfi_demo2}}%
  \only<3-4>{\includegraphics[page=4, trim=0pt 5pt 0 66pt, clip, width=0.88\textwidth]{figure_man/pfi_demo2}}%
  \only<5>{\includegraphics[page=5, trim=0pt 5pt 0 66pt, clip, width=0.88\textwidth]{figure_man/pfi_demo2}}%
  \only<6>{\includegraphics[page=6, trim=0pt 5pt 0 66pt, clip, width=0.88\textwidth]{figure_man/pfi_demo2}}%
  \only<7>{\includegraphics[page=7, trim=0pt 5pt 0 66pt, clip, width=0.88\textwidth]{figure_man/pfi_demo2}}%
  \end{center}
  
  \begin{itemize}
    \only<1-2>{\item[1.]\textbf{Sampling:} Sample feature values from the distribution of $x_S$. \newline $\Rightarrow$ Randomly permute feature $x_S$ (retains the distribution).
        \item[2.] \textbf{Intervention:} Replace the original feature with the permuted feature and create data with permuted feature   $\D_{S}$.}
    \only<2>{\item[3.] \textbf{Prediction:} Make predictions for both data, i.e., $\D$ and $\D_{S}$.}
    \only<3-4>{\item[4.] \textbf{Aggregation:}
      \begin{itemize}
        \item Compute the loss for each observation in both data sets.
      \end{itemize}}
    \only<5>{\item[4.] \textbf{Aggregation:}
      \begin{itemize}
        \item Compute the loss for each observation in both data sets.
       \item Take the difference of both losses $\Delta L$ for each observation.
      \end{itemize}}
     \only<6>{\item[4.] \textbf{Aggregation:}
      \begin{itemize}
        \item Compute the loss for each observation in both data sets.
        \item Take the difference of both losses $\Delta L$ for each observation.
        \item Average this change in loss across all observations.
      \end{itemize}}
    \only<7>{\item[4.] \textbf{Aggregation:}
      \begin{itemize}
        \item Compute the loss for each observation in both data sets.
        \item Take the difference of both losses $\Delta L$ for each observation.
        \item Average this change in loss across all observations.
        \item Also, average over multiple repetitions, if available.
      \end{itemize}}
  \end{itemize}
}

\begin{vbframe}{Bike Sharing Dataset}

\begin{center}
\includegraphics[width=0.75\textwidth]{figure_man/bike-sharing02.png}
\end{center}

\begin{itemize}
 \item The year is the most important feature.
 \item Interpretation: Destroying the information about the year by shuffling this feature increases the mean absolute error of the model by an amount of 816.
 \item Additionally, the $5 \%$ and $95 \%$ quantile of the repetitions are shown as error bars.
\end{itemize}
\end{vbframe}

\begin{vbframe}{Permutation Feature Importance}
 \begin{itemize}
 \itemsep1em
  \item Interpretation: PFI displays the increase of the model error when the feature's information is destroyed.
  \item PFI produces a single importance value per feature. \\
  $\Rightarrow$ It is a global interpretability method.
  \item Results can be unreliable due to random permutations. \\
  $\Rightarrow$ Solution: Average results over multiple repetitions.
  \item Permuting features despite correlation with other features can lead to unrealistic combinations of feature values.
  \item Feature importance automatically includes the importance of interaction effects with other features.
 \end{itemize}
\end{vbframe}

\begin{vbframe}{Permutation Feature Importance: Extrapolation}
 \begin{itemize}
 \itemsep1em
  \item Permuting features despite correlation with other features can lead to unrealistic combinations of feature values.
  \item Example from conditional subgroups paper Christoph
  \item Refer to CFI/SAGE values for non-extrapolating variants
 \end{itemize}
\end{vbframe}

\begin{vbframe}{Permutation Feature Importance: Interactions}
 \begin{itemize}
 \itemsep1em
  \item Feature importance automatically includes the importance of interaction effects with other features.
  \item Example
  \begin{itemize}
    \item two variables, only interaction; further variable no interaction
    \item $Y = \frac{1}{2} \sqrt{X_1 \cdot X_2} + \frac{1}{2} X_3$
    \item both variables are fully attributed with the interaction term
    \item overestimation of the importance of $X_1$ and $X_2$?
 \end{itemize}
 \item refer to SAGE/Shapley for fair attribution
 \end{itemize}
\end{vbframe}

\begin{vbframe}{Testing Importance (PIMP)}

\begin{itemize}
  \item PIMP was originally introduced for random forest feature importance.
  \item It fixes the problem that the importance measure prefers features with many categories.
  \item PIMP answers the questions when the importance significantly differs from 0. 
  \item It computes the distribution of importances under the $H_0$-hypothesis that the feature has no influence.
  \item Sampling under $H_0$ is achieved by permuting the $\ydat$-vector and retraining the model.
  \item We now rescale the importance to a p-value - the tail probability under $H_0$ - as a new importance score. 
\end{itemize}

{\tiny{Altmann, André, et al. "Permutation importance: a corrected feature importance measure." 
Bioinformatics 26.10 (2010): 1340-134.}}

\framebreak

PIMP algorithm:
\begin{enumerate}
	\item For $m \in \{1, \ldots, n_{repetitions}\}$:
		\begin{itemize}
			\item Permute response vector $\ydat$.
			\item Retrain model with data $\Xmat$ and permuted $\ydat$.
			\item Compute feature importance $PFI_j^m$ for each feature $j$.
		\end{itemize}
	\item Train model with $\Xmat$ and unpermuted $\ydat$.
	\item For each feature $j \in \{1,\ldots,p\}$:
		\begin{itemize}
			\item Fit probability distribution of the feature importance values $PFI_j^m$, $m \in \{1, \ldots, n_{repetitions}\}$ (choice between Gaussian, lognormal, gamma or non-parametric).
			\item Compute permutation feature importance $PFI_j$ for the model without permutation.
			\item Retrieve the p-value of $PFI_j$ based on the fitted distribution.
		\end{itemize}
\end{enumerate}
\end{vbframe}

\begin{vbframe}{Excurs: Multiple testing problem}
\begin{itemize}
  \item When should we reject the $H_0$-hypothesis for a feature? 
  \item The larger $p$, the more tests need to performed by PIMP. 
  \item This can lead to the \textbf{multiple testing problem}: If one does not take the multiplicity of tests into account, then the probability that some of the true null hypotheses are rejected (type-I error) by chance alone may be unduly large.
  \item Accounting for multiplicity of individual tests can be achieved by controlling
  an appropriate error rate like the \textbf{familywise error rate} (FWE, the probability of at least one type-I error). 
  \item One classical method to control the FWE is the \textbf{Bonferroni correction} which rejects a null hypothesis if its p-value is smaller than $\alpha/p$ with $p$ as the number of tests. 
  \item We refer to other lectures like \textit{Schätzen \& Testen I} for more details.
  \end{itemize} 
  \vspace{0.2cm}
  {\tiny{Romano, J. P., Shaikh, A. M., and Wolf, M. (2010). Multiple Testing. The New Palgrave Dictionary of Economys. \url{https://home.uchicago.edu/~amshaikh/webfiles/palgrave.pdf}}\par}
\end{vbframe}

\begin{vbframe}{Potential questions}

\begin{itemize}
  \item Model audit
  \begin{itemize}
    \item \textbf{Does the model (mechanistically) rely on feature $\xj$}?
    \item Does prediction share information with $\xj$?
    \item Does feature $\xj$ contribute information that isn't already contained in features $\xv_S$?
  \end{itemize}
  \item Inference
  \begin{itemize}
    \item \textbf{Does $\xj$ share information with $\ydat$}
    \item Does $\xj$ share information with $\ydat$ given $\xv_S$
    \item Is $\xj$ causal for $\ydat$?
  \end{itemize}
\end{itemize}

\end{vbframe}


\endlecture
\end{document}
