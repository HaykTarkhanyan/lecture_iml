\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}
\usepackage[]{graphicx}\usepackage[]{color}
% maxwidth is the original width if it is less than linewidth
% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%

% TODO: Move to latex-math
\newcommand{\xij}{x_j^{(i)}}


\let\hlipl\hlkwb

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}
\newcommand{\SweaveOpts}[1]{}  % do not interfere with LaTeX
\newcommand{\SweaveInput}[1]{} % because they are not real TeX commands
\newcommand{\Sexpr}[1]{}       % will only be parsed by R

\usepackage[english]{babel}
\usepackage[utf8]{inputenc}

\usepackage{dsfont}
\usepackage{verbatim}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{bm}
\usepackage{csquotes}
\usepackage{multirow}
\usepackage{longtable}
\usepackage{booktabs}
\usepackage{enumerate}
\usepackage[absolute,overlay]{textpos}
\usepackage{psfrag}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{eqnarray}
\usepackage{arydshln}
\usepackage{tabularx}
\usepackage{placeins}
\usepackage{tikz}
\usepackage{setspace}
\usepackage{colortbl}
\usepackage{mathtools}
\usepackage{wrapfig}
\usepackage{bm}

\usetikzlibrary{shapes,arrows,automata,positioning,calc,chains,trees, shadows}
\tikzset{
  %Define standard arrow tip
  >=stealth',
  %Define style for boxes
  punkt/.style={
    rectangle,
    rounded corners,
    draw=black, very thick,
    text width=6.5em,
    minimum height=2em,
    text centered},
  % Define arrow style
  pil/.style={
    ->,
    thick,
    shorten <=2pt,
    shorten >=2pt,}
}

\usepackage{subfig}

% Defines macros and environments
\input{../../style/common.tex}

\input{../../latex-math/basic-ml.tex}
\input{../../latex-math/basic-math.tex}

%\usetheme{lmu-lecture}
\newcommand{\titlefigure}{figure_man/shapley_valuefct.png}
\newcommand{\learninggoals}{
\item See prediction as cooperative game
\item Learn about Shapley value to explain individual model predictions
}
\usepackage{../../style/lmu-lecture}

\let\code=\texttt
\let\proglang=\textsf

\setkeys{Gin}{width=0.9\textwidth}

\title{Interpretable Machine Learning}
% \author{Bernd Bischl, Christoph Molnar, Daniel Schalk, Fabian Scheipl}
\institute{\href{https://compstat-lmu.github.io/lecture_i2ml/}{compstat-lmu.github.io/lecture\_i2ml}}
\date{}

\setbeamertemplate{frametitle}{\expandafter\uppercase\expandafter\insertframetitle}

\begin{document}

\lecturechapter{Shapley Values for Local Explanations}
\lecture{Interpretable Machine Learning}

\begin{vbframe}{Shapley Values}
We can use Shapley values to explain individual predictions of a machine learning model $\fh$:
\begin{itemize}
  \item Players $\hat{=}$ feature values of i-the observation $\xij, j \in \pset$.
  \item Features collaborate to produce a prediction $\fh(\xi_1, \xi_2, \ldots, \xi_p)$.
  \item The value function / payout of coalition $S$ for observation $\xv$ is
  $$v(\xv_S) =  \fh_{S} (\xv_S) - \E (\fh(\xv))$$
  i.e., the difference of the marginal prediction of $\xv_S$ and the average prediction.
\item Reason for $ - \E(\fh(\xv))$ is that
\item The marginal prediction is $\fh_{S}(\xv_S) := \int_{X_C} \fh(\xv_S, X_C)d \P_{X_C}$
\item We have already seen the marginal prediction in action in the PDP.
\item By using the marginal prediction, we have defined what it means for features to be \enquote{missing} for the prediction: We remove it by integrating over its distribution.
\end{itemize}
\begin{center}
\vspace{-0.3cm}
\includegraphics[width=0.8\textwidth]{figure_man/shapley_valuefct}
\end{center}

\begin{itemize}
 \item Shapley values tell us what the payout of each feature is, i.e., how each feature contributes to the overall prediction of a specific observation.
    \item The Shapley value is the average marginal contribution of a feature towards the prediction \textbf{across all possible feature coalitions}.
    \item The sum of Shapley values over all features yields the difference between the average prediction of all data points (baseline) and the selected individual prediction.
  \end{itemize}
\end{vbframe}




\begin{vbframe}{Shapley Value - Definition}
\begin{itemize}
  \item The Shapley value of a feature $j$ is defined as:\\
  For $m = 1, \dots, M$ exhaustively select coalition $S_m \subseteq P \setminus j$  and calculate
    $$ \hat{\phi_{j}}(\xv) = \frac{1}{M} \sum_{m=1}^{M} \underbrace{\fh_{S_m \cup \{j \}}(\xv_{S_m \cup \{j \} }) - \fh_{S_m}(\xv_{S_m})}_{\text{marginal contribution of feature $j$}} $$
    \item The term $\E (\fh(\xv))$ drops due to the difference.
  \item Interpretation of Shapley value $\hat{\phi_{j}}(\xv)$ for feature $j$ and observation $\xv$:
  The feature value $\xv_{j}$ contributed $\hat{\phi_{j}}(\xv)$ towards the prediction $\fh(x)$ compared to the average prediction for the dataset.
   \item Contributions can be negative.
\end{itemize}
\lz
\tiny
Shapley, Lloyd S. 1953. $"$A Value for N-Person Games.$"$\\
\vspace{0.2cm}
Strumbelj, Erik, Igor Kononenko, Erik Strumbelj, and Igor Kononenko. 2014. $"$Explaining prediction models and individual predictions with feature contributions.$"$

\end{vbframe}


\begin{vbframe}{Revisited: Axioms for Fair Attributions}
  We take the general axioms for Shapley Values and apply it to predictions:
  \begin{itemize}
    \item \textbf{Efficiency}: Feature contributions add up to the (centered) prediction.
      $\sum\nolimits_{j=1}^p\phi_j=\hat{f}(x)-E_X(\hat{f}(X))$
    \item \textbf{Symmetry}: Two feature that contribute the same to the prediction get the same payout: \\
      $\fh_{S\cup\{j\}}(\xv_{S\cup\{j\}}) = \fh_{S\cup\{k\}}(x_{S\cup\{k\}})$ for all $S \subseteq P\setminus\{j,k\}$ then $\phi_{j}=\phi_{k}$
    \item \textbf{Dummy / Null Player}: Shapley value of a feature that does not influence the prediction is zero: \\
      $\fh_{S\cup\{j\}}(S\cup\{j\})=\fh_{S}(S)$ for all $S \subseteq P$ then $\phi_j=0$
    \item \textbf{Additivity}:  For a prediction with combined payouts (v1 and v2), the
      payout is the sum of payouts: $\phi_j(v1) + \phi_j(v2)$.
  \end{itemize}
How can we attribute outcomes so that these axioms are fulfilled?
\end{vbframe}



\begin{vbframe}{Estimation: A practical problem}
  \begin{itemize}
      \item Feature space is often high-dimensional.
      \item High-dimensionality is problematic for the (exact) Shapley value computation: For only 10 features, there are already $10! \approx 3.6$ million possible orderings of features.
      \item We have a similar problem with the estimation of the marginal prediction: Averaging over the entire dataset for each (sampled) coalition would be very expensive.
      \item The solution to both problems is sampling. We calculate the Shapley value over $M$ samples. For each sample, we sample one ordering of features and one data point to replace missing features.
      \item $M$ is a tradeoff between accuracy of the Shapley value and computational costs. The higher $M$, the closer we get to the true Shapley values, but the more costly the computation becomes.
  \end{itemize}
\end{vbframe}

\begin{vbframe}{Estimation Algorithm}
 DRAFT
\begin{center}
\includegraphics[width=0.85\textwidth]{figure_man/shapley-algorithm1.png}
\end{center}
  \tiny{Strumbelj, Erik, Igor Kononenko, Erik Strumbelj, and Igor Kononenko. 2014. $"$Explaining prediction models and individual predictions with feature contributions.$"$}
\framebreak


\begin{center}
  DRAFT
\includegraphics[width=0.85\textwidth]{figure_man/shapley-algorithm2.png}
\end{center}

\tiny{Strumbelj, Erik, Igor Kononenko, Erik Strumbelj, and Igor Kononenko. 2014. $"$Explaining prediction models and individual predictions with feature contributions.$"$}

\end{vbframe}

\begin{vbframe}{Estimation trick}


  The Shapley value can be estimated more efficiently when certain coalitions are always included in the computation, instead of relying on chance to sample them.
  \begin{itemize}
    \item The coalition with $S = \empty$ (i.e., $|S| = 0$) and $S = \{1, \ldots, p\} \setminus j$ have the highest weights in the Shapley value computation. By including them on purpose, the Shapley value becomes more stable with fewer samples. Sample weights have to be adapted for the sampled coalations afterwards.
    \item Intuition: Adding a feature to the empty coalition gives information about the first order effect of the feature, which is the effect without any interactions. Adding the feature value to the otherwise complete set of feature values gives us the information about ALL interaction effects with other features.
    \item For coaliation $S = \empty$, there are $0! (|P| - 0 - 1)! = 1 \cdot (|P| - 1)! = (p - 1)!$ orderings, which is the same for $S = P \setminus {j}$: $|P \setminus j|! (|P| - |P \setminus j| - 1)! = (p - 1)! (p - (p-1) - 1)! = (p-1)!$.
    \item An example with $p = 5$ features:
      \begin{itemize}
        \item There are $5! = 120$ orderings in total.
        \item In $(5 - 1)! = 24$ orderings, we added feature value $\xij$ to the empty set.
        \item In 24 orderings, we added the feature value to the otherwise full feature set.
        \item That means with just two sets, we can already get $\frac{48}{120} = 0.4$ of the contributions to the Shapley value.
      \end{itemize}
    \item Similarly, we could proceed with all coalitions of $S = \emptyset$ ($|S| = 1$) and $S = P \setminus j$ ($|S| = p - 1$)
    \item When some coalitions are added \enquote{manually}, and the rest are sampled, we have to adapt the weights: Let $w$ be the weight of the \enquote{manually} sampled coalitions, $\phi_j^1$ the Shapley value with only the manual contributions and $\phi_j^2$ the Shapley value with the sampled coalitions, then the Shapley value is: $w \cdot \phi_j^1 + (1 - w) \phi_j^2$.
  \end{itemize}

  \begin{center}
    \includegraphics[width=0.5\textwidth]{figure/shapley-weights}
  \end{center}

\end{vbframe}

\begin{vbframe}{Bike Sharing Dataset}

\begin{center}
\includegraphics[width=0.85\textwidth]{figure_man/bike-sharing03.png}
\end{center}

The plot shows the Shapley values for observation 200.
The difference between the model prediction of this observation and the average prediction of the data is fairly distributed among the features (i.e., 4514 - 4508).
The most positive effect had feature value temp=28.503349, with a contribution (increase of prediction) of + 350.
\end{vbframe}


\endlecture
\end{document}
