\documentclass[11pt,compress,t,notes=noshow, aspectratio=169, xcolor=table]{beamer}

\usepackage{../../style/lmu-lecture}
% Defines macros and environments
\input{../../style/common.tex}

\tikzset{main node/.style={rectangle,draw,minimum size=1cm,inner sep=4pt},}

\title{Interpretable Machine Learning}
% \author{LMU}
%\institute{\href{https://compstat-lmu.github.io/lecture_iml/}{compstat-lmu.github.io/lecture\_iml}}
\date{}

\def\firstrowcolor{}
\def\secondrowcolor{}
\def\thirdrowcolor{}
\def\fourthrowcolor{}

\usepackage{tikz}

\begin{document}

\newcommand{\titlefigure}{figure/ebm.jpg}
\newcommand{\learninggoals}{
% \item Model-based boosting with simple base learners
% \item Feature effect and importance in model-based boosting}
\item Motivation from GAM
\item Intelligible GAM
\item Accurate GAM + Pairwise Interactions
\item FAST feature interaction detection}

\lecturechapter{Explainable Boosting Machines}
\lecture{Interpretable Machine Learning}

\begin{frame}{Generalized Additive Models}
\textbf{Recall idea of GAMs}: $$g(\E (y \mid \xv)) = \theta_0 + f_1(x_1) + f_2(x_2) + \ldots + f_p(x_p)$$
\begin{itemize}
    \item One shape function (spline) for each feature $\leadsto$ Intelligible    
    \item Including non-linear effects
    \item Better than GLMs in accuracy
    \item \textbf{Explainable Boosting Machines} $\leftarrow$ GAM + Gradient Boosting + Tree ensembles \citebutton{Lou et al. 2012}{https://www.cs.cornell.edu/~yinlou/papers/lou-kdd12.pdf}\\$\leadsto$ Much more intelligible than unrestricted full complexity models \&\\ \:\:\:\:\:\:High accuracy close to unrestricted full complexity models
\end{itemize}
\end{frame}

\begin{frame}{Intelligible GAM - Algorithm Sketch - Part I}

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{slides//02_interpretable-models//figure/EBM_Step1.png}
    \label{fig:Intelligible EBM_Step1}
\end{figure}
\begin{itemize}
    \item Train a small bootstrapping tree with 2 to 4 leaves only using the first feature
    \item Update the residual w.r.t. the original target
    \item Go to the second feature
\end{itemize}
\end{frame}

\begin{frame}{Intelligible GAM - Algorithm Sketch - Part II}
\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{slides//02_interpretable-models//figure/EBM_Step2.png}
    \label{fig:Intelligible EBM_Step2}
\end{figure}
\begin{itemize}
    \item Train a small bagging tree only using the $\text{feat}_2$
    \item Update the residual; this residual takes into account both what was learned on the first tree and second tree for the first two features
    \item Go to $\text{feat}_3$
\end{itemize}

\end{frame}

\begin{frame}{Intelligible GAM - Algorithm Sketch - Part III}
\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{slides//02_interpretable-models//figure/EBM_Step3.png}
    \label{fig:Intelligible EBM_Step3}
\end{figure}
\begin{itemize}
    \item Go through every feature and keep updating the residual
    \item Grow a small tree on each feature but just one at a time
    \item Do $M$ iterations through these features until convergence (e.g. $M=10000$) \\
    $\leadsto$ keep the learning rate so small that feature ordering does not matter
\end{itemize}

\end{frame}

\begin{frame}{Intelligible GAM - Prediction}
\begin{itemize}
    \item Output: $M$ trees which were only trained on $\text{feat}_1$ \\
    \qquad\;\; + $M$ trees which were only trained on $\text{feat}_2$ \\
    \qquad\;\; $\cdots$ \\
    \qquad\;\; + $M$ trees which were only trained on $\text{feat}_p$
    \item Summarize the weighted predictions of $M$ trees for each value of $\text{feat}_1$ as a 2D graph
    \item Generate a 2D graph for every feature in the same way (trained in parallel)
\end{itemize}
\quad $\leadsto$ Intelligible GAM: A series of 2D graphs as a perfect summary of predictions
\begin{figure}
    \centering
    \includegraphics[width=1.3\linewidth]{slides//02_interpretable-models//figure/ebm_prediction.png}
    \label{fig:Intelligible ebm_prediction}
\end{figure}
\end{frame}

\begin{frame}{Intelligible GAM - Novelty and Extension}
\begin{itemize}
    \item In each boosting step, use a bagging tree limited in size (2-4 leaves) \begin{itemize}
        \item High accuracy, low variance, prevent overfitting
        \item Provides discrete shape functions
        \item Captures missing information in spline plots
        \item High intelligibility
    \end{itemize}
    \item Not as accurate as unrestricted full complexity model
    \item In the next step: \citebutton{Lou et al. 2013}{https://www.cs.cornell.edu/~yinlou/papers/lou-kdd13.pdf}
    \begin{itemize}
        \item Add pairwise feature interaction terms to the model
        \item Replace the 2-4 regions of small tree
        \item By a dynamic programming algorithm called FAST
    \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{Accurate GAM plus Pairwise Interactions}
\textbf{Generalized Additive Models plus Interactions}: $$g(\E (y \mid \xv)) = \theta_0 + \sum f_i(x_i) + \sum f_{ij}(x_i,x_j)\quad for\quad i,j=1,\cdots,p,\quad i\neq j$$
\begin{itemize}
    \item $O(p^2)$: a large number of pairwise interactions if $p$ is big\\
    $\leadsto$ Select only a small number of the most important interactions
    \item Intuitively, significant reduction in RSS $\to$ Strong pair interaction
    \item \textbf{FAST} algorithm does quick ranking without fully building every interaction function
\end{itemize}
\end{frame}

\begin{frame}{FAST feature interaction detection - PART I}

\begin{columns}[T, totalwidth=\textwidth]
\begin{column}{0.5\textwidth}
\begin{figure}
    \centering
    \includegraphics[width=0.6\linewidth]
    {slides//02_interpretable-models//figure/T_ij.png}
    \label{fig:tree-like predictor1}
\end{figure}
\begin{figure}
    \centering
    \includegraphics[width=\linewidth]
    {slides//02_interpretable-models//figure/FAST.png}
    \label{fig:FAST1}
\end{figure}
\end{column}
\hfill
\centering
\begin{column}{0.5\textwidth}
\begin{itemize}
    \item $T_{ij}$: A tree-like interaction predictor with 4 leaves for each pair of features
    \item $c_i$, $c_j$: One cut (grid point) on the sorted values of feature $x_i$ and $x_j$, respectively
    \item $A$, $B$, $C$, $D$: Notation for 4 leaf regions
    \item \textbf{Idea}: Search for all possible $(c_i, c_j)$ and pick the best $T_{ij}$ with the lowest RSS
    \item Prediction value on region r, $r\in\{A, B, C, D\}$:
    $$\hat{y}_r=T_{ij}.r=\frac{1}{\vert r\vert}\sum_{(\mathbf{x},y)\in r}y$$
\end{itemize}
\end{column}
\end{columns}

\end{frame}

\begin{frame}{FAST feature interaction detection - PART II}

\begin{columns}[T, totalwidth=\textwidth]
\begin{column}{0.5\textwidth}
\begin{figure}
    \centering
    \includegraphics[width=0.6\linewidth]
    {slides//02_interpretable-models//figure/T_ij.png}
    \label{fig:tree-like predictor2}
\end{figure}
\begin{figure}
    \centering
    \includegraphics[width=\linewidth]
    {slides//02_interpretable-models//figure/FAST.png}
    \label{fig:FAST2}
\end{figure}
\end{column}
\hfill
\centering
\begin{column}{0.5\textwidth}
\begin{itemize}
    \item How to quickly get sum of targets and sum of counts for all possible cuts?\\
    $\leadsto$ Use \textbf{marginal} cumulative histograms 
    \item Given certain $(c_i, c_j)$:\\
    $CH_i^t(c_i)$: cumulative sum of targets for points with $x_i\leq c_i$;\\
    $\overline{CH_j^w }(c_j)$: cumulative sum of counts for points with $x_j>c_j$
    \item $a$, $b$, $c$, $d$: Sum of targets (or counts) in the corresponding region
    \item Given any $(c_i, c_j)$, compute $a$ by dynamic programming
    \item $b$, $c$, $d$ can be easily derived with known $a$
\end{itemize}
\end{column}
\end{columns}

\end{frame}

\begin{frame}{FAST feature interaction detection - PART III}

\begin{itemize}
    \item For $x_i$ with $d_i$ possible values and $x_j$ with $d_j$ possible values:\\
    $\leadsto$ compute and save $d_id_j$ matrices of $\left(\begin{matrix}a & b \\c & d \end{matrix}\right)$ for both sum of targets and sum of counts
    \item For each pair, calculate RSS for each $\left(\begin{matrix}a & b \\c & d \end{matrix}\right)$ and pick the best $T_{ij}$ with the lowest RSS
\end{itemize}
\[
\text{\qquad$d_id_j$ matrices}
\left\{
\begin{array}{@{}l@{\quad}l@{\quad}l@{\quad}l@{\quad}l}
    \begin{tikzpicture}[baseline=(current bounding box.center)]
        \draw (0,0) rectangle (1.2,1.2);
        \draw (0.4,0) -- (0.4,1.2); 
        \draw (0,0.8) -- (1.2,0.8); 
        \node at (0.2,0.4) {c};
        \node at (0.8,0.4) {d};
        \node at (0.2,1) {a};
        \node at (0.8,1) {b};
    \end{tikzpicture} & \text{$\Rightarrow$} & \begin{tikzpicture}[baseline=(current bounding box.center)]
        \draw (0,0) rectangle (1.2,1.2);
        \draw (0.4,0) -- (0.4,1.2); 
        \draw (0,0.8) -- (1.2,0.8); 
        \node at (0.2,0.4) {$\hat{y}_C$};
        \node at (0.8,0.4) {$\hat{y}_D$};
        \node at (0.2,1) {$\hat{y}_A$};
        \node at (0.8,1) {$\hat{y}_B$};
    \end{tikzpicture} & \text{$\Rightarrow$} & \text{$RSS_1$}\\[0.7cm] 
    \begin{tikzpicture}[baseline=(current bounding box.center)]
        \draw (0,0) rectangle (1.2,1.2);
        \draw (0.6,0) -- (0.6,1.2); 
        \draw (0,0.6) -- (1.2,0.6); 
        \node at (0.3,0.3) {c};
        \node at (0.9,0.3) {d};
        \node at (0.3,0.9) {a};
        \node at (0.9,0.9) {b};
    \end{tikzpicture} & \text{$\Rightarrow$} & \begin{tikzpicture}[baseline=(current bounding box.center)]
        \draw (0,0) rectangle (1.2,1.2);
        \draw (0.6,0) -- (0.6,1.2); 
        \draw (0,0.6) -- (1.2,0.6); 
        \node at (0.3,0.3) {$\hat{y}_C$};
        \node at (0.9,0.3) {$\hat{y}_D$};
        \node at (0.3,0.9) {$\hat{y}_A$};
        \node at (0.9,0.9) {$\hat{y}_B$};
    \end{tikzpicture} & \text{$\Rightarrow$} & \text{$RSS_2$}\\[0.7cm] 
    \makebox[1cm][c]{\vdots} & & \makebox[1cm][c]{\vdots} & & \makebox[1cm][c]{\vdots}\\[0.3cm]
    \begin{tikzpicture}[baseline=(current bounding box.center)]
        \draw (0,0) rectangle (1.2,1.2);
        \draw (0.8,0) -- (0.8,1.2); 
        \draw (0,0.4) -- (1.2,0.4); 
        \node at (0.4,0.2) {c};
        \node at (1,0.2) {d};
        \node at (0.4,0.8) {a};
        \node at (1,0.8) {b};
    \end{tikzpicture} & \text{$\Rightarrow$} & \begin{tikzpicture}[baseline=(current bounding box.center)]
        \draw (0,0) rectangle (1.2,1.2);
        \draw (0.8,0) -- (0.8,1.2); 
        \draw (0,0.4) -- (1.2,0.4); 
        \node at (0.4,0.2) {$\hat{y}_C$};
        \node at (1,0.2) {$\hat{y}_D$};
        \node at (0.4,0.8) {$\hat{y}_A$};
        \node at (1,0.8) {$\hat{y}_B$};
    \end{tikzpicture} & \text{$\Rightarrow$} & \text{$RSS_{d_id_j}$}\\ 
\end{array}
\right\}
\text{\parbox[t]{6cm}{select the lowest RSS and assign it as the weight for $(x_i, x_j)$ to measure the strength of interaction}}
\]
\end{frame}

\begin{frame}{FAST feature interaction detection - PART IV}
\begin{itemize}
    \item Complexity analysis: \\
    Computing cumulative histograms: $O(N)$ ($N$: number of observations)\\
    Constructing matrices of sum values: $O(N+d_id_j)$\\
    $\leadsto$ Complexity per pair: $O(N+d_id_j)$ 
    \item Discretize continuous feature values in $b$ bins ($b\leq256$)\\
    $\leadsto O(N+b^2)$ per pair\\
    $\leadsto$ Choose optimal number of bins $b$ w.r.t. bias-variance tradeoff
    \item Quick ranking and select top K pairs
\end{itemize}
\end{frame}

\begin{frame}{Accurate GAM - Two-stage Construction}
\begin{enumerate}
    \item Fit Main Effects
    \begin{itemize}
        \item Fit model only using univariate terms
        \item Fix the main effects model
    \end{itemize}
    \item Fit Pairwise residual of main effects model to original targets
    \begin{itemize}
        \item Use FAST to sort through $O(p^2)$ pairs
        \item User selects number $K$ of pairs to be added
        \item Run boosting algorithm to fit $K$ pairs
        \item Final model = $p$ mains + $K$ pairs
    \end{itemize}
\end{enumerate}
\end{frame}

\begin{frame}{Accurate GAM - 2. Stage Algorithm Sketch}
\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{slides//02_interpretable-models//figure/GA2M_Step1.png}
    \label{fig:GA2M_Step1}
\end{figure}
\begin{columns}[T, totalwidth=\textwidth]
    \begin{column}{0.4\textwidth} 
        \begin{figure}
            \vspace{-1.35cm} 
            \hspace{-0.8cm} 
            \includegraphics[width=0.8\linewidth]{slides/02_interpretable-models/figure/GA2M_Step0.png}
            \label{fig:GA2M_predictor}
        \end{figure}
    \end{column}
    \hfill
    \begin{column}{0.7\textwidth}
        \begin{itemize}
            \setlength{\leftskip}{-0.7cm}
            \vspace{-0.8cm}
            \item To model the pairwise interactions effectively and efficiently\\
            $\leadsto$Replace the shallow tree ensembles used in Intelligible GAM with a tree-like predictor (similar to FAST)
            \item 3 cuts instead of 2 cuts
            \item Re-use the stored values in the matrices for sum of targets and sum of counts from FAST
            \item Greedy search for the best combination of cuts (lowest RSS)
        \end{itemize}
    \end{column}
\end{columns}

\end{frame}

\begin{frame}{Accurate GAM - Prediction}
\begin{itemize}
    \item Output: $M$ predictors which were only trained on $\text{Pair}_1$ \\
    \qquad\;\; + $M$ predictors which were only trained on $\text{Pair}_2$ \\
    \qquad\;\; $\cdots$ \\
    \qquad\;\; + $M$ predictors which were only trained on $\text{Pair}_K$
    \item Summarize the weighted predictions of $M$ models in a 3D heatmap
    \item The dimension of color represents the contribution of pairwise feature values to the prediction
    \item Generate a 3D heatmap for each pair of features
\end{itemize}

\begin{figure}    
    \includegraphics[width=0.4\linewidth]
    {slides/02_interpretable-models/figure/3D Heatmap.png}
    \label{fig:3D Heatmap}
\end{figure}

\end{frame}

\begin{frame}{EBM - Final Model}
\begin{figure}    
    \includegraphics[width=1\linewidth]
    {slides/02_interpretable-models/figure/final_ebm.png}
    \label{fig:final_ebm}
\end{figure}
\end{frame}

\end{document}