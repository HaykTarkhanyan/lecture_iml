\documentclass[11pt,compress,t,notes=noshow, aspectratio=169, xcolor=table]{beamer}

\usepackage{../../style/lmu-lecture}
% Defines macros and environments
\input{../../style/common.tex}

\title{Interpretable Machine Learning}
% \author{LMU}
%\institute{\href{https://compstat-lmu.github.io/lecture_iml/}{compstat-lmu.github.io/lecture\_iml}}
\date{}

\def\firstrowcolor{}
\def\secondrowcolor{}
\def\thirdrowcolor{}
\def\fourthrowcolor{}

\definecolor{winter}{RGB} {243,117,108}
\definecolor{spring}{RGB} {121,174,65}
\definecolor{summer}{RGB} {25,188,195}
\definecolor{fall}{RGB} {166,128,185}
\begin{document}

\newcommand{\titlefigure}{figure/whitebox}
\newcommand{\learninggoals}{
\item Interpretation of main effects in LM
\item Inclusion of high-order and interaction effects
\item Regularization via LASSO
%\item Examples of popular interpretable models
%\item Properties of some interpretable models
%\item Focus on how to interpret them, not on math details
}

\lecturechapter{Linear Regression Model}
\lecture{Interpretable Machine Learning}




\begin{frame}[c]{Linear Regression}
% https://towardsdatascience.com/assumptions-of-linear-regression-fdb71ebeaa8b
% https://www.statology.org/linear-regression-assumptions/
% https://link.springer.com/book/10.1007/978-3-642-34333-9
%\textbf{Model formula}
    %$$\mathbb{E}_Y(Y \vert X) = \theta_0 + \theta_1 x_1 + \theta_2 x_2 + \ldots + \theta_p x_p + \epsilon = X^T\mathbf{\theta} + \epsilon$$

\begin{columns}[T, totalwidth = \linewidth]
\begin{column}{0.58\linewidth}
$$y = \theta_0 + \theta_1 x_1 + \theta_2 x_2 + \dots + \theta_p x_p + \epsilon = \xv^\top \boldsymbol{\theta} + \epsilon$$

 \begin{itemize}
        %\item $\mathbb{E}_Y(Y \vert X)$ expected value of target given features $X$
        \item $y$: target / output
        \item $\epsilon$: remaining error / residual (e.g., due to noise)
        \item $\theta_j$: weight of input feature $x_j$ (intercept $\theta_0$)\\
        $\leadsto$ model consists of $p+1$ weights
        %\item Model equation is additive and identical across entire input space
        %\pause
        % \item Polynomial regression extends equation above by
        % \begin{itemize}
        % \item \textbf{higher order main effects} which have their own weights (e.g., quadratic: $\theta_{x_j^2} \cdot x_j^2$)
        % \item \textbf{interaction effects} (e.g., 2-way interaction: $\theta_{x_i, x_j} \cdot x_i \cdot x_j$)
        % \end{itemize}
    \end{itemize}
\end{column}
\begin{column}{0.42\linewidth}
\includegraphics[width=\linewidth]{figure/lm_example.pdf}
\end{column}
\end{columns}
   
   \vspace*{0.2cm} 
   \pause
    \textbf{Properties and assumptions} \citebutton{Faraway (2002), Ch. 7}{https://cran.r-project.org/doc/contrib/Faraway-PRA.pdf} \citebutton{Checking assumptions in R \& Python}{https://towardsdatascience.com/assumptions-of-linear-regression-fdb71ebeaa8b}
    \begin{itemize}[<+->]
    \item \textbf{Linear} relationship between features and target
    %Predictions are \textbf{linear} combination of features $\leadsto$ 
    %Model equation is additive and linear w.r.t. features% and identical across entire input space
    \item $\epsilon$ and $y \vert \xv$ are \textbf{normally} distributed with \textbf{constant variance} (homoscedastic)\\
    $\leadsto$ $\epsilon \sim N(0, \sigma^2) \; \Rightarrow \; (y \vert \xv) \sim N(\xv^\top \theta, \sigma^2)$\\
    $\leadsto$ if violated, inference-based metrics (e.g., p-values) are invalid
    %\item Error terms are assumed to have a \textbf{constant variance} over the entire feature space %(homoscedasticity)
    \item Independence of observations (e.g., no repeated measurements) %(e.g., no autocorrelated error terms)
    \item Independence of features $x_j$ with error term $\epsilon$
    \item No or little multicollinearity (i.e., no strong feature correlations)
    % free of measurement error assumption: https://indigo.uic.edu/articles/thesis/Measurement_Error_in_Generalized_Linear_Models/17025971/1/files/31488719.pdf
    % \item Note: For inference-based metrics (t-statistic, p-values, confidence intervals) to be valid, the error term needs to be normally distributed, i.e., $\epsilon \sim N(0, \sigma^2) \; \Rightarrow \; (y \vert x) \sim N(x^T \theta, \sigma^2)$\\
%$\leadsto$ Restricts use of LMs in practice as distribution of error is a prior assumption about data
        % \item Properties and assumptions:
        % \begin{itemize}
        %     \item linear
        %     \item normality assumption of the target % not true...
        %     \item homoscedastic (i.e., constant variance)
        %     \item independence of features
        %     \item fixed features (i.e., free of noise)
        %     \item no strong correlation of features
        % \end{itemize} 
    \end{itemize}

\end{frame}

%------------------------------------------------------------------
%------------------------------------------------------------------

\begin{frame}[c]{Linear Regression - Interpretation}

$$y = \theta_0 + \theta_1 x_1 + \theta_2 x_2 + \dots + \theta_p x_p + \epsilon = \xv^\top \thetab + \epsilon$$

    Interpretation of weights (\textbf{feature effects}) depend on type of feature:
    \begin{itemize}[<+->]
        \item \textbf{Numerical $x_j$}: Increasing $x_j$ by one unit changes outcome by $\theta_j$, ceteris paribus \\ (\textit{ceteris paribus} (c.p.) means "everything else held constant".)
        \item \textbf{Binary $x_j$}: Weight $\theta_j$ is active or not (multiplication with 1 or 0) where 0 is reference category
    \end{itemize}

\begin{columns}[T, totalwidth = \linewidth]
\begin{column}{0.58\linewidth}
    \begin{itemize}
        \item<3-> %Categorical: One-hot-encoding of $L-1$ new features for $L$ categories (dummy encoding) \\
        \textbf{Categorical $x_j$ with $L$ categories}: Create $L-1$ one-hot-encoded features $x_{j,1}, \hdots, x_{j,L-1}$ (each having its own weight), left out category is reference ($\hat =$ dummy encoding)
        %$x_{j,l}, \forall l \in \{1, \hdots, L-1\}$ (with weight $\theta_{j,l}$), left out category is reference ($\hat =$ dummy encoding)
        \\
        $\leadsto$ Interpretation:
        Outcome changes by $\theta_{j,l}$ for category $l$ compared to reference cat.,  c.p.
        % (for any of the $L-1$ categories):
        %Predicted outcome changes for $l$-th category compared to the reference category by its weight $\theta_{j,l}$ c.p.
        %Predicted outcome changes for category $x_{j,l}$ compared to the reference category by $\theta_j$ c.p.
        \item<4> \textbf{Intercept $\theta_0$}: Expected outcome if all feature values are set to 0 %(baseline) %reflects expected features values if features were standardised (0-mean, 1-stdev)
        %\item Note: In case of higher order or interaction effects, weights cannot be interpreted in isolation
    \end{itemize}
\end{column}
\begin{column}{0.42\linewidth}
\begin{center}
        \includegraphics[width = 0.8\textwidth]{figure/reg_lm_plot_interpreted.pdf}\\
        % Bild aus I2ML: supervised-regression
        %figure/plot_lin_effect.pdf} \\
        %Boxplot of $\hat\theta_j x_j$-values (scale invariant)
        %$\leadsto$ Comparable feature contributions w.r.t. $\hat y$
    \end{center}
\end{column}
\end{columns}
    
\end{frame}

     
\begin{frame}{Linear Regression - Interpretation}
\vspace{-0.2cm}
$$y = \theta_0 + \theta_1 x_1 + \theta_2 x_2 + \dots + \theta_p x_p + \epsilon = \xv^\top \thetab + \epsilon$$
    \textbf{Feature importance}:
    \begin{columns}[T, totalwidth=\linewidth]
    \begin{column}{0.6\textwidth}
    \begin{itemize}
        \item Absolute \textbf{t-statistic} value: $\hat\theta_j$ scaled with its standard error ($SE(\hat\theta_j)$ $\hat =$ reliability of the estimate) 
    $$|t_{\hat\theta_j}| = \left| \tfrac{\hat\theta_j}{SE(\hat\theta_j)} \right|$$
        \item High values indicate important (i.e. significant) features
    \end{itemize}
    \end{column}
    \begin{column}{0.4\textwidth}
    \vspace{-0.2cm}
    \begin{center}
        \includegraphics[width=.9\textwidth]{figure/t_stat.pdf} 
    \end{center}
    \end{column}
    \end{columns}
    \vspace{0.3cm}
    \only<2->{
    \begin{columns}[T, totalwidth=\linewidth]
    \begin{column}{0.6\textwidth}
    \begin{itemize}
        \item \textbf{p-value}: probability of obtaining a test statistic that is more extreme (values that speak against $H_0$) as the test statistic computed from the sample, assuming $H_0$ (here: $\theta_j =0)$ is correct.
        \item The smaller the p-value, the less likely it is to obtain a more extreme test statistic.
    \end{itemize}
    \end{column}
    \begin{column}{0.4\textwidth}
    \vspace{-0.3cm}
    \begin{center}
        \includegraphics[width=.9\textwidth]{figure/p-value.pdf}
    \end{center}
    \end{column}
    \end{columns}}
\end{frame}


\begin{frame}{Example: Linear Regression - Main Effects}

\textbf{Bike data}: predict no. of rented bikes using 4 numeric, 1 categorical feat. (season)

\medskip 

% \begin{footnotesize}
% $$
% \hat y = \hat \theta_0 + \hat \theta_1 \mathds{1}_{(seas = SPRING)} + \hat \theta_2 \mathds{1}_{(seas = SUMMER)} + \hat \theta_3 \mathds{1}_{(seas = FALL)} + \hat \theta_4 temp + \hat \theta_5 hum + \hat \theta_6 windspeed + \hat \theta_7 days\_since\_2011
% $$
% \end{footnotesize}
\begin{columns}[T, totalwidth=\linewidth]
\begin{column}{0.43\linewidth}
\vspace*{-0.3cm}
\begin{align*}
\hat y = 
& \hat \theta_0 + 
\hat \theta_1 \mathds{1}_{x_{season} = SPRING} +\\
& 
\hat \theta_2 \mathds{1}_{x_{season} = SUMMER} +\\
& 
\hat \theta_3 \mathds{1}_{x_{season} = FALL} + 
\hat \theta_4 x_{temp} + \\
& 
\hat \theta_5 x_{hum} + 
\hat \theta_6 x_{windspeed} + \\
& 
\hat \theta_7 x_{days\_since\_2011}
\end{align*}
\end{column}
\begin{column}{0.57\linewidth}
  \centering
\begin{scriptsize}
%\begin{overlayarea}{\textwidth}{\textheight}
%\begin{table}[ht]
\only<2>{\def\firstrowcolor{\rowcolor{lightgray}}}
\only<3>{\def\secondrowcolor{\rowcolor{lightgray}}}
\only<4>{\def\thirdrowcolor{\rowcolor{lightgray}}}
\begin{tabular}{rrrrr}
  \hline
 & Weights & SE & t-stat. & p-val. \\
 \hline
\firstrowcolor (Intercept) & 3229.3 & 220.6 & 14.6 & 0.00 \\ 
\secondrowcolor seasonSPRING & 862.0 & 129.0 & 6.7 & 0.00 \\ 
  seasonSUMMER & 41.6 & 170.2 & 0.2 & 0.81 \\ 
  seasonFALL & 390.1 & 116.6 & 3.3 & 0.00 \\ 
\thirdrowcolor temp & 120.5 & 7.3 & 16.5 & 0.00 \\ 
  hum & -31.1 & 2.6 & -12.1 & 0.00 \\ 
  windspeed & -56.9 & 7.1 & -8.0 & 0.00 \\ 
  days\_since\_2011 & 4.9 & 0.2 & 26.9 & 0.00 \\
   \hline
\end{tabular}
%\end{table}
%\end{overlayarea}
\end{scriptsize}
\end{column}
\end{columns}
%\vspace*{-0.3cm}
\pause

\lz
%\begin{columns}[T, totalwidth=\linewidth]
%\begin{column}<5>{0.46\textwidth}
%\textbf{Vis.}: Boxplot of $\hat\theta_j x_j$-values (scale invariant)\\
%weights multiplied by actual feature value (better comparable due to different scales)
%$\leadsto$ Comparable feature contributions w.r.t. $\hat y$

%\includegraphics[width = \textwidth]{figure/plot_lin_effect.pdf}
%   \begin{center}
%   % Effect of $i$-th observation $= \theta_j x_j^{(i)}$\\

%     %$\leadsto$ Better comparability due to different scales
%   \end{center}
%\pause
%\verbatiminput{figure/lm_output.txt}
%\end{column}\hfill
%\begin{column}{0.54\textwidth}  %%<--- here

\begin{itemize}[<+->]
    \item \textbf{Interpretation intercept}:
    If all feature values are 0 (and season is \code{WINTER} $\hat =$ reference cat.), the expected number of bike rentals is $\hat\theta_0 = 3229.3$
    \item \textbf{Interpretation categorical}: Rentals in \code{SPRING} are by $\hat\theta_1 = 862$ higher than in \code{WINTER}, c.p.
    \item \textbf{Interpretation numerical}: Rentals increase by $\hat\theta_4 = 120.5$ if \code{temp} increases by 1 $^{\circ}$C, c.p.
    %If the temperature increases by 1 order Celsius, the number of bike rentals increases by 120.5 c.p.

\end{itemize}
%\end{column}
%\end{columns}
\end{frame}


%------------------------------------------------------------------
%------------------------------------------------------------------

\begin{frame}{Interaction and High-order Effects}

$$y = \theta_0 + \theta_1 x_1 + \theta_2 x_2 + \dots + \theta_p x_p + \epsilon$$

Equation above can be extended (polynomial regression) by including

\begin{columns}[T, totalwidth=\linewidth]
\begin{column}{0.65\linewidth}
    \begin{itemize}
        \item \textbf{high-order effects} which have their own weights\\
        $\leadsto$ e.g., quadratic effect: $\theta_{x_j^2} \cdot x_j^2$
        \item \textbf{interaction effects} as the product of multiple feat.\\
        $\leadsto$ e.g., 2-way interaction: $\theta_{x_i, x_j} \cdot x_i \cdot x_j$
    \end{itemize}
\end{column}
\begin{column}{0.34\linewidth}
    \vspace{-0.2cm}
    \centering
    \begin{scriptsize}
    \begin{table}[ht]
    \textbf{Bike Data}
        \begin{tabular}{lrr}
        \hline
        Method & $R^2$ & adj. $R^2$ \\ 
        \hline
        Simple LM & 0.85 & 0.84 \\ 
        High-order & 0.87 & 0.87 \\ 
        Interaction  & 0.96 & 0.93 \\ 
    %    Both Types & 0.96 & 0.93 \\ 
        \hline
        \end{tabular}
    \end{table}
    \end{scriptsize}
    % code for tables in rsrc/tab_lm_comparison.R
\end{column}
\end{columns}

\pause
Implications of including high-order and interaction effects: 
\begin{itemize}
    \item Both make the model more flexible but also less interpretable\\
    $\leadsto$ More weights to interpret
    \item Both need to be specified manually (inconvenient and sometimes infeasible)\\
    $\leadsto$ Other ML models learn them often automatically
    \item Marginal effect of a feature cannot be interpreted by single weights anymore\\
$\leadsto$ Feature $x_j$ occurs multiple times (with different weights) in equation
\end{itemize}

%Weights cannot be interpreted in isolation as before
% $$y = \theta_0 + \theta_1 x_1 + \theta_2 x_2 + \dots + \theta_p x_p + \epsilon$$

% \begin{itemize}
% \item Results in polynomial regression 
% \item Weights cannot be interpreted in isolation as before
% \end{itemize}
\end{frame}

% \begin{frame}{Linear and Polynomial Regression}

% \begin{align*}
% \mathbb{E}_Y(Y \vert X) &= \theta_0 + \theta_1 x_1 + \dots + \theta_p x_p + \epsilon \\
%  &= X^T\theta + \mathcal{E}
% \end{align*}

% \begin{itemize}
% \itemsep1em
% \item Model equation is identical across the entire feature space.
% %\item The predictive power of LMs is determined by specifying the correct model structure.
% \item Polynomial regression extends the LM by non-linear effects.
% %A polynomial regression model is an extension of the LM that includes higher order terms or interactions.
% %This enables us to model non-linear data while making use of the entire arsenal of LM functionality.
% \item We can exactly determine feature effects (e.g., beta coefficients, effect plots) and importance scores (e.g., p-values, t-statistics).
% %By knowing the model equation, we can exactly determine feature effects (e.g., beta coefficients, effect plots) and importance scores (e.g., p-values, t-statistics).
% \item For higher order effects or interactions, beta coefficients cannot be interpreted in isolation.
% \item Note: For inference-based metrics (p-values, confidence intervals) to be valid, error term needs to be normally distributed with zero mean, i.e., $\epsilon \sim N(0, \sigma^2) \; \Rightarrow \; (y \vert x) \sim N(x^T \theta, \sigma^2)$.\\
% $\leadsto$ Restricts use of LMs in practice as distribution of error is a prior assumption about data.
% \end{itemize}
% \end{frame}

\begin{frame}{Example: Interaction Effect}
\textbf{Example}: Interaction between \code{temp} and \code{season} will affect marginal effect of \code{temp}% with (right) and without (left) interaction.
\begin{columns}[T, totalwidth=\linewidth]
\begin{column}{0.65\linewidth}
\includegraphics[width = \textwidth]{figure/lm_main_vs_interaction_effects.pdf}
\end{column}
\begin{column}{0.35\linewidth}
\begin{scriptsize}
%\begin{table}[ht]
\only<2->{\def\firstrowcolor{\rowcolor{lightgray}}}
\only<3>{\def\secondrowcolor{\rowcolor{lightgray}}}
\only<4>{\def\thirdrowcolor{\rowcolor{lightgray}}}
\only<5>{\def\fourthrowcolor{\rowcolor{lightgray}}}
\centering
\begin{tabular}{rr}
  \hline
 & Weights \\ 
  \hline
(Intercept) & 3453.9 \\ 
  seasonSPRING & 1317.0 \\ 
  seasonSUMMER & 4894.1 \\ 
  seasonFALL & -114.2 \\ 
  \firstrowcolor temp & 160.5 \\ 
  hum & -37.6 \\ 
  windspeed & -61.9 \\ 
  days\_since\_2011 & 4.9 \\
  \hline
  \secondrowcolor seasonSPRING:temp & -50.7 \\ 
  \thirdrowcolor seasonSUMMER:temp & -222.0 \\ 
  \fourthrowcolor seasonFALL:temp & 27.2 \\ 
   \hline
\end{tabular}
%\end{table}
\end{scriptsize}
\end{column}
\end{columns}
\vfill
\pause
\begin{columns}[T, totalwidth=\linewidth]
\begin{column}{0.65\linewidth}
\textbf{Interpretation}: If \code{temp} increases by 1 $^{\circ}$C, bike rentals
\begin{itemize}[<+->]
    \item increase by 160.5 in \code{WINTER} (reference)
    \item increase by 109.8 (= 160.5 - 50.7) in \code{SPRING}
    \item decrease by -61.5 (= 160.5 - 222) in \code{SUMMER}
    \item increase by 187.7 (= 160.5 + 27.2) in \code{FALL}
\end{itemize} %\\\vspace*{0.2cm}
\end{column}
\begin{column}{0.35\linewidth}
% \lz
% \only<6>{\textbf{Note:}
% Temperature values (on x-axis) depend on \code{season}\\
% $\leadsto$ not acknowledged by LM\\
% $\leadsto$ misleading interpretation due to extrapolation in regions with few points (e.g. low \code{temp} in \code{SUMMER})
% }
\end{column}
\end{columns}
%Value ranges of temperature differ depending on the season $\leadsto$ not acknowledged by LM
\end{frame}


\begin{frame}{Example: Linear Regression - Quadratic Effect}
\textbf{Example}: Adding quadratic effect for \code{temp} \only<2>{(left) and an interaction with \code{season} (right)}
\begin{columns}[T, totalwidth=\linewidth]
\begin{column}{0.65\linewidth}
\includegraphics[width=0.5\textwidth, trim=0cm 0.1cm 10.4cm 0cm, clip]{figure/poly_main_vs_interaction_effects.pdf}\invisible<1>{\includegraphics[width=0.5\textwidth, trim=10cm 0.1cm 0.4cm 0cm, clip]{figure/poly_main_vs_interaction_effects.pdf}}
%\includegraphics[width = \textwidth]{figure/poly_main_vs_interaction_effects.pdf}
%\begin{columns}
%\begin{center}
%\adjincludegraphics[width=\textwidth, trim={0 0 {.55\width} 0}, clip]{figure/poly_main_vs_interaction_effects.pdf}

\textbf{Interpretation}: Not linear anymore!
\begin{itemize}
    \only<1>{\item[$\leadsto$] \code{temp} depends on two weights: $ 280.2 \cdot x_{temp} -  5.6 \cdot x_{temp}^2$}
    \item<2>[$\leadsto$] \code{temp} depends on multiple weights due to \code{season}:\\
    $\leadsto$ \code{WINTER}: ${\color{winter}39.1} \cdot x_{temp} + {\color{winter}8.6} \cdot x_{temp}^2$ \\
    $\leadsto$ \code{SPRING}: 
    $({\color{winter}39.1} {\color{spring}+407.4}) \cdot x_{temp} + ({\color{winter}8.6} {\color{spring} - 18.7}) \cdot x_{temp}^2$ \\
    $\leadsto$ \code{SUMMER}: $({\color{winter}39.1}  {\color{summer}+801.1}) \cdot x_{temp} + ({\color{winter}8.6} {\color{summer} - 27.2}) \cdot x_{temp}^2$  \\
    $\leadsto$ \code{FALL}: $({\color{winter}39.1}  {\color{fall}+217.4}) \cdot x_{temp} + ({\color{winter}8.6} {\color{fall} - 11.3}) \cdot x_{temp}^2$ 
    % \item<2>[$\leadsto$] High-order and interaction effects make the model more flexible but also less interpretable (more weights)
    % \item<2>[$\leadsto$] High-order and interaction effects need to be specified manually (inconvenient) - ML models do this automatically
\end{itemize}

\end{column}
\begin{column}{0.34\linewidth}
%\hfill
\begin{scriptsize}
% \begin{table}[ht]
% %\centering

\def\firstrowcolor{\rowcolor{lightgray}}%
\def\secondrowcolor{\rowcolor{spring}}%
\def\thirdrowcolor{\rowcolor{summer}}%
\def\fourthrowcolor{\rowcolor{fall}}%

\only<1>{%
\begin{tabular}{rr}
  \hline
 & Weights \\ 
  \hline
(Intercept) & 3094.1 \\ 
  seasonSPRING & 619.2 \\ 
  seasonSUMMER & 284.6 \\ 
  seasonFALL & 123.1 \\ 
  hum & -36.4 \\ 
  windspeed & -65.7 \\ 
  days\_since\_2011 & 4.7 \\ 
  \firstrowcolor temp & 280.2 \\ 
  \firstrowcolor temp$^2$ & -5.6 \\ 
  \hline
\end{tabular}%
}
% \end{table}
\only<2>{%
\def\firstrowcolor{\rowcolor{winter}}%
\begin{tabular}{rr}
  \hline
 & Weights \\ 
  \hline
(Intercept) & 3802.1 \\ 
  seasonSPRING & -1345.1 \\ 
  seasonSUMMER & -6006.3 \\ 
  seasonFALL & -681.4 \\ 
  hum & -38.9 \\ 
  windspeed & -64.1 \\ 
  days\_since\_2011 & 4.8 \\ 
 \firstrowcolor temp & 39.1 \\ 
 \firstrowcolor temp$^2$ & 8.6 \\ 
  \hline
  \hline
  \secondrowcolor seasonSPRING:temp & 407.4 \\ 
 \secondrowcolor seasonSPRING:temp$^2$ & -18.7 \\ 
  \thirdrowcolor seasonSUMMER:temp & 801.1 \\ 
 \thirdrowcolor seasonSUMMER:temp$^2$ & -27.2 \\ 
  \fourthrowcolor seasonFALL:temp & 217.4 \\ 
 \fourthrowcolor seasonFALL:temp$^2$ & -11.3 \\ 
   \hline
\end{tabular}%
}%
%Table: Weights for main effect model
\end{scriptsize}
%\end{center}
\end{column}
\end{columns}
\end{frame}


\begin{frame}{Regularization via LASSO \citebutton{Tibshirani (1996)}{https://doi.org/10.1111/j.2517-6161.1996.tb02080.x}}
% \begin{itemize}
%     \item LASSO adds an $L_1$-norm penalization term ($\lambda||\theta||_1$) to the least squares optimization problem%\\
%     % $\leadsto$ Shrinks some feature weights to zero (feature selection)\\
%     % $\leadsto$ Sparser models (with fewer features) are more interpretable
%     % %Aim: Feature selection (sparsity) and regularization of feature weights
%     % \item Penalization parameter $\lambda$ must be chosen (e.g., by CV) %or depending on the number of features to keep
% \end{itemize}

\begin{columns}[T, totalwidth=\linewidth]
\begin{column}{0.6\textwidth}
\begin{itemize}
    \item LASSO adds an $L_1$-norm penalization term ($\lambda||\theta||_1$) %to the least squares optimization problem
    \begin{itemize}
        \item[$\leadsto$] Shrinks some feature weights to zero\\ (feature selection)
        \item[$\leadsto$] Sparser models (fewer features): \\ more interpretable
    \end{itemize}
    %Aim: Feature selection (sparsity) and regularization of feature weights
    \item Penalization parameter $\lambda$ must be chosen (e.g., by CV) %or depending on the number of features to keep
\end{itemize}
\vspace{0.2cm}
$$
min_{\theta} \bigg(\underbrace{\frac{1}{n} \sum_{i=1}^{n} (y^{(i)} - {\xi}^{\top} \theta)^2}_\text{Least square estimate for LM} + \lambda||\theta||_1\bigg) 
$$
\end{column}
\begin{column}{0.4\textwidth}
\centerline{\includegraphics[width=.8\textwidth]{figure/l1_hat.png}} 
\end{column}
\end{columns}

\end{frame}


\begin{frame}{Regularization via LASSO \citebutton{Tibshirani (1996)}{https://doi.org/10.1111/j.2517-6161.1996.tb02080.x}}

\begin{columns}[T, totalwidth=\linewidth]
\begin{column}{0.65\textwidth}
\textbf{Example} (interpretation of weights analogous to LM):
\begin{itemize}
    \item LASSO with main effects and interaction \code{temp} with \code{season}
    \item $\lambda$ is chosen $\leadsto$ 6 selected features ($\neq 0$)
    \item LASSO shrinks weights of single categories separately (due to dummy encoding)\\
    %only weights of single categories are set to zero (due to dummy encoding) not the whole feature\\
    $\leadsto$ No feature selection of whole categorical features\\
    %May be problematic for categorical features as only weights of single categories are set to zero (due to dummy encoding) instead of the whole categorical feature\\ %, polynomials and interactions
    $\leadsto$ Solution: group LASSO \citebutton{Yuan and Lin (2006)}{https://doi.org/10.1111/j.1467-9868.2005.00532.x}
\end{itemize}
\smallskip
\centering
\includegraphics[width = 0.6\textwidth]{figure/reg_lm_plot_interpreted.pdf}

\end{column}
\begin{column}{0.35\textwidth}

\scriptsize
\centering
\begin{tabular}{rr}
  \hline
 & Weights \\ 
  \hline
(Intercept) & 3135.2 \\ 
  seasonSPRING & 767.4 \\ 
  seasonSUMMER & 0.0 \\ 
  seasonFALL & 0.0 \\ 
  temp & 116.7 \\ 
  hum & -28.9 \\ 
  windspeed & -50.5 \\ 
  days\_since\_2011 & 4.8 \\ 
  \hline
  seasonSPRING:temp & 0.0 \\ 
  seasonSUMMER:temp & 0.0 \\ 
  seasonFALL:temp & 30.2 \\ 
   \hline
\end{tabular}
%\end{table}
% \begin{table}[ht]
% \centering
% \begin{tabular}{rr}
%   \hline
%  & Weights \\ 
%   \hline
% (Intercept) & 2665.50 \\ 
%   seasonSPRING & 489.34 \\ 
%   seasonSUMMER & 0.00 \\ 
%   seasonFALL & 0.00 \\ 
%   hum & -19.44 \\ 
%   windspeed & -35.54 \\ 
%   days\_since\_2011 & 4.71 \\ 
%   poly(temp, 2)1 & 109.25 \\ 
%   poly(temp, 2)2 & 0.00 \\ 
%   \hline
% \end{tabular}
% \end{table}
\end{column}
\end{columns}


%\includegraphics[width = 0.8\textwidth]{figure/reg_lm_plot_interpreted.pdf}


\end{frame}
\endlecture
\end{document}