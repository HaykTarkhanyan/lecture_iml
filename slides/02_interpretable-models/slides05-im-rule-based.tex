\documentclass[11pt,compress,t,notes=noshow, aspectratio=169, xcolor=table]{beamer}

\usepackage{../../style/lmu-lecture}
% Defines macros and environments
\input{../../style/common.tex}

\tikzset{main node/.style={rectangle,draw,minimum size=1cm,inner sep=4pt},}

\title{Interpretable Machine Learning}
% \author{LMU}
%\institute{\href{https://compstat-lmu.github.io/lecture_iml/}{compstat-lmu.github.io/lecture\_iml}}
\date{}

\def\firstrowcolor{}
\def\secondrowcolor{}
\def\thirdrowcolor{}
\def\fourthrowcolor{}

\begin{document}

\newcommand{\titlefigure}{figure/decision.png}
\newcommand{\learninggoals}{
% \item Model-based boosting with simple base learners
% \item Feature effect and importance in model-based boosting}
\item Decision trees
\item RuleFit
\item Decision rules}

\lecturechapter{Rule-based Models}
\lecture{Interpretable Machine Learning}


\begin{frame}{Decision Trees \citebutton{Breiman et al. (1984)}{https://doi.org/10.1201/9781315139470}}

%\textbf{Problem}: Can we model non-linear effects and interactions automatically (without manual specification as in GLMs or GAMs)?\\
%Relationship between features and target are non-linear or interactions are present\\
%\medskip
%\pause

\begin{columns}[T, totalwidth=\textwidth]

\begin{column}{0.7\textwidth}
\textbf{Idea of decision trees}: 
%Split data in different subsets based on cut-off values in features 
Partition data into subsets based on cut-off values in features (found by minimizing a split criterion via greedy search) and predict constant mean $c_m$ in leaf node $\mathcal{R}_m$:

$$
\hat f(x) = \sum_{m=1}^M c_m \mathds{1}_{\{x \in \mathcal{R}_m\}}
$$

% \begin{itemize}
% \item where $c_m$ is a constant and 
% \item $\mathcal{R}_m$ the $m$-th leaf node of the tree
% \end{itemize}
\pause
\begin{itemize}
    %\item Finding best split point (CART): Greedy search for the point that minimizes the variance of $y$ (regression) or the Gini index (classification)
    \item Applicable to regression and classification
    \item Able to model interactions and non-linear effects
    \item Able to handle mixed feature spaces and missing values
\end{itemize}

\end{column}
\begin{column}{0.3\textwidth}

\begin{tikzpicture}[scale=0.75, transform shape]
   \usetikzlibrary{arrows}
    \usetikzlibrary{shapes}
     \tikzset{treenode/.style={draw, circle, font=\small}}
     \tikzset{line/.style={draw, thick}}
     \node [treenode, draw=red] (a0) {};
     \node [treenode, below=0.75cm of a0, xshift=-1.5cm]  (a1) {};
     \node [treenode, below=0.75cm of a0, xshift=1.5cm]  (a2) {};

     \node [treenode, below=0.75cm of a2, xshift=-0.75cm] (a3) {$c_1$};
     \node [treenode, below=0.75cm of a2, xshift=0.75cm]  (a4) {$c_2$};

    \node [treenode, below=0.75cm of a1, xshift=-0.75cm] (a5) {$c_3$};
    \node [treenode, below=0.75cm of a1, xshift=0.75cm]  (a6) {$c_4$};

     \path [line] (a0.south) -- + (0,-0.4cm) -| (a1.north) node [midway, above] {$x_1<3$};
     \path [line] (a0.south) -- +(0,-0.4cm) -|  (a2.north) node [midway, above] {$x_1\geq3$};

     \path [line] (a2.south) -- + (0,-0.4cm) -| (a3.north) node [midway, above] {$x_2<6$};;
     \path [line] (a2.south) -- +(0,-0.4cm) -|  (a4.north) node [midway, above] {$x_2\geq6$};

    \path [line] (a1.south) -- + (0,-0.4cm) -| (a5.north) node [midway, above] {$x_3<2$};;
    \path [line] (a1.south) -- +(0,-0.4cm) -|  (a6.north) node [midway, above] {$x_3\geq2$};

    %\path (a5.south) -- +(0,0) -|  (a5.south) node [midway, below] {$f_{1,3}(x_1,x_3)$};
    %\path (a6.south) -- +(0,0) -|  (a6.south) node [midway, below] {$f_{1,3}(x_1,x_3)$};

    %\path (a3.south) -- +(0,0) -|  (a3.south) node [midway, below] {$f_{1,2}(x_1,x_2)$};
    %\path (a3.south) -- +(0,0) -|  (a4.south) node [midway, below] {$f_{1,2}(x_1,x_2)$};
    % \path (a3) edge [bend right, draw=white] node [below] {$f_{1,2}(x_1,x_2)$} (a4);
    % \path (a5) edge [bend right, draw=white] node [below] {$f_{1,3}(x_1,x_3)$} (a6);
   \end{tikzpicture}
   
\end{column}
\end{columns}
\end{frame}


\begin{frame}{Interpretation}
\begin{itemize}
    \item Directly by following the tree structure (i.e., sequence of decision rules)
    \item Importance of $x_j$: Aggregate ``improvement in split criterion'' over all splits where $x_j$ was involved\\
    \item[] $\leadsto$ e.g., variance for regression or Gini index for classification
    %Feature importance: How much did split criterion improve compared to parent node% by (scaled) score of how much splitting criterion (e.g. variance) is reduced compared to a parent node
\end{itemize}

\end{frame}



\begin{frame}{Decision Trees - Example}
\begin{itemize}
    \item Fit decision tree with tree depth of 3 on bike data
    \item E.g., mean prediction for the first 105 days since 2011 is 1798 (applies to $\hat = 15\%$ of the data)
    \item \code{days\_since\_2011} shows highest feature importance (explains most of variance)
\end{itemize}
\begin{columns}[T]
\begin{column}{0.4\textwidth}
\vspace{1.5cm}
\begin{table}[ht]
\centering
\begin{tabular}{lr}
  \hline
 Feature & Importance \\
  \hline
days\_since\_2011 & 79.53 \\ 
  temp & 17.55 \\ 
  hum & 2.92 \\ 
   \hline
\end{tabular}
\end{table}
\end{column}
\begin{column}{0.6\textwidth}
  \includegraphics[width = \textwidth]{figure/tree.pdf} 
\end{column}
\end{columns}
 
\end{frame}
%------------------------------------------------------------------
%------------------------------------------------------------------

%\begin{frame}[c]{Decision Rules}

%\texttt{IF COND$_1$ AND COND$_2$ AND ... THEN value}

%\begin{itemize}
%    \item \texttt{COND$_i$} can be of the form \texttt{feature <op> value} where \texttt{<op>} can be for example $\{=, <, > \}$
%\end{itemize}

%\pause
%\medskip

%Properties:
%\begin{description}
%    \item{Support} Fraction of observations to support appliance of rule
%    \item{Accuracy} for predicting the correct class under the condition(s)
%\end{description}

%$\leadsto$ often trade-off between these two

%\pause
%\medskip

%$\leadsto$ many different ways to learn a set of rules (incl. a default rule if none of the rules are met)

%\end{frame}

%------------------------------------------------------------------
%------------------------------------------------------------------


\begin{frame}[c]{Other Rule-based Models}

\begin{columns}[c, totalwidth=\textwidth]
    \begin{column}{0.6\textwidth}
        \textbf{RuleFit} \citebutton{Friedman and Popescu 2008}{https://arxiv.org/abs/0811.1679}
        \begin{itemize}
            \item Combination of linear models and decision trees 
            \item Allows for feature interactions and non-linearities
        \end{itemize}

        % \only<3->{\textbf{Naive Bayes}
        % \citebutton{Zhang 2004}{https://www.aaai.org/Papers/FLAIRS/2004/Flairs04-097.pdf}
        % %$$P (C_k \mid x ) = \frac{1}{Z} P(C_k) \prod_{i=1}^{n} P(x_i \mid C_k) $$
        % \begin{itemize}
        %     \item Uses Bayes' theorem to assign class prob. to observations
        %     %Product of (conditional) probabilities for a class on the value of each feature
        %     %For each feature, it calculates the probability for a class depending on the value of the feature. 
        %     \item Strong assumption: Independence of features
        % \end{itemize}}

        % \only<4->{\textbf{k-Nearest Neighbor}
        % \citebutton{Cover 1967}{https://doi.org/10.1109/TIT.1967.1053964}
        % \begin{itemize}
        %     \item (Closely related to case-based reasoning)
        %     \item Average of the outcome of neighbors -- local explanation
        % \end{itemize}

        % ...}
    \end{column}
    \begin{column}{0.4\textwidth}
    \vspace{\dimexpr-2\parsep-2\parskip\relax}
        \begin{center}
            \includegraphics[width = 0.6\textwidth]{figure/RuleFit.png} 
            % \only<4->{\includegraphics[width = 0.7\textwidth]{figure/knn.png} 
            % \citebutton{José 2018}{https://towardsdatascience.com/knn-k-nearest-neighbors-1-a4707b24bd1d}}
        \end{center}
    \end{column}
\end{columns}

\medskip

\begin{columns}[c, totalwidth=\textwidth]
    \begin{column}{0.6\textwidth}
        \only<2->{\textbf{Decision Rules} \citebutton{Holte 1993}{https://doi.org/10.1023/A:1022631118932}
        \begin{itemize}
            \item Simple ``if -- then'' statements - very intuitive and easy-to-interpret
            \item Most methods work only for classification and categorical feat.
        \end{itemize}}
    \end{column}
    \begin{column}{0.4\textwidth}
    \vspace{\dimexpr-2\parsep-2\parskip\relax}
        \begin{center}
            \only<2->{\includegraphics[width = 0.8\textwidth]{figure/decision_rules.png} \\
            \citebutton{Molnar 2022}{https://christophm.github.io/interpretable-ml-book/}\\
            \smallskip}
            % \only<4->{\includegraphics[width = 0.7\textwidth]{figure/knn.png} 
            % \citebutton{José 2018}{https://towardsdatascience.com/knn-k-nearest-neighbors-1-a4707b24bd1d}}
        \end{center}
    \end{column}
\end{columns}

\end{frame}





% %------------------------------------------------------------------
% %------------------------------------------------------------------


% \begin{frame}{Model-based Boosting \citebutton{Bühlmann and Yu 2003}{https://doi.org/10.1198/016214503000125}}

% \begin{itemize}%[<+->]
% %\setlength\itemsep{2em}
% \item<1-> Recall: Boosting iteratively combines weak base learners (BL) %to create a powerful ensemble model
% \item<1->
% Idea: Use simple linear BL to ensure interpretability \\
% %$\leadsto$ e.g., linear BL with single features in each iteration
% %Boosting with gradient descent using interpretable base learners (e.g., use base learners with single features in each iteration $\leadsto$ coordinate gradient descent)
% %The resulting ensemble is also interpretable.
% %\pause
% \item<2->
% Possible to combine linear BL of same type (with distinct parameters $\theta$ and $\theta^{\star}$):
% %Two linear base learners $b_j(x, \theta)$ and $b_j(x, \theta^{\star})$ of the same type, but distinct parameter vectors $\theta$ and $\theta^{\star}$ can be combined in a base learner of the same type:
% $$b_j(x, \theta) + b_j(x, \theta^{\star}) = b_j(x, \theta + \theta^{\star})$$
% %\pause
% \item<3-> %In each iteration, a set of BLs is fitted on pseudo residuals. The one with the best fit is added to the previously computed model (using step-size $\nu$), e.g.,
% In each iteration, fit a set of BLs and add the best BL to previous model (using step-size $\nu$):
% %\medskip
% \begin{align*}
% \widehat{f}^{[1]}(x) &= \hat{f}_0 + \nu \textcolor{blue}{b_3(x_3, \theta^{[1]})} \\
% \visible<4->{\widehat{f}^{[2]}(x) &= \widehat{f}^{[1]}(x) + \nu \textcolor{blue}{b_3(x_3, \theta^{[2]})} 
% %= \hat{f}_0 + \nu \textcolor{blue}{b_3(x_3, \theta^{[1]})} + \nu \textcolor{blue}{b_3(x_3, \theta^{[2]})}
% \\
% \visible<5->{
% \widehat{f}^{[3]}(x) &= \widehat{f}^{[2]}(x) + \nu \textcolor{orange}{b_1(x_1, \theta^{[3]})} 
% %= \hat{f}_0 + \nu \textcolor{blue}{b_3(x_3, \theta^{[1]})} + \nu \textcolor{blue}{b_3(x_3, \theta^{[2]})} + \nu \textcolor{orange}{b_1(x_1, \theta^{[3]})} 
% \\
% &= \hat{f}_0 + \nu \left(\textcolor{blue}{b_3(x_3, \theta^{[1]} + \theta^{[2]})} + \textcolor{orange}{b_1(x_1, \theta^{[3]})}\right) 
% \\
% &= \hat{f}_0 + \textcolor{blue}{\hat{f}_3(x_3)} + \textcolor{orange}{\hat{f}_1(x_1)}
% }
% }
% \end{align*}
% \item<6-> Final model is additive (as GAMs), where each component function is interpretable

% \end{itemize}
% \end{frame}


% \begin{frame}{Model-based Boosting - Example}

% Simple case: Use linear model with single feature (including intercept) as BL
% $$
% b_j(x_j, \theta) = x_j\theta + \theta_0 \hspace*{0.2cm}\text{ for } j = 1,\ldots p \hspace*{0.3cm} \leadsto \text{ordinary linear regression}
% $$

% \begin{itemize}
% \item<1-> Here: Interpretation of weights as in LM
% \item<1-> After many iterations, it converges to same solution as least square estimate of LMs
% \item<2-> Early stopping allows feature selection and might prevent overfitting (regularization)
% %\item Specifying loss and link function according to exponential family leads a (regularized) GLM
% \end{itemize}
% \begin{columns}[T]
% \begin{column}{0.49\textwidth}
% \scriptsize
% \begin{table}[ht]
% \centering
% \begin{tabular}{r|r|l}
%   %\hline
% \textbf{1000 iter. with $\nu = 0.1$} & Intercept & Weights \\ 
%   \hline  \hline
% days\_since\_2011 & -1791.06 & 4.9 \\ 
%   \hline
%   hum & 1953.05 & -31.1 \\ 
%     \hline
%   season & 0 &  \begin{tabular}[c]{@{}l@{}}
%   WINTER: -323.4\\
%   SPRING: 539.5\\
%   SUMMER: -280.2\\
%   FALL: 67.2
%   \end{tabular}\\
%     \hline
%   %season &  & WINTER: -323.4, SPRING: 539.5, SUMMER: -280.2, FALL: 67.2 \\ 
%   temp & -1839.85 & 120.4 \\ 
%     \hline
%   windspeed & 725.70 & -56.9 \\ 
%     \hline
%   offset & 4504.35 &  \\ 
%    %\hline
% \end{tabular}
% \end{table}
% \centering
% $\Rightarrow$ Converges to solution of LM
% \end{column}
% \begin{column}{0.49\textwidth}

% \only<1>{\scriptsize
% Relative frequency of selected BLs across iterations
% \includegraphics[width = .95 \textwidth]{figure/compboost_base_linear.pdf}}

% \pause
% \scriptsize
% \only<2>{
% \begin{table}[ht]
% \centering
% \begin{tabular}{r|r|l}
%   %\hline
%  \textbf{20 iter. with $\nu = 0.1$} & Intercept & Weights \\ 
%   \hline  \hline
%   days\_since\_2011 & -1210.27 & 3.3 \\ 
%     \hline
%    season & 0 & 
%    \begin{tabular}[c]{@{}l@{}}
%   WINTER: -276.9\\
%   SPRING: 137.6\\
%   SUMMER: 112.8\\
%   FALL: 20.3
%    \end{tabular}\\
%      \hline
%   temp & -1118.94 & 73.2 \\ 
%     \hline
%   offset & 4504.35 &  \\ 
%    %\hline
% \end{tabular}
% \end{table}
% \centering
% $\Rightarrow$ 3 BLs selected after 20 iter. (feature selection)
% }
% \end{column}
% \end{columns}
% % \begin{itemize}
% %     \item Linear base learners for numeric features and categorical base learner for season
% %     \item 3 base learners selected after 100 iterations
% % \end{itemize}
% \end{frame}

% \begin{frame}{Model-based Boosting - Interpretation}

% \begin{itemize}
%     \item Fit model on bike data with different BL types \citebutton{Daniel Schalk et al. 2018}{https://doi.org/10.21105/joss.00967}
%     \item BLs: linear and centered splines for numeric features, categorical for season
%     %and categorical base learner for season
% \end{itemize}
% \begin{columns}[T]
% \visible<2->{
% \begin{column}{0.5\textwidth}
% \hspace{45pt}{\scriptsize{Feature importance}}
%  \includegraphics[width = \textwidth]{figure/compboost_pfi.pdf}
% %Feature importance (risk reduction over iter.)\\
% %$\leadsto$ \code{days\_since\_2011} most important
% %\scriptsize
% %\verbatiminput{figure/mboost_output.txt}
% \end{column}
% }
% \visible<3->{
% \begin{column}{0.5\textwidth}  %%<--- here
% \hspace{23pt}{\scriptsize{Feature effect}}
%   \includegraphics[width = \textwidth]{figure/compboost_pfe.pdf}
% %Partial feature effect for \code{days\_since\_2011}\\
% %$\leadsto$ Total effect: Combination of partial effects of linear BL and centered spline BL
% \end{column}
% }
% \end{columns}
% \begin{itemize}
%     \item<2->  Feature importance (risk reduction over iter.) $\leadsto$ \code{days\_since\_2011} most important
%      \item<3-> Total effect for \code{days\_since\_2011}\\
% $\leadsto$ Combination of partial effects of linear BL and centered spline BL
% \end{itemize}
% \end{frame}


\endlecture
\end{document}