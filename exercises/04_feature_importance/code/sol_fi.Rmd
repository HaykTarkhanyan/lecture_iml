---
title: "Exercise Feature Importance"
output: html_document
---
# Libraries
```{r}
library(docstring)
library(ggplot2)
#source("utils.R")
```

# Exercise 1: Permutation feature importance
## 1c)
```{r}
### Read in the dataset via the read.csv()-function and remove the column that only contains the row indices.
df <- read.csv(file = "extrapolation.csv")
df <- df[ ,-which(names(df) == "X")]

### The classical training and testing data split (here 70% training, 30% testing data).
set.seed(100)
train <- sample(nrow(df), 0.7 * nrow(df))
training_data <- df[train, ]
test_data <- df[-train, ]

### Fit a linear model using the training data.
model <- lm(y ~ ., data = training_data)

### Assess the MSE on the test dataset.
preds <- predict(model, newdata = test_data)
mse <- mean((test_data$y - preds) ^ 2)
print(paste("MSE:", mse))
```

## 1d)
First, we implement PFI (with mean squared error as performance metric). 
```{r}
### Calculate the PFI score for a single feature given by fname.
pfi_fname <- function(fname, model, X_test, y_test) {
  
  #' Function that returns the PFI for a single feature.
  #'
  #' @param fname: Name of the selected feature.
  #' @param model: Classifier which has a predict method.
  #' @param X_test (data.frame): Input X_test data.
  #' @param y_test (vector): y_test values
  #'
  #' @return performance (integer(1)): performance metric
  
  ### Permute the observations for feature fname.
  X_test_perm <- X_test
  X_test_perm[[fname]] <- sample(X_test_perm[[fname]])
  
  ### Predict on the original data situation as well as on the permuted one.
  preds_original <- predict(model, X_test)
  preds_perm <- predict(model, X_test_perm)
  
  ### Calculate the MSE on both data situations.
  mse_original <- mean((y_test - preds_original) ^ 2)
  mse_perm <- mean((y_test - preds_perm) ^ 2)
  
  ### The PFI score is now defined as the increase in MSE when permuting the feature.
  mse_perm - mse_original
}

### Calculate the PFI score defined via the function fi_fname_func for all features in X_test.
fi <- function(fi_fname_func, ...) {
  
  #' Feature importance for all features given a feature importance method for a single column
  #' model, X_test, y_test,
  #' @param model: Classifier which has a predict method.
  #' @param X_test (data.frame): Input X_test data.
  #' @param y_test (vector): y_test values
  #'
  #' @return results (vector): relevance for each feature (in the order of X_test.columns)
  
  ### Iterate over all features in X_test and calculate their single feature PFI score.
  unlist(lapply(colnames(X_test), fi_fname_func, ...))
}
```

This is the function to run n times for all naive functions.
Different arguments will be passed here according to the order of the parameters in each naive function.
In total, there are 7 arguments: model, X_test, y_test, X_train, n_marginalize, df, 'y'(y_name).
```{r}
n_times <- function(func, n, return_raw, ...) {
  #' Run for n times but pass different naive function.
  #'
  #' @param func: a feature importance method
  #' @param n (integer(1)): How many times to run.
  #' @param return_raw (bool): whether to have row results or not.
  #'
  #' @return final_Result (list): A list containing mean_fi, std_fi (row results).
  
  ### Apply the function n times. We need to take the transpose to get the result into the right shape.
  results <- t(sapply(1:n, function(i) func(...)))
  
  ### Return the mean_fi, the std_fi and if wanted the raw results contained in a list.
  list(colMeans(results), apply(results, 2, sd), if (return_raw) results)
}
```

Now we apply the method to our model and dataset. 
```{r}
### Create appropriate datasets to use our implemented functions.
X_train <- training_data[ , -which(names(training_data) == "y")]
X_test <- test_data[ , -which(names(test_data) == "y")]
y_train <- training_data[ , which(names(training_data) == "y")]
y_test <- test_data[ , which(names(test_data) == "y")]

barplot_results <- function(results) {
  ### Create a data.frame to be able to use ggplot2 appropriately.
  results_mean_std <- data.frame(results[1], results[2])
  rownames(results_mean_std) <- c('x1', 'x2', 'x3', 'x4')
  colnames(results_mean_std) <- c('col_means', 'col_stds')
  
  ### Use ggplot2 to create the barplot.
  ggplot(cbind(Features = rownames(results_mean_std), results_mean_std[1:4, ]), 
         aes(x = reorder(Features, results_mean_std$col_means), y = results_mean_std$col_means)) +
    ### Plot the mean value bars.
    geom_bar(stat = "identity", fill = "steelblue") +
    ### Plot the standard deviations.
    geom_errorbar(aes(ymin = results_mean_std$col_means - results_mean_std$col_stds,
                      ymax = results_mean_std$col_means + results_mean_std$col_stds), width = .1) +
    ### Set the labels correctly.
    labs(y = "Mean Value", x = "Features")
}

### Put all three above implemented functions together to get the PFI score for all features.
pfi_results <- n_times(fi, 10, TRUE, pfi_fname, model, X_test, y_test)

### Plot the result.
barplot_results(pfi_results)
```


## 1f)
```{r}
### Get the coefficients and the intercept.
model$coefficients

### Show the correlation structure in the testing dataset.
cor(test_data)
```

## 1h)
We use the extrapolation.csv dataset. We create a pairplot showing the pairwise scatterplot of the original feature as well as the corresponding perturbed variable (x2 in our case) with all remaining feature variables (and potentially y).
```{r}
### Bring the dataset in the correct form for ggplot2. This means we need every combination of x2 value to any other variable (which is done in the value column). The series column is for the group membership. Moreover we add a permuted version of x2 to the column x2. To keep track of this memberships we have the column type.
reshaped_test_data <- data.frame(x2 = c(rep(test_data$x2, 4), rep(sample(test_data$x2), 4)),
                                 type = rep(c('original', 'permuted'), each = 4 * nrow(test_data)),
                                 series = rep(rep(c('x1', 'x3', 'x4', 'y'), each = nrow(test_data)), 2),
                                 value = rep(c(test_data$x1, test_data$x3, test_data$x4, test_data$y), 2))

### Create a scatterplot for both data situations at once.
ggplot(reshaped_test_data, aes(x2, value)) +
  geom_point() +
  facet_grid(series ~ type)
```

# Exercise 2: Conditional sampling based feature importance techniques
## 2a)
```{r}
sample_cond_gaussian <- function(J, C, X_train, X_test, num_samples) {
  ### Order the training data for the decomposition.
  X_train_reordered <- cbind(X_train[ , J], X_train[ , C])
  
  ### Get the overall mean vector and covariance matrix.
  mu <- colMeans(X_train_reordered)
  sigma <- cov(X_train_reordered)
  
  ### Decomposition of the mean vector. We use seq_along() since it might be that there are no variables to condition on.
  mu_1 <- as.matrix(mu[seq_along(J)])
  mu_2 <- as.matrix(mu[seq_along(C) + length(J)])
  
  ### Decomposition of the necessary parts of the covariance matrix.
  sigma_12 <- sigma[seq_along(J), seq_along(C) + length(J)]
  sigma_22 <- sigma[seq_along(C) + length(J), seq_along(C) + length(J)]
  
  ### Get a matrix for the mean vectors for all features in the conditional set at once. The mean vectors are the columns. When giving just one matrix to the solve()-function it calculates the inverse via solving linear systems. This is much better conditioned than explicitly calculating the inverse.
  mu_bar_all_C <- as.matrix(sapply(1:nrow(X_test), function(i) mu_1 + sigma_12 %*% solve(sigma_22) %*% (t(as.matrix(X_test[i , C], ncol = length(C))) - mu_2)), nrow = length(J))
  
  ### The Schur complement of sigma_22 in sigma as described in the Wikipedia article (https://en.wikipedia.org/wiki/Multivariate_normal_distribution#Conditional_distributions).
  sigma_bar <- solve(solve(sigma)[seq_along(J), seq_along(J)])
  
  ### Now sample from the distributions given by mean vector and covariance matrix for all features in the conditional set at once. Return the result as a data.frame.
  as.data.frame(sapply(1:ncol(mu_bar_all_C), function(i) MASS::mvrnorm(num_samples, mu_bar_all_C[ , i], sigma_bar)))
}

sample_cond <- function(J, C, X_train, X_test, num_samples) {
  #' Conditional sampler
  #'
  #' @param J: Names of interested features.
  #' @param C: Names of features conditioned on.
  #' @param X_train (data.frame): Input X_train data.
  #' @param X_test (data.frame): Input X_test data.
  #' @param num_samples (integer(1)): Times to sample.
  #'
  #' @return sample(data.frame): Sample results.
  
  dist <- GaussianConditionalEstimator$new()
  dist$fit(X_train[ , J], X_train[ , C])
  sample <- dist$sample(X_test[ , C], num_samples, J)
  return(sample)
}
```
Show the sample results.
```{r}
J <- c('x1', 'x3')
C <- c('x2', 'x4')
sample <- sample_cond_gaussian(J, C, X_train, X_test, 10)
head(sample)
```

## 2b) 
We implement conditional feature importance (CFI)
```{r}
cfi_fname = function(fname, model, X_train, X_test) {
  #' Conditional feature importance for a feature.
  #'
  #' @param fname: Name of the selected feature.
  #' @param model: Classifier which has a predict method.
  #' @param X_train (data.frame): Input X_train data.
  #' @param X_test (data.frame): Input X_test data.
  #'
  #' @return performance: performance metric
  
  ### Get the names of the features to be conditioned on.
  C <- colnames(X_train)[colnames(X_train) != fname]
  
  ### Apply the conditional sampling procedure.
  sample <- sample_cond_gaussian(fname, C, X_train, X_test, 1)
  
  ### Permute the observations for feature fname.
  X_test_perm <- X_test
  X_test_perm[fname] <- sample
  
  ### Predict on the original data situation as well as on the permuted one.
  preds_original <- predict(model, X_test)
  preds_perm <- predict(model, X_test_perm)
  
  ### Calculate the MSE on both data situations.
  mse_original <- mean((y_test - preds_original) ^ 2)
  mse_perm <- mean((y_test - preds_perm) ^ 2)
  
  ### The PFI score is now defined as the increase in MSE when permuting the feature.
  mse_perm - mse_original
}
```


## 2c)
```{r}
### The exact same procedure to generate the plot as in exercise 1.
cfi_results <- n_times(fi, 10, TRUE, cfi_fname, model, X_train, X_test)

barplot_results(cfi_results)
```

## 2d)
```{r}
c_value_func_partial <- function(J, X_train, X_test, y_test, n_marginalize, model) {
  #' Partial value function, i.e. performance given all features are
  #' reconstructed conditional on the set J, marginalization of the prediction
  #' over the remaining features
  #'
  #' @param J (vector): Name of the selected feature.
  #' @param X_train (data.frame): Input X_train data.
  #' @param X_test (data.frame): Input X_test data.
  #' @param y_test (vector): y_test values
  #' @param n_marginalize (integer(1)): Number to marginalize.
  #' @param model: Classifier which has a predict method.
  #'
  #' @return performance: performance metric
  
  ### Get the feature names to be conditioned on.
  C <- colnames(X_train)[!(colnames(X_train) %in% J)]
  
  ### When there are no features to be conditioned on we just use the ordinary MSE.
  if (!length(C)) {
    X_test_prediction <- predict(model, X_test)
    return(mean((y_test - X_test_prediction) ^ 2))
  }
  
  ### Else we use our conditional sampler.
  sample <- sample_cond_gaussian(C, J, X_train, X_test, n_marginalize)
  
  ### Replicate each row n_marginalize times to create a new dataset.
  replication_x_test <- X_test[rep(1:nrow(X_test), each = n_marginalize), ]
  
  ### Replace the corresponding columns using the sample.
  replication_x_test[C] <- sample
  
  ### Predict the test set observations based on the replicated dataset.
  new_y_test <- predict(model, replication_x_test)
  
  ### Get the mean prediction.
  y_hat_agg <- colMeans(matrix(new_y_test, n_marginalize))
  
  ### Calculate the overall MSE.
  mean((y_test - y_hat_agg) ^ 2)
}

c_value_func <- function(J, S, X_train, X_test, y_test, n_marginalize, model) {
  #' Conditional SAGE value function of variables j given coalition S.
  #'
  #' @param J (vector): Name of the selected feature.
  #' @param S (vector): name of the Coalition features.
  #' @param X_train (data.frame): Input X_train data.
  #' @param X_test (data.frame): Input X_test data.
  #' @param y_test (vector): y_test values
  #' @param n_marginalize (integer(1)): Number to marginalize.
  #' @param model: Classifier which has a predict method.
  #'
  #' @return: Performance difference between S and J+S.
  
  ### The partial value function only for S.
  partial_for_S <- c_value_func_partial(S, X_train, X_test, y_test, n_marginalize, model)
  
  ### The partial value function for J+S.
  partial_for_J_and_S <- c_value_func_partial(c(J, S), X_train, X_test, y_test, n_marginalize, model)
  
  ### The performance difference.
  partial_for_S - partial_for_J_and_S
}

sage_val_empty <- function(J, X_train, X_test, y_test, n_marginalize, model) {
  #' SAGE value given empty coalition.
  #'
  #' @param J (vector): Name of the selected feature.
  #' @param X_train (data.frame): Input X_train data.
  #' @param X_test (data.frame): Input X_test data.
  #' @param y_test (vector): y_test values
  #' @param n_marginalize (integer(1)): Number to marginalize.
  #' @param model: Classifier which has a predict method.
  #'
  #' @return: Score difference given empty coalition.
  
  ### Return the conditional SAGE value for empty coalition S.
  c_value_func(J, c(), X_train, X_test, y_test, n_marginalize, model)
}

sage_val_remainder <- function(J, X_train, X_test, y_test, n_marginalize, model) {
  #' Sage value given full remainder as coalition.
  #'
  #' @param J (vector): Name of the selected feature.
  #' @param X_train (data.frame): Input X_train data.
  #' @param X_test (data.frame): Input X_test data.
  #' @param y_test (vector): y_test values
  #' @param n_marginalize (integer(1)): Number to marginalize.
  #' @param model: Classifier which has a predict method.
  #'
  #' @return: Score difference given full remainder as coalition.
  
  ### Get the remaining column names.
  remainder <- colnames(X_train)[!(colnames(X_train) %in% J)]
  
  ### Return the conditional SAGE value for the remainder.
  c_value_func(J, remainder, X_train, X_test, y_test, n_marginalize, model)
}
```


## 2e)
```{r}
### The exact same procedure to generate the plot as in exercise 1.
sage_empty_results <- n_times(fi, 10, FALSE, sage_val_empty, X_train, X_test, y_test, 50, model)

barplot_results(sage_empty_results)
```

```{r}
### The exact same procedure to generate the plot as in exercise 1.
sage_remainder_results <- n_times(fi, 10, FALSE, sage_val_remainder, X_train, X_test, y_test, 50, model)

barplot_results(sage_remainder_results)
```



# Exercise 3: Refitting based importance
## 3a)
```{r}
loco <- function(fname, original_model, X_test, y_test, original_df, y_name) {
  #' LOCO importance for feature fname
  #'
  #' @param fname: Name of the selected feature.
  #' @param original_df (data.frame): Original dataset.
  #' @param originalModel: Original classifier which has a predict method.
  #' @param X_test (data.frame): Input X_test data.
  #' @param y_test (vector): y_test values.
  #' @param y_name: Name of the y_test column.
  #'
  #' @return performance: performance metric
  
  ### Get the training data without the column with the feature of interest.
  remainder <- original_df[colnames(original_df) != fname]
  
  ### The usual training and testing split (with 70% training data).
  set.seed(100)
  inds <- sample(nrow(remainder), 0.7 * nrow(remainder))
  new_training_data <- remainder[inds, ]
  new_test_data <- remainder[-inds, ]
  
  ### Get the features and the target.
  loco_X_test <- new_test_data[ , colnames(new_test_data) != y_name]
  loco_y_test <- new_test_data[ , y_name]
  
  ### Generate the formula object we will give to the lm()-function.
  outcome <- names(new_training_data[y_name])
  variables <- names(loco_X_test)
  f <- as.formula(paste(outcome, paste(variables, collapse = " + "), sep = " ~ "))
  
  ### Train the OLS model.
  new_model <- lm(f, data = new_training_data)
  
  ### Get the MSE for the model with all features.
  preds_for_original <- predict(original_model, X_test)
  original_mse <- mean((y_test - preds_for_original) ^ 2)
  
  ### Get the MSE for the model without the feature of interest.
  predict_for_loco <- predict(new_model, loco_X_test)
  loco_mse <- mean((loco_y_test - predict_for_loco) ^ 2)
  
  ### The performance is given by the differences of the MSEs.
  loco_mse - original_mse
}

loco_naive <- function(original_df, original_model, X_test, y_test, y_name, ...) {
  #' Naive loco feature importance implementation.
  #'
  #' @param original_df (data.frame): Original dataset.
  #' @param originalModel: Original classifier which has a predict method.
  #' @param X_test (data.frame): Input X_test data.
  #' @param y_test (vector): y_test values.
  #' @param y_name: Name of the y_test column.
  #'
  #' @return results: relevance for each feature (in the order of X_test.columns)
  
  ### Iterate over all features and apply the above implemented LOCO function.
  sapply(colnames(X_test), function(name) loco(name, original_df, original_model, X_test, y_test, y_name))
}
```
## 3b)
```{r}
### The exact same procedure to generate the plot as in exercise 1.
loco_results <- n_times(fi, 10, FALSE, loco, model, X_test, y_test, df, 'y')

barplot_results(loco_results)
```
