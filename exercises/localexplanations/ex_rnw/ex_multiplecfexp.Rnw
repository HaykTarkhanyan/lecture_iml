You received a dataset with 11 observations and two features. 
<<echo=FALSE, results='asis'>>=
library(ggplot2)
set.seed(1234L)
x1 = seq(-1, 1, 0.2)
x2 = x1^2+rnorm(length(x1), sd = 0.04)
y = 5*x1 + -2*x2 + rnorm(length(x1))
d = data.frame(y, x1, x2)
library(xtable)
print(xtable(t(d), digits = 2))
@

\begin{enumerate}[a)]
  \item Compute the Pearson correlation of $x_1$ and $x_2$. 
  The formula is: 
  $$ 
  \rho(x_1, x_2) = \frac{\sum_{i = 1}^n (x_1^{(i)} - \bar{x_2})(x_1^{(i)} - \bar{x_2})}{\sqrt{\sum_{i = 1}^n (x_1^{(i)} - \bar{x_2})} \sqrt{\sum_{i = 1}^n (x_1^{(i)} - \bar{x_2})}}
  $$ 
  Interprete the results. Based on $\rho(x_1, x_2)$, are $x_1$ and $x_2$ correlated? 
  
  \item The scatter plot of the two features is shown in the following figure: 
  
  \begin{center}
<<fig.height=3, fig.width=4, echo = FALSE, warning=FALSE,message=FALSE,out.width='3in'>>=
ggplot(d, aes(x = x1, y = x2)) + 
  geom_point() +
  theme_bw()
@
  \end{center}
 Would you still consider the Pearson correlation coefficient a reliable measure to 
 detect correlations for above's use case?
 \item One method to detect non-linear relationships is the usage of 
  generalized additive models (GAM).
  The following shows the output of the GAM when $x_2$ depends on a smooth function of $x_1$. 
  Interprete the results. 
<<echo = FALSE, message=FALSE>>=
library(mgcv)
mod <- gam(x2 ~ s(x1), data = d) 
summary(mod)
@
  What conclusions could you draw for the relationship between $x_1$ and $x_2$? 
  
  \item 
  Instead of 11 datapoints you now received a dataset with 1000 datapoints from 
  the same data generating process. 
  You fitted a linear model to the data 
  $\fh(\xv) = \beta_0 + \hat\beta_1 \xv_1 + \hat\beta_2 \xv_2 + \hat\beta_3 \xv_1 \xv_2$
  The following plots show the PDP (first two figures) and ALE (last two figures) for $x_1$ and $x_2$. 
  \begin{enumerate}
  \item Interprete the plots with respect to the feature effect of $x_1$ and $x_2$. 
  \item Would you rather trust the PDP or ALE plot? Give reasons for your 
  decision. 
  
  <<fig.height=3, fig.width=6, echo = FALSE, message = FALSE>>= 
  library(ggplot2)
  set.seed(1234L)
  x1 = runif(1000, -1, 1)
  x2 = x1^2+rnorm(length(x1), sd = 0.6)
  y = 5*x1 + -2*x2 + 3*x1*x2 + rnorm(length(x1))
  d = data.frame(y, x1, x2)
  
  rf = randomForest::randomForest(y~x1+x2, d = d)
  pred = iml::Predictor$new(rf, data = d, y = y)
  plot(iml::FeatureEffects$new(pred, feature = c("x1","x2"), method = "pdp"))
  @
  
  <<fig.height=3, fig.width=6, echo = FALSE, message = FALSE>>= 
  plot(iml::FeatureEffects$new(pred, feature = c("x1","x2"), method = "ale"))
  @
  \end{enumerate}
\end{enumerate}